{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reseach Topic: Predict Wage with Machline Methods\n",
    "## What am I doing ?\n",
    "I have no idea.\n",
    "\n",
    "## Data\n",
    "The source of data is NLSY97. The portal can be accessed from [here](https://www.nlsinfo.org/investigator/pages/search.jsp?s=NLSY97). The data set is pre-cleaned by STATA with around 4500 obervations each year from 2. Features include age, year of experience, gender, schooling, race, marital status, industry, region(not yet!!). \n",
    "\n",
    "## Run some models\n",
    "Bofore running anything, we split the data into training set, validation set, test set. <br/> I am preparing to do the following models:\n",
    "- Linear model (mincer equations)\n",
    "- Trees and Forests\n",
    "- Neural Nets\n",
    "\n",
    "The performences of models will be compared in accuracy and R-square\n",
    "\n",
    "## Further thoughts\n",
    "What can we do besides running those well-defined models on a dataset and seeing some results we already know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # turn off warnings a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''Preprocessing Steps\n",
    "'''\n",
    "data_path = './data/mincer.xlsx'\n",
    "#dat.describe(include=\"all\")\n",
    "dat = pd.read_excel(data_path)\n",
    "dat = dat.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_tmp = dat[dat['year']==2015]\n",
    "train, test =  train_test_split(dat_tmp, test_size = 0.25, random_state = 12)\n",
    "y_train, X_train, y_test, X_test = train['lnwage'], train.iloc[:,4:], test['lnwage'], test.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Linear Model (Mincer Equation)\n",
    "It is a panel data regression using random effect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16406052219835443\n"
     ]
    }
   ],
   "source": [
    "r2 = mincer(X_train, y_train,  X_test, y_test)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07022620887151076"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees(X_train, y_train, X_test, y_test, n_estimators=50, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Neural Nets\n",
    "Use the most basic model, multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3350 samples, validate on 1117 samples\n",
      "Epoch 1/5\n",
      "3350/3350 [==============================] - 1s 366us/step - loss: 38.6974 - val_loss: 24.2731\n",
      "Epoch 2/5\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.5752 - val_loss: 23.7931\n",
      "Epoch 3/5\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.3156 - val_loss: 23.6480\n",
      "Epoch 4/5\n",
      "3350/3350 [==============================] - 1s 173us/step - loss: 18.2133 - val_loss: 23.6424\n",
      "Epoch 5/5\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.1737 - val_loss: 23.7056\n",
      "The r_squrare of this MLP is: 0.48\n",
      "(1117,) (1117,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.012844338394809252"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Over the Years\n",
    "Now I am running the models over the years (2005 to 2011, 2013, 2015). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------current test year is 2005---------\n",
      "Train on 3525 samples, validate on 1175 samples\n",
      "Epoch 1/200\n",
      "3525/3525 [==============================] - 1s 359us/step - loss: 92.2099 - val_loss: 84.4956\n",
      "Epoch 2/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 77.7263 - val_loss: 70.4645\n",
      "Epoch 3/200\n",
      "3525/3525 [==============================] - 0s 138us/step - loss: 64.5446 - val_loss: 58.4736\n",
      "Epoch 4/200\n",
      "3525/3525 [==============================] - 1s 150us/step - loss: 53.6733 - val_loss: 48.2661\n",
      "Epoch 5/200\n",
      "3525/3525 [==============================] - 1s 147us/step - loss: 43.7569 - val_loss: 38.7822\n",
      "Epoch 6/200\n",
      "3525/3525 [==============================] - 1s 148us/step - loss: 34.5016 - val_loss: 29.9246\n",
      "Epoch 7/200\n",
      "3525/3525 [==============================] - 1s 147us/step - loss: 27.4699 - val_loss: 24.9804\n",
      "Epoch 8/200\n",
      "3525/3525 [==============================] - 1s 166us/step - loss: 24.4088 - val_loss: 23.7197\n",
      "Epoch 9/200\n",
      "3525/3525 [==============================] - 1s 152us/step - loss: 23.6829 - val_loss: 23.4930\n",
      "Epoch 10/200\n",
      "3525/3525 [==============================] - 0s 138us/step - loss: 23.5686 - val_loss: 23.4773\n",
      "Epoch 11/200\n",
      "3525/3525 [==============================] - 0s 137us/step - loss: 23.5502 - val_loss: 23.4827\n",
      "Epoch 12/200\n",
      "3525/3525 [==============================] - 0s 139us/step - loss: 23.5497 - val_loss: 23.4834\n",
      "Epoch 13/200\n",
      "3525/3525 [==============================] - 1s 160us/step - loss: 23.5494 - val_loss: 23.4850\n",
      "Epoch 14/200\n",
      "3525/3525 [==============================] - 0s 130us/step - loss: 23.5531 - val_loss: 23.4867\n",
      "Epoch 15/200\n",
      "3525/3525 [==============================] - 1s 146us/step - loss: 23.5510 - val_loss: 23.4954\n",
      "Epoch 16/200\n",
      "3525/3525 [==============================] - 0s 138us/step - loss: 23.5491 - val_loss: 23.4866\n",
      "Epoch 17/200\n",
      "3525/3525 [==============================] - 0s 118us/step - loss: 23.5499 - val_loss: 23.4884\n",
      "Epoch 18/200\n",
      "3525/3525 [==============================] - 0s 118us/step - loss: 23.5510 - val_loss: 23.4906\n",
      "Epoch 19/200\n",
      "3525/3525 [==============================] - 0s 129us/step - loss: 23.5493 - val_loss: 23.4897\n",
      "Epoch 20/200\n",
      "3525/3525 [==============================] - 0s 132us/step - loss: 23.5502 - val_loss: 23.4868\n",
      "Epoch 21/200\n",
      "3525/3525 [==============================] - 0s 133us/step - loss: 23.5515 - val_loss: 23.4898\n",
      "Epoch 22/200\n",
      "3525/3525 [==============================] - 0s 133us/step - loss: 23.5493 - val_loss: 23.4874\n",
      "Epoch 23/200\n",
      "3525/3525 [==============================] - 0s 120us/step - loss: 23.5529 - val_loss: 23.4867\n",
      "Epoch 24/200\n",
      "3525/3525 [==============================] - 0s 122us/step - loss: 23.5520 - val_loss: 23.4885\n",
      "Epoch 25/200\n",
      "3525/3525 [==============================] - 0s 123us/step - loss: 23.5536 - val_loss: 23.4879\n",
      "Epoch 26/200\n",
      "3525/3525 [==============================] - 0s 141us/step - loss: 23.5529 - val_loss: 23.4877\n",
      "Epoch 27/200\n",
      "3525/3525 [==============================] - 1s 150us/step - loss: 23.5500 - val_loss: 23.4916\n",
      "Epoch 28/200\n",
      "3525/3525 [==============================] - 1s 161us/step - loss: 23.5536 - val_loss: 23.4892\n",
      "Epoch 29/200\n",
      "3525/3525 [==============================] - 1s 160us/step - loss: 23.5467 - val_loss: 23.4865\n",
      "Epoch 30/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5505 - val_loss: 23.4863\n",
      "Epoch 31/200\n",
      "3525/3525 [==============================] - 0s 127us/step - loss: 23.5476 - val_loss: 23.4854\n",
      "Epoch 32/200\n",
      "3525/3525 [==============================] - 0s 125us/step - loss: 23.5503 - val_loss: 23.4825\n",
      "Epoch 33/200\n",
      "3525/3525 [==============================] - 0s 139us/step - loss: 23.5540 - val_loss: 23.4814\n",
      "Epoch 34/200\n",
      "3525/3525 [==============================] - 1s 148us/step - loss: 23.5516 - val_loss: 23.4865\n",
      "Epoch 35/200\n",
      "3525/3525 [==============================] - 1s 153us/step - loss: 23.5503 - val_loss: 23.4859\n",
      "Epoch 36/200\n",
      "3525/3525 [==============================] - 1s 157us/step - loss: 23.5473 - val_loss: 23.4878\n",
      "Epoch 37/200\n",
      "3525/3525 [==============================] - 1s 155us/step - loss: 23.5501 - val_loss: 23.4915\n",
      "Epoch 38/200\n",
      "3525/3525 [==============================] - 0s 134us/step - loss: 23.5545 - val_loss: 23.4898\n",
      "Epoch 39/200\n",
      "3525/3525 [==============================] - 0s 135us/step - loss: 23.5512 - val_loss: 23.4904\n",
      "Epoch 40/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5575 - val_loss: 23.4907\n",
      "Epoch 41/200\n",
      "3525/3525 [==============================] - 1s 149us/step - loss: 23.5476 - val_loss: 23.4846\n",
      "Epoch 42/200\n",
      "3525/3525 [==============================] - 1s 150us/step - loss: 23.5536 - val_loss: 23.4846\n",
      "Epoch 43/200\n",
      "3525/3525 [==============================] - 1s 150us/step - loss: 23.5518 - val_loss: 23.4834\n",
      "Epoch 44/200\n",
      "3525/3525 [==============================] - 0s 129us/step - loss: 23.5523 - val_loss: 23.4848\n",
      "Epoch 45/200\n",
      "3525/3525 [==============================] - 0s 132us/step - loss: 23.5496 - val_loss: 23.4905\n",
      "Epoch 46/200\n",
      "3525/3525 [==============================] - 0s 139us/step - loss: 23.5512 - val_loss: 23.4917\n",
      "Epoch 47/200\n",
      "3525/3525 [==============================] - 0s 140us/step - loss: 23.5334 - val_loss: 23.4799\n",
      "Epoch 48/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5513 - val_loss: 23.4830\n",
      "Epoch 49/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5512 - val_loss: 23.4836\n",
      "Epoch 50/200\n",
      "3525/3525 [==============================] - 0s 140us/step - loss: 23.5526 - val_loss: 23.4853\n",
      "Epoch 51/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5508 - val_loss: 23.4849\n",
      "Epoch 52/200\n",
      "3525/3525 [==============================] - 1s 146us/step - loss: 23.5496 - val_loss: 23.4882\n",
      "Epoch 53/200\n",
      "3525/3525 [==============================] - 1s 144us/step - loss: 23.5503 - val_loss: 23.4901\n",
      "Epoch 54/200\n",
      "3525/3525 [==============================] - 1s 146us/step - loss: 23.5513 - val_loss: 23.4909\n",
      "Epoch 55/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5507 - val_loss: 23.4881\n",
      "Epoch 56/200\n",
      "3525/3525 [==============================] - 1s 148us/step - loss: 23.5485 - val_loss: 23.4874\n",
      "Epoch 57/200\n",
      "3525/3525 [==============================] - 1s 153us/step - loss: 23.5507 - val_loss: 23.4910\n",
      "Epoch 58/200\n",
      "3525/3525 [==============================] - 1s 165us/step - loss: 23.5496 - val_loss: 23.4922\n",
      "Epoch 59/200\n",
      "3525/3525 [==============================] - 1s 163us/step - loss: 23.5505 - val_loss: 23.4923\n",
      "Epoch 60/200\n",
      "3525/3525 [==============================] - 1s 161us/step - loss: 23.5501 - val_loss: 23.4889\n",
      "Epoch 61/200\n",
      "3525/3525 [==============================] - 1s 164us/step - loss: 23.5495 - val_loss: 23.4951\n",
      "Epoch 62/200\n",
      "3525/3525 [==============================] - 1s 161us/step - loss: 23.5484 - val_loss: 23.4872\n",
      "Epoch 63/200\n",
      "3525/3525 [==============================] - 0s 129us/step - loss: 23.5509 - val_loss: 23.4901\n",
      "Epoch 64/200\n",
      "3525/3525 [==============================] - 0s 122us/step - loss: 23.5490 - val_loss: 23.4921\n",
      "Epoch 65/200\n",
      "3525/3525 [==============================] - 0s 120us/step - loss: 23.5466 - val_loss: 23.4866\n",
      "Epoch 66/200\n",
      "3525/3525 [==============================] - 0s 128us/step - loss: 23.5502 - val_loss: 23.4887\n",
      "Epoch 67/200\n",
      "3525/3525 [==============================] - 0s 136us/step - loss: 23.5487 - val_loss: 23.4927\n",
      "Epoch 68/200\n",
      "3525/3525 [==============================] - 0s 139us/step - loss: 23.5360 - val_loss: 23.4816\n",
      "Epoch 69/200\n",
      "3525/3525 [==============================] - 1s 160us/step - loss: 23.5477 - val_loss: 23.4833\n",
      "Epoch 70/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5495 - val_loss: 23.4874\n",
      "Epoch 71/200\n",
      "3525/3525 [==============================] - 1s 161us/step - loss: 23.5529 - val_loss: 23.4871\n",
      "Epoch 72/200\n",
      "3525/3525 [==============================] - 1s 165us/step - loss: 23.5544 - val_loss: 23.4894\n",
      "Epoch 73/200\n",
      "3525/3525 [==============================] - 0s 135us/step - loss: 23.5441 - val_loss: 23.4885\n",
      "Epoch 74/200\n",
      "3525/3525 [==============================] - 0s 126us/step - loss: 23.5537 - val_loss: 23.4858\n",
      "Epoch 75/200\n",
      "3525/3525 [==============================] - 0s 130us/step - loss: 23.5570 - val_loss: 23.4868\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3525/3525 [==============================] - 0s 137us/step - loss: 23.5505 - val_loss: 23.4888\n",
      "Epoch 77/200\n",
      "3525/3525 [==============================] - 0s 140us/step - loss: 23.5491 - val_loss: 23.4888\n",
      "Epoch 78/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5508 - val_loss: 23.4917\n",
      "Epoch 79/200\n",
      "3525/3525 [==============================] - 1s 144us/step - loss: 23.5486 - val_loss: 23.4880\n",
      "Epoch 80/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5555 - val_loss: 23.4868\n",
      "Epoch 81/200\n",
      "3525/3525 [==============================] - 1s 144us/step - loss: 23.5486 - val_loss: 23.4878\n",
      "Epoch 82/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5491 - val_loss: 23.4896\n",
      "Epoch 83/200\n",
      "3525/3525 [==============================] - 0s 126us/step - loss: 23.5493 - val_loss: 23.4925\n",
      "Epoch 84/200\n",
      "3525/3525 [==============================] - 1s 146us/step - loss: 23.5457 - val_loss: 23.4865\n",
      "Epoch 85/200\n",
      "3525/3525 [==============================] - 1s 144us/step - loss: 23.5505 - val_loss: 23.4872\n",
      "Epoch 86/200\n",
      "3525/3525 [==============================] - 0s 137us/step - loss: 23.5518 - val_loss: 23.4834\n",
      "Epoch 87/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5512 - val_loss: 23.4867\n",
      "Epoch 88/200\n",
      "3525/3525 [==============================] - 0s 141us/step - loss: 23.5476 - val_loss: 23.4883\n",
      "Epoch 89/200\n",
      "3525/3525 [==============================] - 1s 144us/step - loss: 23.5499 - val_loss: 23.4877\n",
      "Epoch 90/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5486 - val_loss: 23.4846\n",
      "Epoch 91/200\n",
      "3525/3525 [==============================] - 0s 136us/step - loss: 23.5531 - val_loss: 23.4826\n",
      "Epoch 92/200\n",
      "3525/3525 [==============================] - 0s 135us/step - loss: 23.5488 - val_loss: 23.4842\n",
      "Epoch 93/200\n",
      "3525/3525 [==============================] - 0s 137us/step - loss: 23.5495 - val_loss: 23.4853\n",
      "Epoch 94/200\n",
      "3525/3525 [==============================] - 1s 148us/step - loss: 23.5504 - val_loss: 23.4865\n",
      "Epoch 95/200\n",
      "3525/3525 [==============================] - 1s 149us/step - loss: 23.5513 - val_loss: 23.4831\n",
      "Epoch 96/200\n",
      "3525/3525 [==============================] - 0s 128us/step - loss: 23.5513 - val_loss: 23.4842\n",
      "Epoch 97/200\n",
      "3525/3525 [==============================] - 0s 141us/step - loss: 23.5524 - val_loss: 23.4855\n",
      "Epoch 98/200\n",
      "3525/3525 [==============================] - 0s 139us/step - loss: 23.5487 - val_loss: 23.4865\n",
      "Epoch 99/200\n",
      "3525/3525 [==============================] - 1s 158us/step - loss: 23.5582 - val_loss: 23.4858\n",
      "Epoch 100/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5479 - val_loss: 23.4881\n",
      "Epoch 101/200\n",
      "3525/3525 [==============================] - 1s 165us/step - loss: 23.5485 - val_loss: 23.4878\n",
      "Epoch 102/200\n",
      "3525/3525 [==============================] - 1s 168us/step - loss: 23.5485 - val_loss: 23.4868\n",
      "Epoch 103/200\n",
      "3525/3525 [==============================] - 1s 163us/step - loss: 23.5522 - val_loss: 23.4866\n",
      "Epoch 104/200\n",
      "3525/3525 [==============================] - 0s 128us/step - loss: 23.5494 - val_loss: 23.4838\n",
      "Epoch 105/200\n",
      "3525/3525 [==============================] - 0s 128us/step - loss: 23.5518 - val_loss: 23.4878\n",
      "Epoch 106/200\n",
      "3525/3525 [==============================] - 0s 130us/step - loss: 23.5474 - val_loss: 23.4890\n",
      "Epoch 107/200\n",
      "3525/3525 [==============================] - 0s 137us/step - loss: 23.5497 - val_loss: 23.4881\n",
      "Epoch 108/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5502 - val_loss: 23.4897\n",
      "Epoch 109/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5495 - val_loss: 23.4888\n",
      "Epoch 110/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5491 - val_loss: 23.4919\n",
      "Epoch 111/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5507 - val_loss: 23.4932\n",
      "Epoch 112/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5477 - val_loss: 23.4930\n",
      "Epoch 113/200\n",
      "3525/3525 [==============================] - 0s 141us/step - loss: 23.5513 - val_loss: 23.4934\n",
      "Epoch 114/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5502 - val_loss: 23.4940\n",
      "Epoch 115/200\n",
      "3525/3525 [==============================] - 0s 135us/step - loss: 23.5483 - val_loss: 23.4876\n",
      "Epoch 116/200\n",
      "3525/3525 [==============================] - 0s 132us/step - loss: 23.5505 - val_loss: 23.4870\n",
      "Epoch 117/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5514 - val_loss: 23.4882\n",
      "Epoch 118/200\n",
      "3525/3525 [==============================] - 0s 141us/step - loss: 23.5525 - val_loss: 23.4893\n",
      "Epoch 119/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5516 - val_loss: 23.4877\n",
      "Epoch 120/200\n",
      "3525/3525 [==============================] - 1s 147us/step - loss: 23.5505 - val_loss: 23.4863\n",
      "Epoch 121/200\n",
      "3525/3525 [==============================] - 1s 147us/step - loss: 23.5469 - val_loss: 23.4899\n",
      "Epoch 122/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5501 - val_loss: 23.4918\n",
      "Epoch 123/200\n",
      "3525/3525 [==============================] - 1s 144us/step - loss: 23.5491 - val_loss: 23.4920\n",
      "Epoch 124/200\n",
      "3525/3525 [==============================] - 1s 157us/step - loss: 23.5497 - val_loss: 23.4868\n",
      "Epoch 125/200\n",
      "3525/3525 [==============================] - 1s 156us/step - loss: 23.5492 - val_loss: 23.4895\n",
      "Epoch 126/200\n",
      "3525/3525 [==============================] - 1s 170us/step - loss: 23.5498 - val_loss: 23.4895\n",
      "Epoch 127/200\n",
      "3525/3525 [==============================] - 1s 177us/step - loss: 23.5459 - val_loss: 23.4910\n",
      "Epoch 128/200\n",
      "3525/3525 [==============================] - 0s 137us/step - loss: 23.5476 - val_loss: 23.4925\n",
      "Epoch 129/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5490 - val_loss: 23.4904\n",
      "Epoch 130/200\n",
      "3525/3525 [==============================] - 1s 168us/step - loss: 23.5449 - val_loss: 23.4857\n",
      "Epoch 131/200\n",
      "3525/3525 [==============================] - 1s 164us/step - loss: 23.5494 - val_loss: 23.4840\n",
      "Epoch 132/200\n",
      "3525/3525 [==============================] - 1s 149us/step - loss: 23.5503 - val_loss: 23.4863\n",
      "Epoch 133/200\n",
      "3525/3525 [==============================] - 1s 148us/step - loss: 23.5517 - val_loss: 23.4855\n",
      "Epoch 134/200\n",
      "3525/3525 [==============================] - 1s 147us/step - loss: 23.5500 - val_loss: 23.4867\n",
      "Epoch 135/200\n",
      "3525/3525 [==============================] - 1s 150us/step - loss: 23.5507 - val_loss: 23.4882\n",
      "Epoch 136/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5530 - val_loss: 23.4873\n",
      "Epoch 137/200\n",
      "3525/3525 [==============================] - 1s 148us/step - loss: 23.5466 - val_loss: 23.4827\n",
      "Epoch 138/200\n",
      "3525/3525 [==============================] - 0s 122us/step - loss: 23.5469 - val_loss: 23.4852\n",
      "Epoch 139/200\n",
      "3525/3525 [==============================] - 0s 133us/step - loss: 23.5478 - val_loss: 23.4887\n",
      "Epoch 140/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5489 - val_loss: 23.4886\n",
      "Epoch 141/200\n",
      "3525/3525 [==============================] - 1s 150us/step - loss: 23.5503 - val_loss: 23.4862\n",
      "Epoch 142/200\n",
      "3525/3525 [==============================] - 0s 142us/step - loss: 23.5488 - val_loss: 23.4903\n",
      "Epoch 143/200\n",
      "3525/3525 [==============================] - 1s 147us/step - loss: 23.5515 - val_loss: 23.4862\n",
      "Epoch 144/200\n",
      "3525/3525 [==============================] - 1s 143us/step - loss: 23.5488 - val_loss: 23.4894\n",
      "Epoch 145/200\n",
      "3525/3525 [==============================] - 1s 144us/step - loss: 23.5448 - val_loss: 23.4862\n",
      "Epoch 146/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5520 - val_loss: 23.4873\n",
      "Epoch 147/200\n",
      "3525/3525 [==============================] - 1s 145us/step - loss: 23.5508 - val_loss: 23.4862\n",
      "Epoch 148/200\n",
      "3525/3525 [==============================] - 1s 146us/step - loss: 23.5482 - val_loss: 23.4864\n",
      "Epoch 149/200\n",
      "3525/3525 [==============================] - 1s 152us/step - loss: 23.5533 - val_loss: 23.4886\n",
      "Epoch 150/200\n",
      "3525/3525 [==============================] - 0s 138us/step - loss: 23.5477 - val_loss: 23.4903\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3525/3525 [==============================] - 0s 142us/step - loss: 23.5506 - val_loss: 23.4904\n",
      "Epoch 152/200\n",
      "3525/3525 [==============================] - 1s 153us/step - loss: 23.5490 - val_loss: 23.4904\n",
      "Epoch 153/200\n",
      "3525/3525 [==============================] - 1s 155us/step - loss: 23.5522 - val_loss: 23.4921\n",
      "Epoch 154/200\n",
      "3525/3525 [==============================] - 1s 154us/step - loss: 23.5499 - val_loss: 23.4908\n",
      "Epoch 155/200\n",
      "3525/3525 [==============================] - 1s 150us/step - loss: 23.5519 - val_loss: 23.4933\n",
      "Epoch 156/200\n",
      "3525/3525 [==============================] - 0s 116us/step - loss: 23.5485 - val_loss: 23.4911\n",
      "Epoch 157/200\n",
      "3525/3525 [==============================] - 0s 138us/step - loss: 23.5501 - val_loss: 23.4893\n",
      "Epoch 158/200\n",
      "3525/3525 [==============================] - 0s 133us/step - loss: 23.5510 - val_loss: 23.4892\n",
      "Epoch 159/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5467 - val_loss: 23.4864\n",
      "Epoch 160/200\n",
      "3525/3525 [==============================] - 0s 138us/step - loss: 23.5494 - val_loss: 23.4867\n",
      "Epoch 161/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5509 - val_loss: 23.4896\n",
      "Epoch 162/200\n",
      "3525/3525 [==============================] - 0s 139us/step - loss: 23.5481 - val_loss: 23.4923\n",
      "Epoch 163/200\n",
      "3525/3525 [==============================] - 1s 155us/step - loss: 23.5515 - val_loss: 23.4906\n",
      "Epoch 164/200\n",
      "3525/3525 [==============================] - 1s 152us/step - loss: 23.5492 - val_loss: 23.4909\n",
      "Epoch 165/200\n",
      "3525/3525 [==============================] - 1s 159us/step - loss: 23.5502 - val_loss: 23.4891\n",
      "Epoch 166/200\n",
      "3525/3525 [==============================] - 1s 153us/step - loss: 23.5495 - val_loss: 23.4924\n",
      "Epoch 167/200\n",
      "3525/3525 [==============================] - 1s 159us/step - loss: 23.5469 - val_loss: 23.4918\n",
      "Epoch 168/200\n",
      "3525/3525 [==============================] - 1s 153us/step - loss: 23.5470 - val_loss: 23.4906\n",
      "Epoch 169/200\n",
      "3525/3525 [==============================] - 1s 156us/step - loss: 23.5516 - val_loss: 23.4873\n",
      "Epoch 170/200\n",
      "3525/3525 [==============================] - 1s 155us/step - loss: 23.5475 - val_loss: 23.4890\n",
      "Epoch 171/200\n",
      "3525/3525 [==============================] - 1s 154us/step - loss: 23.5390 - val_loss: 23.4867\n",
      "Epoch 172/200\n",
      "3525/3525 [==============================] - 1s 156us/step - loss: 23.5530 - val_loss: 23.4854\n",
      "Epoch 173/200\n",
      "3525/3525 [==============================] - 0s 124us/step - loss: 23.5467 - val_loss: 23.4890\n",
      "Epoch 174/200\n",
      "3525/3525 [==============================] - 0s 116us/step - loss: 23.5486 - val_loss: 23.4870\n",
      "Epoch 175/200\n",
      "3525/3525 [==============================] - 0s 118us/step - loss: 23.5492 - val_loss: 23.4892\n",
      "Epoch 176/200\n",
      "3525/3525 [==============================] - 0s 118us/step - loss: 23.5457 - val_loss: 23.4840\n",
      "Epoch 177/200\n",
      "3525/3525 [==============================] - 0s 122us/step - loss: 23.5481 - val_loss: 23.4858\n",
      "Epoch 178/200\n",
      "3525/3525 [==============================] - 0s 128us/step - loss: 23.5475 - val_loss: 23.4892\n",
      "Epoch 179/200\n",
      "3525/3525 [==============================] - 1s 142us/step - loss: 23.5477 - val_loss: 23.4880\n",
      "Epoch 180/200\n",
      "3525/3525 [==============================] - 1s 157us/step - loss: 23.5483 - val_loss: 23.4889\n",
      "Epoch 181/200\n",
      "3525/3525 [==============================] - 1s 160us/step - loss: 23.5466 - val_loss: 23.4868\n",
      "Epoch 182/200\n",
      "3525/3525 [==============================] - 1s 159us/step - loss: 23.5475 - val_loss: 23.4872\n",
      "Epoch 183/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5511 - val_loss: 23.4920\n",
      "Epoch 184/200\n",
      "3525/3525 [==============================] - 1s 164us/step - loss: 23.5497 - val_loss: 23.4868\n",
      "Epoch 185/200\n",
      "3525/3525 [==============================] - 0s 131us/step - loss: 23.5462 - val_loss: 23.4878\n",
      "Epoch 186/200\n",
      "3525/3525 [==============================] - 0s 113us/step - loss: 23.5540 - val_loss: 23.4907\n",
      "Epoch 187/200\n",
      "3525/3525 [==============================] - 0s 125us/step - loss: 23.5486 - val_loss: 23.4902\n",
      "Epoch 188/200\n",
      "3525/3525 [==============================] - 0s 128us/step - loss: 23.5458 - val_loss: 23.4907\n",
      "Epoch 189/200\n",
      "3525/3525 [==============================] - 1s 146us/step - loss: 23.5479 - val_loss: 23.4917\n",
      "Epoch 190/200\n",
      "3525/3525 [==============================] - 1s 157us/step - loss: 23.5478 - val_loss: 23.4909\n",
      "Epoch 191/200\n",
      "3525/3525 [==============================] - 1s 163us/step - loss: 23.5472 - val_loss: 23.4912\n",
      "Epoch 192/200\n",
      "3525/3525 [==============================] - 0s 125us/step - loss: 23.5492 - val_loss: 23.4907\n",
      "Epoch 193/200\n",
      "3525/3525 [==============================] - 1s 153us/step - loss: 23.5503 - val_loss: 23.4900\n",
      "Epoch 194/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5488 - val_loss: 23.4923\n",
      "Epoch 195/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5500 - val_loss: 23.4906\n",
      "Epoch 196/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5497 - val_loss: 23.4915\n",
      "Epoch 197/200\n",
      "3525/3525 [==============================] - 1s 161us/step - loss: 23.5495 - val_loss: 23.4860\n",
      "Epoch 198/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5498 - val_loss: 23.4860\n",
      "Epoch 199/200\n",
      "3525/3525 [==============================] - 1s 162us/step - loss: 23.5505 - val_loss: 23.4861\n",
      "Epoch 200/200\n",
      "3525/3525 [==============================] - 1s 157us/step - loss: 23.5465 - val_loss: 23.4920\n",
      "---------current test year is 2006---------\n",
      "Train on 3708 samples, validate on 1236 samples\n",
      "Epoch 1/200\n",
      "3708/3708 [==============================] - 1s 302us/step - loss: 38.0553 - val_loss: 20.1955\n",
      "Epoch 2/200\n",
      "3708/3708 [==============================] - 0s 116us/step - loss: 20.9439 - val_loss: 20.2643\n",
      "Epoch 3/200\n",
      "3708/3708 [==============================] - 0s 123us/step - loss: 20.9426 - val_loss: 20.2042\n",
      "Epoch 4/200\n",
      "3708/3708 [==============================] - 1s 139us/step - loss: 20.9612 - val_loss: 20.2017\n",
      "Epoch 5/200\n",
      "3708/3708 [==============================] - 1s 169us/step - loss: 20.9537 - val_loss: 20.2232\n",
      "Epoch 6/200\n",
      "3708/3708 [==============================] - 1s 138us/step - loss: 20.9618 - val_loss: 20.1909\n",
      "Epoch 7/200\n",
      "3708/3708 [==============================] - 1s 154us/step - loss: 20.9669 - val_loss: 20.1931\n",
      "Epoch 8/200\n",
      "3708/3708 [==============================] - 1s 148us/step - loss: 20.9426 - val_loss: 20.3154\n",
      "Epoch 9/200\n",
      "3708/3708 [==============================] - 1s 152us/step - loss: 20.9773 - val_loss: 20.1892\n",
      "Epoch 10/200\n",
      "3708/3708 [==============================] - 1s 150us/step - loss: 20.9431 - val_loss: 20.2580\n",
      "Epoch 11/200\n",
      "3708/3708 [==============================] - 0s 133us/step - loss: 20.9722 - val_loss: 20.1892\n",
      "Epoch 12/200\n",
      "3708/3708 [==============================] - 0s 125us/step - loss: 20.9409 - val_loss: 20.2735\n",
      "Epoch 13/200\n",
      "3708/3708 [==============================] - 1s 151us/step - loss: 20.9717 - val_loss: 20.2101\n",
      "Epoch 14/200\n",
      "3708/3708 [==============================] - 1s 158us/step - loss: 20.9541 - val_loss: 20.1900\n",
      "Epoch 15/200\n",
      "3708/3708 [==============================] - 1s 156us/step - loss: 20.8956 - val_loss: 20.2007\n",
      "Epoch 16/200\n",
      "3708/3708 [==============================] - 1s 157us/step - loss: 20.9651 - val_loss: 20.1897\n",
      "Epoch 17/200\n",
      "3708/3708 [==============================] - 0s 124us/step - loss: 20.9551 - val_loss: 20.2976\n",
      "Epoch 18/200\n",
      "3708/3708 [==============================] - 0s 128us/step - loss: 20.9506 - val_loss: 20.1878\n",
      "Epoch 19/200\n",
      "3708/3708 [==============================] - 0s 128us/step - loss: 20.9473 - val_loss: 20.2591\n",
      "Epoch 20/200\n",
      "3708/3708 [==============================] - 1s 141us/step - loss: 20.9187 - val_loss: 20.1879\n",
      "Epoch 21/200\n",
      "3708/3708 [==============================] - 1s 152us/step - loss: 20.9671 - val_loss: 20.1942\n",
      "Epoch 22/200\n",
      "3708/3708 [==============================] - 1s 159us/step - loss: 20.9437 - val_loss: 20.2017\n",
      "Epoch 23/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9441 - val_loss: 20.1930\n",
      "Epoch 24/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9524 - val_loss: 20.1981\n",
      "Epoch 25/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9610 - val_loss: 20.1891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "3708/3708 [==============================] - 1s 144us/step - loss: 20.9377 - val_loss: 20.2165\n",
      "Epoch 27/200\n",
      "3708/3708 [==============================] - 1s 153us/step - loss: 20.9596 - val_loss: 20.2013\n",
      "Epoch 28/200\n",
      "3708/3708 [==============================] - 1s 154us/step - loss: 20.9426 - val_loss: 20.2164\n",
      "Epoch 29/200\n",
      "3708/3708 [==============================] - 1s 154us/step - loss: 20.9545 - val_loss: 20.1984\n",
      "Epoch 30/200\n",
      "3708/3708 [==============================] - 1s 154us/step - loss: 20.9496 - val_loss: 20.1886\n",
      "Epoch 31/200\n",
      "3708/3708 [==============================] - 1s 159us/step - loss: 20.9505 - val_loss: 20.1948\n",
      "Epoch 32/200\n",
      "3708/3708 [==============================] - 1s 159us/step - loss: 20.9613 - val_loss: 20.1885\n",
      "Epoch 33/200\n",
      "3708/3708 [==============================] - 1s 149us/step - loss: 20.9432 - val_loss: 20.2074\n",
      "Epoch 34/200\n",
      "3708/3708 [==============================] - 1s 137us/step - loss: 20.9564 - val_loss: 20.2113\n",
      "Epoch 35/200\n",
      "3708/3708 [==============================] - 1s 147us/step - loss: 20.9259 - val_loss: 20.2292\n",
      "Epoch 36/200\n",
      "3708/3708 [==============================] - 1s 148us/step - loss: 20.9290 - val_loss: 20.2296\n",
      "Epoch 37/200\n",
      "3708/3708 [==============================] - 1s 153us/step - loss: 20.9474 - val_loss: 20.1958\n",
      "Epoch 38/200\n",
      "3708/3708 [==============================] - 1s 151us/step - loss: 20.9218 - val_loss: 20.5759\n",
      "Epoch 39/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9719 - val_loss: 20.3023\n",
      "Epoch 40/200\n",
      "3708/3708 [==============================] - 1s 156us/step - loss: 20.9558 - val_loss: 20.1982\n",
      "Epoch 41/200\n",
      "3708/3708 [==============================] - 1s 158us/step - loss: 20.9573 - val_loss: 20.1951\n",
      "Epoch 42/200\n",
      "3708/3708 [==============================] - 1s 151us/step - loss: 20.9372 - val_loss: 20.2159\n",
      "Epoch 43/200\n",
      "3708/3708 [==============================] - 1s 153us/step - loss: 20.9415 - val_loss: 20.2238\n",
      "Epoch 44/200\n",
      "3708/3708 [==============================] - 1s 151us/step - loss: 20.9491 - val_loss: 20.2023\n",
      "Epoch 45/200\n",
      "3708/3708 [==============================] - 1s 136us/step - loss: 20.9094 - val_loss: 20.1975\n",
      "Epoch 46/200\n",
      "3708/3708 [==============================] - 0s 129us/step - loss: 20.9335 - val_loss: 20.2057\n",
      "Epoch 47/200\n",
      "3708/3708 [==============================] - 0s 131us/step - loss: 20.9255 - val_loss: 20.2057\n",
      "Epoch 48/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9640 - val_loss: 20.2032\n",
      "Epoch 49/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9406 - val_loss: 20.1949\n",
      "Epoch 50/200\n",
      "3708/3708 [==============================] - 0s 130us/step - loss: 20.9387 - val_loss: 20.1966\n",
      "Epoch 51/200\n",
      "3708/3708 [==============================] - 1s 152us/step - loss: 20.9563 - val_loss: 20.4241\n",
      "Epoch 52/200\n",
      "3708/3708 [==============================] - 1s 157us/step - loss: 20.9394 - val_loss: 20.1956\n",
      "Epoch 53/200\n",
      "3708/3708 [==============================] - 1s 157us/step - loss: 20.9258 - val_loss: 20.1957\n",
      "Epoch 54/200\n",
      "3708/3708 [==============================] - 1s 159us/step - loss: 20.9264 - val_loss: 20.2312\n",
      "Epoch 55/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9472 - val_loss: 20.2018\n",
      "Epoch 56/200\n",
      "3708/3708 [==============================] - 0s 126us/step - loss: 20.9127 - val_loss: 20.3219\n",
      "Epoch 57/200\n",
      "3708/3708 [==============================] - 1s 138us/step - loss: 20.9311 - val_loss: 20.1943\n",
      "Epoch 58/200\n",
      "3708/3708 [==============================] - 1s 153us/step - loss: 20.9460 - val_loss: 20.2488\n",
      "Epoch 59/200\n",
      "3708/3708 [==============================] - 1s 149us/step - loss: 20.9191 - val_loss: 20.3025\n",
      "Epoch 60/200\n",
      "3708/3708 [==============================] - 1s 139us/step - loss: 20.9624 - val_loss: 20.2290\n",
      "Epoch 61/200\n",
      "3708/3708 [==============================] - 1s 138us/step - loss: 20.9368 - val_loss: 20.2235\n",
      "Epoch 62/200\n",
      "3708/3708 [==============================] - 1s 138us/step - loss: 20.9365 - val_loss: 20.2030\n",
      "Epoch 63/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.8637 - val_loss: 20.2708\n",
      "Epoch 64/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9246 - val_loss: 20.2085\n",
      "Epoch 65/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9410 - val_loss: 20.1967\n",
      "Epoch 66/200\n",
      "3708/3708 [==============================] - 1s 147us/step - loss: 20.9361 - val_loss: 20.2332\n",
      "Epoch 67/200\n",
      "3708/3708 [==============================] - 0s 131us/step - loss: 20.9165 - val_loss: 20.2014\n",
      "Epoch 68/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9615 - val_loss: 20.2117\n",
      "Epoch 69/200\n",
      "3708/3708 [==============================] - 1s 135us/step - loss: 20.9506 - val_loss: 20.2052\n",
      "Epoch 70/200\n",
      "3708/3708 [==============================] - 1s 147us/step - loss: 20.9365 - val_loss: 20.3685\n",
      "Epoch 71/200\n",
      "3708/3708 [==============================] - 1s 164us/step - loss: 20.9387 - val_loss: 20.2252\n",
      "Epoch 72/200\n",
      "3708/3708 [==============================] - 1s 153us/step - loss: 20.9216 - val_loss: 20.2159\n",
      "Epoch 73/200\n",
      "3708/3708 [==============================] - 0s 127us/step - loss: 20.9475 - val_loss: 20.2242\n",
      "Epoch 74/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9356 - val_loss: 20.2064\n",
      "Epoch 75/200\n",
      "3708/3708 [==============================] - 1s 157us/step - loss: 20.9541 - val_loss: 20.2358\n",
      "Epoch 76/200\n",
      "3708/3708 [==============================] - 1s 159us/step - loss: 20.9404 - val_loss: 20.2170\n",
      "Epoch 77/200\n",
      "3708/3708 [==============================] - 1s 158us/step - loss: 20.9428 - val_loss: 20.2134\n",
      "Epoch 78/200\n",
      "3708/3708 [==============================] - 0s 129us/step - loss: 20.9446 - val_loss: 20.2046\n",
      "Epoch 79/200\n",
      "3708/3708 [==============================] - 0s 125us/step - loss: 20.9489 - val_loss: 20.2596\n",
      "Epoch 80/200\n",
      "3708/3708 [==============================] - 1s 137us/step - loss: 20.9401 - val_loss: 20.2302\n",
      "Epoch 81/200\n",
      "3708/3708 [==============================] - 1s 142us/step - loss: 20.9240 - val_loss: 20.2495\n",
      "Epoch 82/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9478 - val_loss: 20.2414\n",
      "Epoch 83/200\n",
      "3708/3708 [==============================] - 1s 144us/step - loss: 20.9556 - val_loss: 20.1954\n",
      "Epoch 84/200\n",
      "3708/3708 [==============================] - 0s 131us/step - loss: 20.9166 - val_loss: 20.2164\n",
      "Epoch 85/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9381 - val_loss: 20.2077\n",
      "Epoch 86/200\n",
      "3708/3708 [==============================] - 1s 148us/step - loss: 20.9483 - val_loss: 20.2155\n",
      "Epoch 87/200\n",
      "3708/3708 [==============================] - 1s 154us/step - loss: 20.9233 - val_loss: 20.3334\n",
      "Epoch 88/200\n",
      "3708/3708 [==============================] - 1s 149us/step - loss: 20.9478 - val_loss: 20.2011\n",
      "Epoch 89/200\n",
      "3708/3708 [==============================] - 1s 142us/step - loss: 20.9442 - val_loss: 20.2075\n",
      "Epoch 90/200\n",
      "3708/3708 [==============================] - 0s 124us/step - loss: 20.9392 - val_loss: 20.2768\n",
      "Epoch 91/200\n",
      "3708/3708 [==============================] - 1s 144us/step - loss: 20.9285 - val_loss: 20.2151\n",
      "Epoch 92/200\n",
      "3708/3708 [==============================] - 1s 167us/step - loss: 20.9354 - val_loss: 20.1943\n",
      "Epoch 93/200\n",
      "3708/3708 [==============================] - 1s 170us/step - loss: 20.9484 - val_loss: 20.2014\n",
      "Epoch 94/200\n",
      "3708/3708 [==============================] - 1s 172us/step - loss: 20.9344 - val_loss: 20.1951\n",
      "Epoch 95/200\n",
      "3708/3708 [==============================] - 0s 124us/step - loss: 20.9383 - val_loss: 20.1999\n",
      "Epoch 96/200\n",
      "3708/3708 [==============================] - 0s 127us/step - loss: 20.9275 - val_loss: 20.1944\n",
      "Epoch 97/200\n",
      "3708/3708 [==============================] - 0s 127us/step - loss: 20.9168 - val_loss: 20.2515\n",
      "Epoch 98/200\n",
      "3708/3708 [==============================] - 1s 149us/step - loss: 20.9310 - val_loss: 20.2000\n",
      "Epoch 99/200\n",
      "3708/3708 [==============================] - 1s 170us/step - loss: 20.9412 - val_loss: 20.1943\n",
      "Epoch 100/200\n",
      "3708/3708 [==============================] - 1s 169us/step - loss: 20.9209 - val_loss: 20.1995\n",
      "Epoch 101/200\n",
      "3708/3708 [==============================] - 1s 167us/step - loss: 20.9394 - val_loss: 20.1973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200\n",
      "3708/3708 [==============================] - 1s 170us/step - loss: 20.9392 - val_loss: 20.2310\n",
      "Epoch 103/200\n",
      "3708/3708 [==============================] - 1s 167us/step - loss: 20.9362 - val_loss: 20.2771\n",
      "Epoch 104/200\n",
      "3708/3708 [==============================] - 1s 167us/step - loss: 20.9372 - val_loss: 20.1943\n",
      "Epoch 105/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9315 - val_loss: 20.2932\n",
      "Epoch 106/200\n",
      "3708/3708 [==============================] - 0s 118us/step - loss: 20.9506 - val_loss: 20.1972\n",
      "Epoch 107/200\n",
      "3708/3708 [==============================] - 0s 118us/step - loss: 20.9165 - val_loss: 20.2794\n",
      "Epoch 108/200\n",
      "3708/3708 [==============================] - 0s 118us/step - loss: 20.9416 - val_loss: 20.2068\n",
      "Epoch 109/200\n",
      "3708/3708 [==============================] - 0s 118us/step - loss: 20.9243 - val_loss: 20.2767\n",
      "Epoch 110/200\n",
      "3708/3708 [==============================] - 1s 135us/step - loss: 20.9393 - val_loss: 20.2102\n",
      "Epoch 111/200\n",
      "3708/3708 [==============================] - 1s 142us/step - loss: 20.9372 - val_loss: 20.1960\n",
      "Epoch 112/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9353 - val_loss: 20.1986\n",
      "Epoch 113/200\n",
      "3708/3708 [==============================] - 1s 149us/step - loss: 20.9447 - val_loss: 20.2226\n",
      "Epoch 114/200\n",
      "3708/3708 [==============================] - 1s 176us/step - loss: 20.8973 - val_loss: 20.4775\n",
      "Epoch 115/200\n",
      "3708/3708 [==============================] - 1s 178us/step - loss: 20.9637 - val_loss: 20.2389\n",
      "Epoch 116/200\n",
      "3708/3708 [==============================] - 1s 171us/step - loss: 20.9443 - val_loss: 20.2078\n",
      "Epoch 117/200\n",
      "3708/3708 [==============================] - 0s 120us/step - loss: 20.9132 - val_loss: 20.2066\n",
      "Epoch 118/200\n",
      "3708/3708 [==============================] - 0s 122us/step - loss: 20.9330 - val_loss: 20.1939\n",
      "Epoch 119/200\n",
      "3708/3708 [==============================] - 0s 124us/step - loss: 20.9407 - val_loss: 20.2005\n",
      "Epoch 120/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9526 - val_loss: 20.2061\n",
      "Epoch 121/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9223 - val_loss: 20.1928\n",
      "Epoch 122/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9213 - val_loss: 20.1997\n",
      "Epoch 123/200\n",
      "3708/3708 [==============================] - 1s 138us/step - loss: 20.9136 - val_loss: 20.2450\n",
      "Epoch 124/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9264 - val_loss: 20.2047\n",
      "Epoch 125/200\n",
      "3708/3708 [==============================] - 1s 138us/step - loss: 20.9543 - val_loss: 20.2436\n",
      "Epoch 126/200\n",
      "3708/3708 [==============================] - 1s 151us/step - loss: 20.9384 - val_loss: 20.2476\n",
      "Epoch 127/200\n",
      "3708/3708 [==============================] - 1s 156us/step - loss: 20.9258 - val_loss: 20.2006\n",
      "Epoch 128/200\n",
      "3708/3708 [==============================] - 1s 144us/step - loss: 20.9204 - val_loss: 20.2129\n",
      "Epoch 129/200\n",
      "3708/3708 [==============================] - 0s 122us/step - loss: 20.9335 - val_loss: 20.2673\n",
      "Epoch 130/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9418 - val_loss: 20.1992\n",
      "Epoch 131/200\n",
      "3708/3708 [==============================] - 1s 167us/step - loss: 20.8952 - val_loss: 20.2174\n",
      "Epoch 132/200\n",
      "3708/3708 [==============================] - 1s 170us/step - loss: 20.9527 - val_loss: 20.1954\n",
      "Epoch 133/200\n",
      "3708/3708 [==============================] - 1s 171us/step - loss: 20.9346 - val_loss: 20.3056\n",
      "Epoch 134/200\n",
      "3708/3708 [==============================] - 0s 121us/step - loss: 20.9261 - val_loss: 20.3490\n",
      "Epoch 135/200\n",
      "3708/3708 [==============================] - 0s 125us/step - loss: 20.9332 - val_loss: 20.2044\n",
      "Epoch 136/200\n",
      "3708/3708 [==============================] - 0s 134us/step - loss: 20.9247 - val_loss: 20.2352\n",
      "Epoch 137/200\n",
      "3708/3708 [==============================] - 1s 151us/step - loss: 20.9334 - val_loss: 20.2208\n",
      "Epoch 138/200\n",
      "3708/3708 [==============================] - 1s 156us/step - loss: 20.9178 - val_loss: 20.2197\n",
      "Epoch 139/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9433 - val_loss: 20.1918\n",
      "Epoch 140/200\n",
      "3708/3708 [==============================] - 0s 125us/step - loss: 20.9393 - val_loss: 20.1968\n",
      "Epoch 141/200\n",
      "3708/3708 [==============================] - 0s 128us/step - loss: 20.9241 - val_loss: 20.1999\n",
      "Epoch 142/200\n",
      "3708/3708 [==============================] - 1s 144us/step - loss: 20.9214 - val_loss: 20.2005\n",
      "Epoch 143/200\n",
      "3708/3708 [==============================] - 1s 168us/step - loss: 20.9121 - val_loss: 20.2013\n",
      "Epoch 144/200\n",
      "3708/3708 [==============================] - 1s 179us/step - loss: 20.9261 - val_loss: 20.2136\n",
      "Epoch 145/200\n",
      "3708/3708 [==============================] - 0s 131us/step - loss: 20.9468 - val_loss: 20.2422\n",
      "Epoch 146/200\n",
      "3708/3708 [==============================] - 0s 130us/step - loss: 20.9143 - val_loss: 20.2093\n",
      "Epoch 147/200\n",
      "3708/3708 [==============================] - 1s 148us/step - loss: 20.9407 - val_loss: 20.2065\n",
      "Epoch 148/200\n",
      "3708/3708 [==============================] - 1s 170us/step - loss: 20.9305 - val_loss: 20.2180\n",
      "Epoch 149/200\n",
      "3708/3708 [==============================] - 1s 165us/step - loss: 20.9385 - val_loss: 20.1982\n",
      "Epoch 150/200\n",
      "3708/3708 [==============================] - 1s 173us/step - loss: 20.9402 - val_loss: 20.2402\n",
      "Epoch 151/200\n",
      "3708/3708 [==============================] - 1s 159us/step - loss: 20.9165 - val_loss: 20.2587\n",
      "Epoch 152/200\n",
      "3708/3708 [==============================] - 1s 147us/step - loss: 20.9389 - val_loss: 20.2087\n",
      "Epoch 153/200\n",
      "3708/3708 [==============================] - 1s 173us/step - loss: 20.9138 - val_loss: 20.2406\n",
      "Epoch 154/200\n",
      "3708/3708 [==============================] - 1s 175us/step - loss: 20.9241 - val_loss: 20.2029\n",
      "Epoch 155/200\n",
      "3708/3708 [==============================] - 0s 121us/step - loss: 20.9197 - val_loss: 20.3089\n",
      "Epoch 156/200\n",
      "3708/3708 [==============================] - 0s 129us/step - loss: 20.9489 - val_loss: 20.2466\n",
      "Epoch 157/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9142 - val_loss: 20.1949\n",
      "Epoch 158/200\n",
      "3708/3708 [==============================] - 1s 162us/step - loss: 20.9686 - val_loss: 20.1952\n",
      "Epoch 159/200\n",
      "3708/3708 [==============================] - 1s 171us/step - loss: 20.9410 - val_loss: 20.1951\n",
      "Epoch 160/200\n",
      "3708/3708 [==============================] - 1s 138us/step - loss: 20.9234 - val_loss: 20.2084\n",
      "Epoch 161/200\n",
      "3708/3708 [==============================] - 0s 122us/step - loss: 20.9066 - val_loss: 20.2076\n",
      "Epoch 162/200\n",
      "3708/3708 [==============================] - 1s 142us/step - loss: 20.9398 - val_loss: 20.2012\n",
      "Epoch 163/200\n",
      "3708/3708 [==============================] - 1s 174us/step - loss: 20.9351 - val_loss: 20.2158\n",
      "Epoch 164/200\n",
      "3708/3708 [==============================] - 1s 178us/step - loss: 20.9369 - val_loss: 20.2077\n",
      "Epoch 165/200\n",
      "3708/3708 [==============================] - 1s 149us/step - loss: 20.9336 - val_loss: 20.2612\n",
      "Epoch 166/200\n",
      "3708/3708 [==============================] - 0s 129us/step - loss: 20.9125 - val_loss: 20.2833\n",
      "Epoch 167/200\n",
      "3708/3708 [==============================] - 0s 132us/step - loss: 20.9214 - val_loss: 20.2480\n",
      "Epoch 168/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9492 - val_loss: 20.1956\n",
      "Epoch 169/200\n",
      "3708/3708 [==============================] - 1s 147us/step - loss: 20.9245 - val_loss: 20.1990\n",
      "Epoch 170/200\n",
      "3708/3708 [==============================] - 1s 145us/step - loss: 20.9418 - val_loss: 20.2133\n",
      "Epoch 171/200\n",
      "3708/3708 [==============================] - 1s 151us/step - loss: 20.9277 - val_loss: 20.1953\n",
      "Epoch 172/200\n",
      "3708/3708 [==============================] - 1s 141us/step - loss: 20.9158 - val_loss: 20.1956\n",
      "Epoch 173/200\n",
      "3708/3708 [==============================] - 1s 142us/step - loss: 20.9221 - val_loss: 20.2044\n",
      "Epoch 174/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9347 - val_loss: 20.1949\n",
      "Epoch 175/200\n",
      "3708/3708 [==============================] - 1s 145us/step - loss: 20.9328 - val_loss: 20.2124\n",
      "Epoch 176/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9342 - val_loss: 20.2301\n",
      "Epoch 177/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9275 - val_loss: 20.1967\n",
      "Epoch 178/200\n",
      "3708/3708 [==============================] - 0s 127us/step - loss: 20.9332 - val_loss: 20.1942\n",
      "Epoch 179/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9322 - val_loss: 20.2493\n",
      "Epoch 180/200\n",
      "3708/3708 [==============================] - 1s 165us/step - loss: 20.9277 - val_loss: 20.1929\n",
      "Epoch 181/200\n",
      "3708/3708 [==============================] - 1s 162us/step - loss: 20.9109 - val_loss: 20.1934\n",
      "Epoch 182/200\n",
      "3708/3708 [==============================] - 1s 165us/step - loss: 20.9217 - val_loss: 20.1952\n",
      "Epoch 183/200\n",
      "3708/3708 [==============================] - 0s 130us/step - loss: 20.9171 - val_loss: 20.2047\n",
      "Epoch 184/200\n",
      "3708/3708 [==============================] - 0s 121us/step - loss: 20.9327 - val_loss: 20.1969\n",
      "Epoch 185/200\n",
      "3708/3708 [==============================] - 1s 140us/step - loss: 20.9321 - val_loss: 20.1936\n",
      "Epoch 186/200\n",
      "3708/3708 [==============================] - 1s 164us/step - loss: 20.9327 - val_loss: 20.1990\n",
      "Epoch 187/200\n",
      "3708/3708 [==============================] - 1s 167us/step - loss: 20.9338 - val_loss: 20.1947\n",
      "Epoch 188/200\n",
      "3708/3708 [==============================] - 1s 156us/step - loss: 20.9267 - val_loss: 20.1997\n",
      "Epoch 189/200\n",
      "3708/3708 [==============================] - 0s 124us/step - loss: 20.9288 - val_loss: 20.2396\n",
      "Epoch 190/200\n",
      "3708/3708 [==============================] - 0s 127us/step - loss: 20.9363 - val_loss: 20.1949\n",
      "Epoch 191/200\n",
      "3708/3708 [==============================] - 0s 135us/step - loss: 20.9209 - val_loss: 20.2012\n",
      "Epoch 192/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9235 - val_loss: 20.1951\n",
      "Epoch 193/200\n",
      "3708/3708 [==============================] - 1s 147us/step - loss: 20.9248 - val_loss: 20.2072\n",
      "Epoch 194/200\n",
      "3708/3708 [==============================] - 1s 141us/step - loss: 20.9349 - val_loss: 20.2158\n",
      "Epoch 195/200\n",
      "3708/3708 [==============================] - 0s 133us/step - loss: 20.9438 - val_loss: 20.2050\n",
      "Epoch 196/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9355 - val_loss: 20.1937\n",
      "Epoch 197/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9425 - val_loss: 20.1986\n",
      "Epoch 198/200\n",
      "3708/3708 [==============================] - 1s 146us/step - loss: 20.9308 - val_loss: 20.2054\n",
      "Epoch 199/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9299 - val_loss: 20.2110\n",
      "Epoch 200/200\n",
      "3708/3708 [==============================] - 1s 143us/step - loss: 20.9275 - val_loss: 20.2016\n",
      "---------current test year is 2007---------\n",
      "Train on 3630 samples, validate on 1211 samples\n",
      "Epoch 1/200\n",
      "3630/3630 [==============================] - 1s 331us/step - loss: 42.4586 - val_loss: 20.4474\n",
      "Epoch 2/200\n",
      "3630/3630 [==============================] - 1s 171us/step - loss: 22.7936 - val_loss: 20.3865\n",
      "Epoch 3/200\n",
      "3630/3630 [==============================] - 1s 160us/step - loss: 22.7794 - val_loss: 20.4902\n",
      "Epoch 4/200\n",
      "3630/3630 [==============================] - 1s 143us/step - loss: 22.7963 - val_loss: 20.3587\n",
      "Epoch 5/200\n",
      "3630/3630 [==============================] - 1s 158us/step - loss: 22.7788 - val_loss: 20.4926\n",
      "Epoch 6/200\n",
      "3630/3630 [==============================] - 1s 153us/step - loss: 22.7842 - val_loss: 20.3780\n",
      "Epoch 7/200\n",
      "3630/3630 [==============================] - 1s 148us/step - loss: 22.7819 - val_loss: 20.3990\n",
      "Epoch 8/200\n",
      "3630/3630 [==============================] - 1s 148us/step - loss: 22.7928 - val_loss: 20.4018\n",
      "Epoch 9/200\n",
      "3630/3630 [==============================] - 1s 151us/step - loss: 22.8364 - val_loss: 20.4121\n",
      "Epoch 10/200\n",
      "3630/3630 [==============================] - 1s 148us/step - loss: 22.7914 - val_loss: 20.3769\n",
      "Epoch 11/200\n",
      "3630/3630 [==============================] - 1s 151us/step - loss: 22.8025 - val_loss: 20.4778\n",
      "Epoch 12/200\n",
      "3630/3630 [==============================] - 1s 152us/step - loss: 22.7942 - val_loss: 20.5707\n",
      "Epoch 13/200\n",
      "3630/3630 [==============================] - 1s 150us/step - loss: 22.8032 - val_loss: 20.3574\n",
      "Epoch 14/200\n",
      "3630/3630 [==============================] - 1s 140us/step - loss: 22.7914 - val_loss: 20.4659\n",
      "Epoch 15/200\n",
      "3630/3630 [==============================] - 1s 145us/step - loss: 22.8084 - val_loss: 20.4421\n",
      "Epoch 16/200\n",
      "3630/3630 [==============================] - 1s 161us/step - loss: 22.7740 - val_loss: 20.3609\n",
      "Epoch 17/200\n",
      "3630/3630 [==============================] - 1s 139us/step - loss: 22.8246 - val_loss: 20.3958\n",
      "Epoch 18/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7971 - val_loss: 20.4160\n",
      "Epoch 19/200\n",
      "3630/3630 [==============================] - 1s 151us/step - loss: 22.7903 - val_loss: 20.4103\n",
      "Epoch 20/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.8264 - val_loss: 20.3660\n",
      "Epoch 21/200\n",
      "3630/3630 [==============================] - 1s 152us/step - loss: 22.8097 - val_loss: 20.3643\n",
      "Epoch 22/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.8065 - val_loss: 20.4401\n",
      "Epoch 23/200\n",
      "3630/3630 [==============================] - 1s 152us/step - loss: 22.8193 - val_loss: 20.3703\n",
      "Epoch 24/200\n",
      "3630/3630 [==============================] - 1s 155us/step - loss: 22.7795 - val_loss: 20.3696\n",
      "Epoch 25/200\n",
      "3630/3630 [==============================] - 1s 144us/step - loss: 22.8682 - val_loss: 20.3666\n",
      "Epoch 26/200\n",
      "3630/3630 [==============================] - 1s 139us/step - loss: 22.8053 - val_loss: 20.3789\n",
      "Epoch 27/200\n",
      "3630/3630 [==============================] - 1s 143us/step - loss: 22.7830 - val_loss: 20.4030\n",
      "Epoch 28/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7981 - val_loss: 20.4585\n",
      "Epoch 29/200\n",
      "3630/3630 [==============================] - 1s 170us/step - loss: 22.7915 - val_loss: 20.3810\n",
      "Epoch 30/200\n",
      "3630/3630 [==============================] - 1s 164us/step - loss: 22.8011 - val_loss: 20.3735\n",
      "Epoch 31/200\n",
      "3630/3630 [==============================] - 1s 142us/step - loss: 22.7967 - val_loss: 20.4037\n",
      "Epoch 32/200\n",
      "3630/3630 [==============================] - 1s 159us/step - loss: 22.7893 - val_loss: 20.3781\n",
      "Epoch 33/200\n",
      "3630/3630 [==============================] - 1s 160us/step - loss: 22.7882 - val_loss: 20.4232\n",
      "Epoch 34/200\n",
      "3630/3630 [==============================] - 1s 166us/step - loss: 22.8126 - val_loss: 20.3652\n",
      "Epoch 35/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7979 - val_loss: 20.4308\n",
      "Epoch 36/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.8016 - val_loss: 20.6000\n",
      "Epoch 37/200\n",
      "3630/3630 [==============================] - 0s 138us/step - loss: 22.8109 - val_loss: 20.3920\n",
      "Epoch 38/200\n",
      "3630/3630 [==============================] - 1s 163us/step - loss: 22.8136 - val_loss: 20.4589\n",
      "Epoch 39/200\n",
      "3630/3630 [==============================] - 1s 162us/step - loss: 22.8076 - val_loss: 20.3700\n",
      "Epoch 40/200\n",
      "3630/3630 [==============================] - 1s 166us/step - loss: 22.7900 - val_loss: 20.3809\n",
      "Epoch 41/200\n",
      "3630/3630 [==============================] - 1s 173us/step - loss: 22.8038 - val_loss: 20.3662\n",
      "Epoch 42/200\n",
      "3630/3630 [==============================] - 1s 176us/step - loss: 22.8026 - val_loss: 20.3695\n",
      "Epoch 43/200\n",
      "3630/3630 [==============================] - 1s 172us/step - loss: 22.8067 - val_loss: 20.4625\n",
      "Epoch 44/200\n",
      "3630/3630 [==============================] - 1s 167us/step - loss: 22.7955 - val_loss: 20.4086\n",
      "Epoch 45/200\n",
      "3630/3630 [==============================] - 1s 175us/step - loss: 22.7823 - val_loss: 20.3798\n",
      "Epoch 46/200\n",
      "3630/3630 [==============================] - 1s 164us/step - loss: 22.7868 - val_loss: 20.3648\n",
      "Epoch 47/200\n",
      "3630/3630 [==============================] - 0s 135us/step - loss: 22.8128 - val_loss: 20.4148\n",
      "Epoch 48/200\n",
      "3630/3630 [==============================] - 1s 140us/step - loss: 22.8195 - val_loss: 20.3753\n",
      "Epoch 49/200\n",
      "3630/3630 [==============================] - 0s 137us/step - loss: 22.7871 - val_loss: 20.3775\n",
      "Epoch 50/200\n",
      "3630/3630 [==============================] - 1s 145us/step - loss: 22.7964 - val_loss: 20.4684\n",
      "Epoch 51/200\n",
      "3630/3630 [==============================] - 1s 156us/step - loss: 22.8037 - val_loss: 20.3621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200\n",
      "3630/3630 [==============================] - 1s 168us/step - loss: 22.7960 - val_loss: 20.3605\n",
      "Epoch 53/200\n",
      "3630/3630 [==============================] - 1s 182us/step - loss: 22.8000 - val_loss: 20.5012\n",
      "Epoch 54/200\n",
      "3630/3630 [==============================] - 1s 184us/step - loss: 22.8045 - val_loss: 20.3631\n",
      "Epoch 55/200\n",
      "3630/3630 [==============================] - 1s 170us/step - loss: 22.7985 - val_loss: 20.3739\n",
      "Epoch 56/200\n",
      "3630/3630 [==============================] - 0s 130us/step - loss: 22.7896 - val_loss: 20.3667\n",
      "Epoch 57/200\n",
      "3630/3630 [==============================] - 0s 125us/step - loss: 22.7884 - val_loss: 20.3587\n",
      "Epoch 58/200\n",
      "3630/3630 [==============================] - 0s 130us/step - loss: 22.7946 - val_loss: 20.3625\n",
      "Epoch 59/200\n",
      "3630/3630 [==============================] - 1s 150us/step - loss: 22.7977 - val_loss: 20.3729\n",
      "Epoch 60/200\n",
      "3630/3630 [==============================] - 1s 163us/step - loss: 22.7835 - val_loss: 20.3733\n",
      "Epoch 61/200\n",
      "3630/3630 [==============================] - 1s 169us/step - loss: 22.7957 - val_loss: 20.3981\n",
      "Epoch 62/200\n",
      "3630/3630 [==============================] - 1s 177us/step - loss: 22.7567 - val_loss: 20.4643\n",
      "Epoch 63/200\n",
      "3630/3630 [==============================] - 1s 175us/step - loss: 22.8043 - val_loss: 20.3795\n",
      "Epoch 64/200\n",
      "3630/3630 [==============================] - 1s 176us/step - loss: 22.7801 - val_loss: 20.3592\n",
      "Epoch 65/200\n",
      "3630/3630 [==============================] - 1s 177us/step - loss: 22.7937 - val_loss: 20.3709\n",
      "Epoch 66/200\n",
      "3630/3630 [==============================] - 1s 177us/step - loss: 22.8074 - val_loss: 20.4002\n",
      "Epoch 67/200\n",
      "3630/3630 [==============================] - 1s 176us/step - loss: 22.7885 - val_loss: 20.4307\n",
      "Epoch 68/200\n",
      "3630/3630 [==============================] - 0s 123us/step - loss: 22.7857 - val_loss: 20.4478\n",
      "Epoch 69/200\n",
      "3630/3630 [==============================] - 0s 128us/step - loss: 22.7879 - val_loss: 20.3877\n",
      "Epoch 70/200\n",
      "3630/3630 [==============================] - 0s 124us/step - loss: 22.7936 - val_loss: 20.3797\n",
      "Epoch 71/200\n",
      "3630/3630 [==============================] - 0s 127us/step - loss: 22.7907 - val_loss: 20.4414\n",
      "Epoch 72/200\n",
      "3630/3630 [==============================] - 0s 122us/step - loss: 22.7923 - val_loss: 20.3568\n",
      "Epoch 73/200\n",
      "3630/3630 [==============================] - 0s 138us/step - loss: 22.7938 - val_loss: 20.3910\n",
      "Epoch 74/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7932 - val_loss: 20.3780\n",
      "Epoch 75/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7939 - val_loss: 20.3619\n",
      "Epoch 76/200\n",
      "3630/3630 [==============================] - 1s 154us/step - loss: 22.8112 - val_loss: 20.4120\n",
      "Epoch 77/200\n",
      "3630/3630 [==============================] - 1s 168us/step - loss: 22.7957 - val_loss: 20.4351\n",
      "Epoch 78/200\n",
      "3630/3630 [==============================] - 1s 167us/step - loss: 22.7960 - val_loss: 20.4106\n",
      "Epoch 79/200\n",
      "3630/3630 [==============================] - 1s 139us/step - loss: 22.7846 - val_loss: 20.4048\n",
      "Epoch 80/200\n",
      "3630/3630 [==============================] - 1s 142us/step - loss: 22.7982 - val_loss: 20.3674\n",
      "Epoch 81/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.7845 - val_loss: 20.4191\n",
      "Epoch 82/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7782 - val_loss: 20.5523\n",
      "Epoch 83/200\n",
      "3630/3630 [==============================] - 1s 150us/step - loss: 22.7874 - val_loss: 20.4722\n",
      "Epoch 84/200\n",
      "3630/3630 [==============================] - 1s 147us/step - loss: 22.7863 - val_loss: 20.3739\n",
      "Epoch 85/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.7754 - val_loss: 20.3717\n",
      "Epoch 86/200\n",
      "3630/3630 [==============================] - 1s 144us/step - loss: 22.8016 - val_loss: 20.4971\n",
      "Epoch 87/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.7899 - val_loss: 20.3909\n",
      "Epoch 88/200\n",
      "3630/3630 [==============================] - 1s 155us/step - loss: 22.8042 - val_loss: 20.3750\n",
      "Epoch 89/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7888 - val_loss: 20.3610\n",
      "Epoch 90/200\n",
      "3630/3630 [==============================] - 0s 132us/step - loss: 22.7801 - val_loss: 20.6723\n",
      "Epoch 91/200\n",
      "3630/3630 [==============================] - 1s 140us/step - loss: 22.8158 - val_loss: 20.5458\n",
      "Epoch 92/200\n",
      "3630/3630 [==============================] - 1s 152us/step - loss: 22.8137 - val_loss: 20.3671\n",
      "Epoch 93/200\n",
      "3630/3630 [==============================] - 1s 165us/step - loss: 22.7887 - val_loss: 20.3783\n",
      "Epoch 94/200\n",
      "3630/3630 [==============================] - 1s 168us/step - loss: 22.7907 - val_loss: 20.3657\n",
      "Epoch 95/200\n",
      "3630/3630 [==============================] - 1s 171us/step - loss: 22.7678 - val_loss: 20.5529\n",
      "Epoch 96/200\n",
      "3630/3630 [==============================] - 1s 168us/step - loss: 22.8015 - val_loss: 20.3749\n",
      "Epoch 97/200\n",
      "3630/3630 [==============================] - 1s 165us/step - loss: 22.7878 - val_loss: 20.3607\n",
      "Epoch 98/200\n",
      "3630/3630 [==============================] - 1s 171us/step - loss: 22.7951 - val_loss: 20.4822\n",
      "Epoch 99/200\n",
      "3630/3630 [==============================] - 1s 169us/step - loss: 22.7835 - val_loss: 20.4095\n",
      "Epoch 100/200\n",
      "3630/3630 [==============================] - 1s 168us/step - loss: 22.7858 - val_loss: 20.4024\n",
      "Epoch 101/200\n",
      "3630/3630 [==============================] - 1s 166us/step - loss: 22.7985 - val_loss: 20.5977\n",
      "Epoch 102/200\n",
      "3630/3630 [==============================] - 1s 168us/step - loss: 22.7785 - val_loss: 20.6351\n",
      "Epoch 103/200\n",
      "3630/3630 [==============================] - 1s 170us/step - loss: 22.7937 - val_loss: 20.4988\n",
      "Epoch 104/200\n",
      "3630/3630 [==============================] - 1s 170us/step - loss: 22.8206 - val_loss: 20.3748\n",
      "Epoch 105/200\n",
      "3630/3630 [==============================] - 1s 147us/step - loss: 22.7805 - val_loss: 20.3633\n",
      "Epoch 106/200\n",
      "3630/3630 [==============================] - 0s 122us/step - loss: 22.7683 - val_loss: 20.4701\n",
      "Epoch 107/200\n",
      "3630/3630 [==============================] - 0s 124us/step - loss: 22.7957 - val_loss: 20.3832\n",
      "Epoch 108/200\n",
      "3630/3630 [==============================] - 0s 122us/step - loss: 22.7546 - val_loss: 20.3785\n",
      "Epoch 109/200\n",
      "3630/3630 [==============================] - 0s 121us/step - loss: 22.7928 - val_loss: 20.3566\n",
      "Epoch 110/200\n",
      "3630/3630 [==============================] - 0s 126us/step - loss: 22.7850 - val_loss: 20.3687\n",
      "Epoch 111/200\n",
      "3630/3630 [==============================] - 0s 124us/step - loss: 22.7683 - val_loss: 20.3652\n",
      "Epoch 112/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7893 - val_loss: 20.4028\n",
      "Epoch 113/200\n",
      "3630/3630 [==============================] - 1s 176us/step - loss: 22.7985 - val_loss: 20.3848\n",
      "Epoch 114/200\n",
      "3630/3630 [==============================] - 1s 174us/step - loss: 22.7950 - val_loss: 20.3856\n",
      "Epoch 115/200\n",
      "3630/3630 [==============================] - 1s 178us/step - loss: 22.7748 - val_loss: 20.4928\n",
      "Epoch 116/200\n",
      "3630/3630 [==============================] - 1s 175us/step - loss: 22.8115 - val_loss: 20.3610\n",
      "Epoch 117/200\n",
      "3630/3630 [==============================] - 0s 129us/step - loss: 22.7844 - val_loss: 20.3590\n",
      "Epoch 118/200\n",
      "3630/3630 [==============================] - 0s 124us/step - loss: 22.8067 - val_loss: 20.3666\n",
      "Epoch 119/200\n",
      "3630/3630 [==============================] - 0s 131us/step - loss: 22.7781 - val_loss: 20.3979\n",
      "Epoch 120/200\n",
      "3630/3630 [==============================] - 1s 141us/step - loss: 22.7950 - val_loss: 20.4087\n",
      "Epoch 121/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.7865 - val_loss: 20.3607\n",
      "Epoch 122/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.8037 - val_loss: 20.3945\n",
      "Epoch 123/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7797 - val_loss: 20.3589\n",
      "Epoch 124/200\n",
      "3630/3630 [==============================] - 1s 154us/step - loss: 22.7897 - val_loss: 20.4326\n",
      "Epoch 125/200\n",
      "3630/3630 [==============================] - 1s 147us/step - loss: 22.8056 - val_loss: 20.4168\n",
      "Epoch 126/200\n",
      "3630/3630 [==============================] - 1s 152us/step - loss: 22.7835 - val_loss: 20.4974\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3630/3630 [==============================] - 1s 152us/step - loss: 22.7949 - val_loss: 20.3751\n",
      "Epoch 128/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.7870 - val_loss: 20.3816\n",
      "Epoch 129/200\n",
      "3630/3630 [==============================] - 1s 145us/step - loss: 22.8078 - val_loss: 20.3653\n",
      "Epoch 130/200\n",
      "3630/3630 [==============================] - 1s 156us/step - loss: 22.8005 - val_loss: 20.4087\n",
      "Epoch 131/200\n",
      "3630/3630 [==============================] - 1s 156us/step - loss: 22.7982 - val_loss: 20.3951\n",
      "Epoch 132/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7831 - val_loss: 20.4216\n",
      "Epoch 133/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7938 - val_loss: 20.4173\n",
      "Epoch 134/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7836 - val_loss: 20.3838\n",
      "Epoch 135/200\n",
      "3630/3630 [==============================] - 1s 156us/step - loss: 22.7787 - val_loss: 20.4094\n",
      "Epoch 136/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7917 - val_loss: 20.4234\n",
      "Epoch 137/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7818 - val_loss: 20.3848\n",
      "Epoch 138/200\n",
      "3630/3630 [==============================] - 1s 155us/step - loss: 22.7859 - val_loss: 20.3947\n",
      "Epoch 139/200\n",
      "3630/3630 [==============================] - 0s 133us/step - loss: 22.7839 - val_loss: 20.3589\n",
      "Epoch 140/200\n",
      "3630/3630 [==============================] - 0s 124us/step - loss: 22.7811 - val_loss: 20.3666\n",
      "Epoch 141/200\n",
      "3630/3630 [==============================] - 0s 126us/step - loss: 22.7865 - val_loss: 20.6030\n",
      "Epoch 142/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.8058 - val_loss: 20.3821\n",
      "Epoch 143/200\n",
      "3630/3630 [==============================] - 1s 161us/step - loss: 22.7690 - val_loss: 20.4145\n",
      "Epoch 144/200\n",
      "3630/3630 [==============================] - 1s 163us/step - loss: 22.8116 - val_loss: 20.4361\n",
      "Epoch 145/200\n",
      "3630/3630 [==============================] - 1s 146us/step - loss: 22.7918 - val_loss: 20.4065\n",
      "Epoch 146/200\n",
      "3630/3630 [==============================] - 1s 155us/step - loss: 22.7876 - val_loss: 20.3991\n",
      "Epoch 147/200\n",
      "3630/3630 [==============================] - 1s 151us/step - loss: 22.8005 - val_loss: 20.3613\n",
      "Epoch 148/200\n",
      "3630/3630 [==============================] - 1s 150us/step - loss: 22.8112 - val_loss: 20.3682\n",
      "Epoch 149/200\n",
      "3630/3630 [==============================] - 1s 147us/step - loss: 22.7732 - val_loss: 20.3982\n",
      "Epoch 150/200\n",
      "3630/3630 [==============================] - 0s 132us/step - loss: 22.7837 - val_loss: 20.3648\n",
      "Epoch 151/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.8009 - val_loss: 20.4196\n",
      "Epoch 152/200\n",
      "3630/3630 [==============================] - 1s 176us/step - loss: 22.7831 - val_loss: 20.3993\n",
      "Epoch 153/200\n",
      "3630/3630 [==============================] - 1s 177us/step - loss: 22.7798 - val_loss: 20.4378\n",
      "Epoch 154/200\n",
      "3630/3630 [==============================] - 1s 173us/step - loss: 22.7836 - val_loss: 20.3805\n",
      "Epoch 155/200\n",
      "3630/3630 [==============================] - 1s 163us/step - loss: 22.7900 - val_loss: 20.3951\n",
      "Epoch 156/200\n",
      "3630/3630 [==============================] - 0s 129us/step - loss: 22.7848 - val_loss: 20.4307\n",
      "Epoch 157/200\n",
      "3630/3630 [==============================] - 0s 132us/step - loss: 22.7975 - val_loss: 20.3616\n",
      "Epoch 158/200\n",
      "3630/3630 [==============================] - 0s 129us/step - loss: 22.7857 - val_loss: 20.4419\n",
      "Epoch 159/200\n",
      "3630/3630 [==============================] - 0s 135us/step - loss: 22.7753 - val_loss: 20.3795\n",
      "Epoch 160/200\n",
      "3630/3630 [==============================] - 1s 143us/step - loss: 22.8030 - val_loss: 20.3905\n",
      "Epoch 161/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7770 - val_loss: 20.4026\n",
      "Epoch 162/200\n",
      "3630/3630 [==============================] - 1s 157us/step - loss: 22.7865 - val_loss: 20.3778\n",
      "Epoch 163/200\n",
      "3630/3630 [==============================] - 1s 171us/step - loss: 22.7947 - val_loss: 20.5021\n",
      "Epoch 164/200\n",
      "3630/3630 [==============================] - 1s 171us/step - loss: 22.7982 - val_loss: 20.4050\n",
      "Epoch 165/200\n",
      "3630/3630 [==============================] - 1s 171us/step - loss: 22.7827 - val_loss: 20.3601\n",
      "Epoch 166/200\n",
      "3630/3630 [==============================] - 1s 151us/step - loss: 22.7913 - val_loss: 20.3639\n",
      "Epoch 167/200\n",
      "3630/3630 [==============================] - 1s 166us/step - loss: 22.7815 - val_loss: 20.3644\n",
      "Epoch 168/200\n",
      "3630/3630 [==============================] - 1s 151us/step - loss: 22.8090 - val_loss: 20.3623\n",
      "Epoch 169/200\n",
      "3630/3630 [==============================] - 1s 156us/step - loss: 22.7676 - val_loss: 20.3616\n",
      "Epoch 170/200\n",
      "3630/3630 [==============================] - 1s 171us/step - loss: 22.7734 - val_loss: 20.3628\n",
      "Epoch 171/200\n",
      "3630/3630 [==============================] - 1s 165us/step - loss: 22.7736 - val_loss: 20.3806\n",
      "Epoch 172/200\n",
      "3630/3630 [==============================] - 0s 129us/step - loss: 22.8000 - val_loss: 20.3850\n",
      "Epoch 173/200\n",
      "3630/3630 [==============================] - 1s 140us/step - loss: 22.7594 - val_loss: 20.5558\n",
      "Epoch 174/200\n",
      "3630/3630 [==============================] - 1s 151us/step - loss: 22.8080 - val_loss: 20.3721\n",
      "Epoch 175/200\n",
      "3630/3630 [==============================] - 1s 174us/step - loss: 22.7887 - val_loss: 20.4310\n",
      "Epoch 176/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7876 - val_loss: 20.3673\n",
      "Epoch 177/200\n",
      "3630/3630 [==============================] - 0s 132us/step - loss: 22.7899 - val_loss: 20.3586\n",
      "Epoch 178/200\n",
      "3630/3630 [==============================] - 1s 162us/step - loss: 22.7825 - val_loss: 20.4264\n",
      "Epoch 179/200\n",
      "3630/3630 [==============================] - 1s 165us/step - loss: 22.7778 - val_loss: 20.4784\n",
      "Epoch 180/200\n",
      "3630/3630 [==============================] - 1s 162us/step - loss: 22.7897 - val_loss: 20.4493\n",
      "Epoch 181/200\n",
      "3630/3630 [==============================] - 1s 160us/step - loss: 22.8024 - val_loss: 20.3629\n",
      "Epoch 182/200\n",
      "3630/3630 [==============================] - 1s 166us/step - loss: 22.7837 - val_loss: 20.4019\n",
      "Epoch 183/200\n",
      "3630/3630 [==============================] - 1s 161us/step - loss: 22.7945 - val_loss: 20.4204\n",
      "Epoch 184/200\n",
      "3630/3630 [==============================] - 1s 161us/step - loss: 22.7813 - val_loss: 20.3607\n",
      "Epoch 185/200\n",
      "3630/3630 [==============================] - 1s 163us/step - loss: 22.7767 - val_loss: 20.4869\n",
      "Epoch 186/200\n",
      "3630/3630 [==============================] - 1s 161us/step - loss: 22.7706 - val_loss: 20.4681\n",
      "Epoch 187/200\n",
      "3630/3630 [==============================] - 1s 159us/step - loss: 22.7997 - val_loss: 20.5011\n",
      "Epoch 188/200\n",
      "3630/3630 [==============================] - 0s 119us/step - loss: 22.7828 - val_loss: 20.3798\n",
      "Epoch 189/200\n",
      "3630/3630 [==============================] - 0s 132us/step - loss: 22.7663 - val_loss: 20.3615\n",
      "Epoch 190/200\n",
      "3630/3630 [==============================] - 0s 130us/step - loss: 22.7813 - val_loss: 20.3607\n",
      "Epoch 191/200\n",
      "3630/3630 [==============================] - 0s 134us/step - loss: 22.7917 - val_loss: 20.3847\n",
      "Epoch 192/200\n",
      "3630/3630 [==============================] - 0s 136us/step - loss: 22.8055 - val_loss: 20.3773\n",
      "Epoch 193/200\n",
      "3630/3630 [==============================] - 1s 149us/step - loss: 22.7789 - val_loss: 20.4291\n",
      "Epoch 194/200\n",
      "3630/3630 [==============================] - 1s 153us/step - loss: 22.7755 - val_loss: 20.3980\n",
      "Epoch 195/200\n",
      "3630/3630 [==============================] - 1s 162us/step - loss: 22.7775 - val_loss: 20.3578\n",
      "Epoch 196/200\n",
      "3630/3630 [==============================] - 1s 162us/step - loss: 22.7793 - val_loss: 20.3627\n",
      "Epoch 197/200\n",
      "3630/3630 [==============================] - 1s 164us/step - loss: 22.8017 - val_loss: 20.3997\n",
      "Epoch 198/200\n",
      "3630/3630 [==============================] - 1s 161us/step - loss: 22.7920 - val_loss: 20.3707\n",
      "Epoch 199/200\n",
      "3630/3630 [==============================] - 0s 136us/step - loss: 22.7957 - val_loss: 20.3671\n",
      "Epoch 200/200\n",
      "3630/3630 [==============================] - 1s 142us/step - loss: 22.7786 - val_loss: 20.3650\n",
      "---------current test year is 2008---------\n",
      "Train on 3633 samples, validate on 1212 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633/3633 [==============================] - 1s 352us/step - loss: 94.0400 - val_loss: 84.9260\n",
      "Epoch 2/200\n",
      "3633/3633 [==============================] - 1s 162us/step - loss: 77.2772 - val_loss: 70.6603\n",
      "Epoch 3/200\n",
      "3633/3633 [==============================] - 1s 171us/step - loss: 63.3579 - val_loss: 57.8720\n",
      "Epoch 4/200\n",
      "3633/3633 [==============================] - 1s 173us/step - loss: 51.3165 - val_loss: 46.0253\n",
      "Epoch 5/200\n",
      "3633/3633 [==============================] - 1s 173us/step - loss: 40.1236 - val_loss: 35.0298\n",
      "Epoch 6/200\n",
      "3633/3633 [==============================] - 1s 172us/step - loss: 30.0453 - val_loss: 25.8705\n",
      "Epoch 7/200\n",
      "3633/3633 [==============================] - 0s 131us/step - loss: 23.4429 - val_loss: 21.8255\n",
      "Epoch 8/200\n",
      "3633/3633 [==============================] - 0s 123us/step - loss: 21.1981 - val_loss: 20.6070\n",
      "Epoch 9/200\n",
      "3633/3633 [==============================] - 0s 133us/step - loss: 20.6760 - val_loss: 20.4315\n",
      "Epoch 10/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.6151 - val_loss: 20.3809\n",
      "Epoch 11/200\n",
      "3633/3633 [==============================] - 1s 163us/step - loss: 20.6015 - val_loss: 20.3744\n",
      "Epoch 12/200\n",
      "3633/3633 [==============================] - 1s 166us/step - loss: 20.5937 - val_loss: 20.3669\n",
      "Epoch 13/200\n",
      "3633/3633 [==============================] - 1s 163us/step - loss: 20.5915 - val_loss: 20.3636\n",
      "Epoch 14/200\n",
      "3633/3633 [==============================] - 1s 164us/step - loss: 20.5923 - val_loss: 20.3630\n",
      "Epoch 15/200\n",
      "3633/3633 [==============================] - 1s 162us/step - loss: 20.5930 - val_loss: 20.3634\n",
      "Epoch 16/200\n",
      "3633/3633 [==============================] - 1s 165us/step - loss: 20.5927 - val_loss: 20.3628\n",
      "Epoch 17/200\n",
      "3633/3633 [==============================] - 1s 165us/step - loss: 20.5930 - val_loss: 20.3621\n",
      "Epoch 18/200\n",
      "3633/3633 [==============================] - 1s 165us/step - loss: 20.5920 - val_loss: 20.3597\n",
      "Epoch 19/200\n",
      "3633/3633 [==============================] - 1s 165us/step - loss: 20.5908 - val_loss: 20.3613\n",
      "Epoch 20/200\n",
      "3633/3633 [==============================] - 1s 165us/step - loss: 20.5936 - val_loss: 20.3605\n",
      "Epoch 21/200\n",
      "3633/3633 [==============================] - 1s 162us/step - loss: 20.5918 - val_loss: 20.3589\n",
      "Epoch 22/200\n",
      "3633/3633 [==============================] - 1s 165us/step - loss: 20.5883 - val_loss: 20.3648\n",
      "Epoch 23/200\n",
      "3633/3633 [==============================] - 0s 135us/step - loss: 20.5901 - val_loss: 20.3618\n",
      "Epoch 24/200\n",
      "3633/3633 [==============================] - 0s 132us/step - loss: 20.5919 - val_loss: 20.3605\n",
      "Epoch 25/200\n",
      "3633/3633 [==============================] - 0s 135us/step - loss: 20.5915 - val_loss: 20.3591\n",
      "Epoch 26/200\n",
      "3633/3633 [==============================] - 0s 134us/step - loss: 20.5922 - val_loss: 20.3595\n",
      "Epoch 27/200\n",
      "3633/3633 [==============================] - 0s 134us/step - loss: 20.5945 - val_loss: 20.3586\n",
      "Epoch 28/200\n",
      "3633/3633 [==============================] - 0s 135us/step - loss: 20.5898 - val_loss: 20.3623\n",
      "Epoch 29/200\n",
      "3633/3633 [==============================] - 0s 125us/step - loss: 20.5926 - val_loss: 20.3616\n",
      "Epoch 30/200\n",
      "3633/3633 [==============================] - 1s 142us/step - loss: 20.5910 - val_loss: 20.3625\n",
      "Epoch 31/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5954 - val_loss: 20.3605\n",
      "Epoch 32/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5929 - val_loss: 20.3614\n",
      "Epoch 33/200\n",
      "3633/3633 [==============================] - 1s 176us/step - loss: 20.5917 - val_loss: 20.3615\n",
      "Epoch 34/200\n",
      "3633/3633 [==============================] - 1s 163us/step - loss: 20.5930 - val_loss: 20.3618\n",
      "Epoch 35/200\n",
      "3633/3633 [==============================] - 0s 135us/step - loss: 20.5915 - val_loss: 20.3608\n",
      "Epoch 36/200\n",
      "3633/3633 [==============================] - 1s 150us/step - loss: 20.5913 - val_loss: 20.3626\n",
      "Epoch 37/200\n",
      "3633/3633 [==============================] - 0s 138us/step - loss: 20.5919 - val_loss: 20.3630\n",
      "Epoch 38/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5897 - val_loss: 20.3621\n",
      "Epoch 39/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5905 - val_loss: 20.3605\n",
      "Epoch 40/200\n",
      "3633/3633 [==============================] - 1s 153us/step - loss: 20.5924 - val_loss: 20.3602\n",
      "Epoch 41/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5870 - val_loss: 20.3636\n",
      "Epoch 42/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5918 - val_loss: 20.3670\n",
      "Epoch 43/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5917 - val_loss: 20.3632\n",
      "Epoch 44/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5925 - val_loss: 20.3617\n",
      "Epoch 45/200\n",
      "3633/3633 [==============================] - 1s 143us/step - loss: 20.5919 - val_loss: 20.3599\n",
      "Epoch 46/200\n",
      "3633/3633 [==============================] - 1s 151us/step - loss: 20.5902 - val_loss: 20.3598\n",
      "Epoch 47/200\n",
      "3633/3633 [==============================] - 1s 167us/step - loss: 20.5899 - val_loss: 20.3614\n",
      "Epoch 48/200\n",
      "3633/3633 [==============================] - 1s 165us/step - loss: 20.5922 - val_loss: 20.3638\n",
      "Epoch 49/200\n",
      "3633/3633 [==============================] - 1s 163us/step - loss: 20.5931 - val_loss: 20.3605\n",
      "Epoch 50/200\n",
      "3633/3633 [==============================] - 1s 164us/step - loss: 20.5845 - val_loss: 20.3682\n",
      "Epoch 51/200\n",
      "3633/3633 [==============================] - 0s 126us/step - loss: 20.5944 - val_loss: 20.3653\n",
      "Epoch 52/200\n",
      "3633/3633 [==============================] - 0s 137us/step - loss: 20.5933 - val_loss: 20.3645\n",
      "Epoch 53/200\n",
      "3633/3633 [==============================] - 1s 145us/step - loss: 20.5930 - val_loss: 20.3612\n",
      "Epoch 54/200\n",
      "3633/3633 [==============================] - 1s 149us/step - loss: 20.5911 - val_loss: 20.3599\n",
      "Epoch 55/200\n",
      "3633/3633 [==============================] - 1s 151us/step - loss: 20.5933 - val_loss: 20.3606\n",
      "Epoch 56/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5928 - val_loss: 20.3610\n",
      "Epoch 57/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5864 - val_loss: 20.3645\n",
      "Epoch 58/200\n",
      "3633/3633 [==============================] - 1s 179us/step - loss: 20.5924 - val_loss: 20.3644\n",
      "Epoch 59/200\n",
      "3633/3633 [==============================] - 1s 179us/step - loss: 20.5933 - val_loss: 20.3616\n",
      "Epoch 60/200\n",
      "3633/3633 [==============================] - 1s 176us/step - loss: 20.5924 - val_loss: 20.3617\n",
      "Epoch 61/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5910 - val_loss: 20.3607\n",
      "Epoch 62/200\n",
      "3633/3633 [==============================] - 0s 132us/step - loss: 20.5901 - val_loss: 20.3582\n",
      "Epoch 63/200\n",
      "3633/3633 [==============================] - 0s 132us/step - loss: 20.5919 - val_loss: 20.3603\n",
      "Epoch 64/200\n",
      "3633/3633 [==============================] - 0s 135us/step - loss: 20.5906 - val_loss: 20.3610\n",
      "Epoch 65/200\n",
      "3633/3633 [==============================] - 1s 145us/step - loss: 20.5900 - val_loss: 20.3605\n",
      "Epoch 66/200\n",
      "3633/3633 [==============================] - 1s 155us/step - loss: 20.5877 - val_loss: 20.3635\n",
      "Epoch 67/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5925 - val_loss: 20.3614\n",
      "Epoch 68/200\n",
      "3633/3633 [==============================] - 1s 152us/step - loss: 20.5945 - val_loss: 20.3610\n",
      "Epoch 69/200\n",
      "3633/3633 [==============================] - 1s 148us/step - loss: 20.5915 - val_loss: 20.3621\n",
      "Epoch 70/200\n",
      "3633/3633 [==============================] - 1s 155us/step - loss: 20.5923 - val_loss: 20.3613\n",
      "Epoch 71/200\n",
      "3633/3633 [==============================] - 1s 152us/step - loss: 20.5907 - val_loss: 20.3627\n",
      "Epoch 72/200\n",
      "3633/3633 [==============================] - 1s 152us/step - loss: 20.5924 - val_loss: 20.3606\n",
      "Epoch 73/200\n",
      "3633/3633 [==============================] - 1s 140us/step - loss: 20.5882 - val_loss: 20.3624\n",
      "Epoch 74/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5952 - val_loss: 20.3625\n",
      "Epoch 75/200\n",
      "3633/3633 [==============================] - 1s 151us/step - loss: 20.5893 - val_loss: 20.3607\n",
      "Epoch 76/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5839 - val_loss: 20.3665\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633/3633 [==============================] - 1s 152us/step - loss: 20.5930 - val_loss: 20.3647\n",
      "Epoch 78/200\n",
      "3633/3633 [==============================] - 1s 150us/step - loss: 20.5914 - val_loss: 20.3622\n",
      "Epoch 79/200\n",
      "3633/3633 [==============================] - 1s 141us/step - loss: 20.5924 - val_loss: 20.3612\n",
      "Epoch 80/200\n",
      "3633/3633 [==============================] - 1s 144us/step - loss: 20.5880 - val_loss: 20.3640\n",
      "Epoch 81/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5929 - val_loss: 20.3618\n",
      "Epoch 82/200\n",
      "3633/3633 [==============================] - 1s 152us/step - loss: 20.5925 - val_loss: 20.3607\n",
      "Epoch 83/200\n",
      "3633/3633 [==============================] - 1s 153us/step - loss: 20.5945 - val_loss: 20.3620\n",
      "Epoch 84/200\n",
      "3633/3633 [==============================] - 1s 138us/step - loss: 20.5917 - val_loss: 20.3607\n",
      "Epoch 85/200\n",
      "3633/3633 [==============================] - 1s 163us/step - loss: 20.5927 - val_loss: 20.3603\n",
      "Epoch 86/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5893 - val_loss: 20.3629\n",
      "Epoch 87/200\n",
      "3633/3633 [==============================] - 1s 159us/step - loss: 20.5950 - val_loss: 20.3622\n",
      "Epoch 88/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5934 - val_loss: 20.3611\n",
      "Epoch 89/200\n",
      "3633/3633 [==============================] - 0s 126us/step - loss: 20.5911 - val_loss: 20.3605\n",
      "Epoch 90/200\n",
      "3633/3633 [==============================] - 0s 135us/step - loss: 20.5926 - val_loss: 20.3604\n",
      "Epoch 91/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5915 - val_loss: 20.3593\n",
      "Epoch 92/200\n",
      "3633/3633 [==============================] - 1s 179us/step - loss: 20.5844 - val_loss: 20.3653\n",
      "Epoch 93/200\n",
      "3633/3633 [==============================] - 1s 188us/step - loss: 20.5926 - val_loss: 20.3623\n",
      "Epoch 94/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5921 - val_loss: 20.3615\n",
      "Epoch 95/200\n",
      "3633/3633 [==============================] - 0s 131us/step - loss: 20.5903 - val_loss: 20.3600\n",
      "Epoch 96/200\n",
      "3633/3633 [==============================] - 1s 144us/step - loss: 20.5904 - val_loss: 20.3598\n",
      "Epoch 97/200\n",
      "3633/3633 [==============================] - 1s 147us/step - loss: 20.5891 - val_loss: 20.3622\n",
      "Epoch 98/200\n",
      "3633/3633 [==============================] - 1s 150us/step - loss: 20.5918 - val_loss: 20.3613\n",
      "Epoch 99/200\n",
      "3633/3633 [==============================] - 1s 155us/step - loss: 20.5857 - val_loss: 20.3658\n",
      "Epoch 100/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5926 - val_loss: 20.3644\n",
      "Epoch 101/200\n",
      "3633/3633 [==============================] - 1s 149us/step - loss: 20.5897 - val_loss: 20.3633\n",
      "Epoch 102/200\n",
      "3633/3633 [==============================] - 1s 153us/step - loss: 20.5932 - val_loss: 20.3614\n",
      "Epoch 103/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5913 - val_loss: 20.3630\n",
      "Epoch 104/200\n",
      "3633/3633 [==============================] - 1s 149us/step - loss: 20.5905 - val_loss: 20.3652\n",
      "Epoch 105/200\n",
      "3633/3633 [==============================] - 1s 150us/step - loss: 20.5921 - val_loss: 20.3618\n",
      "Epoch 106/200\n",
      "3633/3633 [==============================] - 1s 144us/step - loss: 20.5900 - val_loss: 20.3625\n",
      "Epoch 107/200\n",
      "3633/3633 [==============================] - 1s 158us/step - loss: 20.5943 - val_loss: 20.3620\n",
      "Epoch 108/200\n",
      "3633/3633 [==============================] - 1s 156us/step - loss: 20.5902 - val_loss: 20.3603\n",
      "Epoch 109/200\n",
      "3633/3633 [==============================] - 1s 139us/step - loss: 20.5914 - val_loss: 20.3605\n",
      "Epoch 110/200\n",
      "3633/3633 [==============================] - 1s 159us/step - loss: 20.5912 - val_loss: 20.3606\n",
      "Epoch 111/200\n",
      "3633/3633 [==============================] - 1s 147us/step - loss: 20.5883 - val_loss: 20.3624\n",
      "Epoch 112/200\n",
      "3633/3633 [==============================] - 1s 149us/step - loss: 20.5918 - val_loss: 20.3613\n",
      "Epoch 113/200\n",
      "3633/3633 [==============================] - 1s 150us/step - loss: 20.5915 - val_loss: 20.3599\n",
      "Epoch 114/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5912 - val_loss: 20.3614\n",
      "Epoch 115/200\n",
      "3633/3633 [==============================] - 1s 151us/step - loss: 20.5919 - val_loss: 20.3596\n",
      "Epoch 116/200\n",
      "3633/3633 [==============================] - 0s 138us/step - loss: 20.5904 - val_loss: 20.3598\n",
      "Epoch 117/200\n",
      "3633/3633 [==============================] - 1s 155us/step - loss: 20.5903 - val_loss: 20.3604\n",
      "Epoch 118/200\n",
      "3633/3633 [==============================] - 1s 153us/step - loss: 20.5908 - val_loss: 20.3612\n",
      "Epoch 119/200\n",
      "3633/3633 [==============================] - 1s 158us/step - loss: 20.5916 - val_loss: 20.3609\n",
      "Epoch 120/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5923 - val_loss: 20.3610\n",
      "Epoch 121/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5927 - val_loss: 20.3605\n",
      "Epoch 122/200\n",
      "3633/3633 [==============================] - 0s 129us/step - loss: 20.5938 - val_loss: 20.3595\n",
      "Epoch 123/200\n",
      "3633/3633 [==============================] - 1s 142us/step - loss: 20.5877 - val_loss: 20.3640\n",
      "Epoch 124/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5914 - val_loss: 20.3636\n",
      "Epoch 125/200\n",
      "3633/3633 [==============================] - 1s 170us/step - loss: 20.5915 - val_loss: 20.3646\n",
      "Epoch 126/200\n",
      "3633/3633 [==============================] - 1s 174us/step - loss: 20.5920 - val_loss: 20.3619\n",
      "Epoch 127/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5918 - val_loss: 20.3609\n",
      "Epoch 128/200\n",
      "3633/3633 [==============================] - 0s 131us/step - loss: 20.5919 - val_loss: 20.3604\n",
      "Epoch 129/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5917 - val_loss: 20.3609\n",
      "Epoch 130/200\n",
      "3633/3633 [==============================] - 1s 161us/step - loss: 20.5897 - val_loss: 20.3625\n",
      "Epoch 131/200\n",
      "3633/3633 [==============================] - 1s 158us/step - loss: 20.5909 - val_loss: 20.3614\n",
      "Epoch 132/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5895 - val_loss: 20.3607\n",
      "Epoch 133/200\n",
      "3633/3633 [==============================] - 0s 132us/step - loss: 20.5899 - val_loss: 20.3607\n",
      "Epoch 134/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5916 - val_loss: 20.3611\n",
      "Epoch 135/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5904 - val_loss: 20.3612\n",
      "Epoch 136/200\n",
      "3633/3633 [==============================] - 1s 156us/step - loss: 20.5892 - val_loss: 20.3647\n",
      "Epoch 137/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5929 - val_loss: 20.3616\n",
      "Epoch 138/200\n",
      "3633/3633 [==============================] - 1s 140us/step - loss: 20.5906 - val_loss: 20.3615\n",
      "Epoch 139/200\n",
      "3633/3633 [==============================] - 1s 150us/step - loss: 20.5923 - val_loss: 20.3609\n",
      "Epoch 140/200\n",
      "3633/3633 [==============================] - 1s 156us/step - loss: 20.5911 - val_loss: 20.3596\n",
      "Epoch 141/200\n",
      "3633/3633 [==============================] - 1s 140us/step - loss: 20.5918 - val_loss: 20.3605\n",
      "Epoch 142/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5883 - val_loss: 20.3625\n",
      "Epoch 143/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5910 - val_loss: 20.3620\n",
      "Epoch 144/200\n",
      "3633/3633 [==============================] - 1s 143us/step - loss: 20.5913 - val_loss: 20.3617\n",
      "Epoch 145/200\n",
      "3633/3633 [==============================] - 1s 149us/step - loss: 20.5940 - val_loss: 20.3622\n",
      "Epoch 146/200\n",
      "3633/3633 [==============================] - 1s 155us/step - loss: 20.5907 - val_loss: 20.3609\n",
      "Epoch 147/200\n",
      "3633/3633 [==============================] - 1s 153us/step - loss: 20.5917 - val_loss: 20.3622\n",
      "Epoch 148/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5929 - val_loss: 20.3621\n",
      "Epoch 149/200\n",
      "3633/3633 [==============================] - 1s 151us/step - loss: 20.5905 - val_loss: 20.3598\n",
      "Epoch 150/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5837 - val_loss: 20.3667\n",
      "Epoch 151/200\n",
      "3633/3633 [==============================] - 1s 149us/step - loss: 20.5938 - val_loss: 20.3673\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5927 - val_loss: 20.3649\n",
      "Epoch 153/200\n",
      "3633/3633 [==============================] - 1s 152us/step - loss: 20.5929 - val_loss: 20.3636\n",
      "Epoch 154/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5914 - val_loss: 20.3623\n",
      "Epoch 155/200\n",
      "3633/3633 [==============================] - 0s 133us/step - loss: 20.5933 - val_loss: 20.3622\n",
      "Epoch 156/200\n",
      "3633/3633 [==============================] - 1s 155us/step - loss: 20.5896 - val_loss: 20.3611\n",
      "Epoch 157/200\n",
      "3633/3633 [==============================] - 1s 159us/step - loss: 20.5922 - val_loss: 20.3616\n",
      "Epoch 158/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5887 - val_loss: 20.3596\n",
      "Epoch 159/200\n",
      "3633/3633 [==============================] - 1s 160us/step - loss: 20.5915 - val_loss: 20.3585\n",
      "Epoch 160/200\n",
      "3633/3633 [==============================] - 1s 141us/step - loss: 20.5880 - val_loss: 20.3629\n",
      "Epoch 161/200\n",
      "3633/3633 [==============================] - 0s 134us/step - loss: 20.5913 - val_loss: 20.3620\n",
      "Epoch 162/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5919 - val_loss: 20.3609\n",
      "Epoch 163/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5915 - val_loss: 20.3605\n",
      "Epoch 164/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5922 - val_loss: 20.3601\n",
      "Epoch 165/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5904 - val_loss: 20.3617\n",
      "Epoch 166/200\n",
      "3633/3633 [==============================] - 1s 149us/step - loss: 20.5918 - val_loss: 20.3614\n",
      "Epoch 167/200\n",
      "3633/3633 [==============================] - 1s 151us/step - loss: 20.5917 - val_loss: 20.3595\n",
      "Epoch 168/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5900 - val_loss: 20.3604\n",
      "Epoch 169/200\n",
      "3633/3633 [==============================] - 1s 154us/step - loss: 20.5883 - val_loss: 20.3622\n",
      "Epoch 170/200\n",
      "3633/3633 [==============================] - 1s 152us/step - loss: 20.5904 - val_loss: 20.3614\n",
      "Epoch 171/200\n",
      "3633/3633 [==============================] - 1s 141us/step - loss: 20.5914 - val_loss: 20.3599\n",
      "Epoch 172/200\n",
      "3633/3633 [==============================] - 1s 147us/step - loss: 20.5909 - val_loss: 20.3622\n",
      "Epoch 173/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5911 - val_loss: 20.3630\n",
      "Epoch 174/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5914 - val_loss: 20.3627\n",
      "Epoch 175/200\n",
      "3633/3633 [==============================] - 1s 163us/step - loss: 20.5920 - val_loss: 20.3619\n",
      "Epoch 176/200\n",
      "3633/3633 [==============================] - 1s 158us/step - loss: 20.5891 - val_loss: 20.3592\n",
      "Epoch 177/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5907 - val_loss: 20.3603\n",
      "Epoch 178/200\n",
      "3633/3633 [==============================] - 1s 170us/step - loss: 20.5908 - val_loss: 20.3602\n",
      "Epoch 179/200\n",
      "3633/3633 [==============================] - 1s 176us/step - loss: 20.5915 - val_loss: 20.3609\n",
      "Epoch 180/200\n",
      "3633/3633 [==============================] - 1s 168us/step - loss: 20.5901 - val_loss: 20.3602\n",
      "Epoch 181/200\n",
      "3633/3633 [==============================] - 1s 146us/step - loss: 20.5918 - val_loss: 20.3606\n",
      "Epoch 182/200\n",
      "3633/3633 [==============================] - 1s 167us/step - loss: 20.5884 - val_loss: 20.3614\n",
      "Epoch 183/200\n",
      "3633/3633 [==============================] - 1s 163us/step - loss: 20.5903 - val_loss: 20.3612\n",
      "Epoch 184/200\n",
      "3633/3633 [==============================] - 1s 169us/step - loss: 20.5905 - val_loss: 20.3611\n",
      "Epoch 185/200\n",
      "3633/3633 [==============================] - 1s 164us/step - loss: 20.5902 - val_loss: 20.3605\n",
      "Epoch 186/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5895 - val_loss: 20.3619\n",
      "Epoch 187/200\n",
      "3633/3633 [==============================] - 0s 130us/step - loss: 20.5908 - val_loss: 20.3615\n",
      "Epoch 188/200\n",
      "3633/3633 [==============================] - 0s 128us/step - loss: 20.5895 - val_loss: 20.3613\n",
      "Epoch 189/200\n",
      "3633/3633 [==============================] - 1s 138us/step - loss: 20.5896 - val_loss: 20.3613\n",
      "Epoch 190/200\n",
      "3633/3633 [==============================] - 1s 157us/step - loss: 20.5919 - val_loss: 20.3604\n",
      "Epoch 191/200\n",
      "3633/3633 [==============================] - 1s 179us/step - loss: 20.5922 - val_loss: 20.3597\n",
      "Epoch 192/200\n",
      "3633/3633 [==============================] - 1s 151us/step - loss: 20.5912 - val_loss: 20.3614\n",
      "Epoch 193/200\n",
      "3633/3633 [==============================] - 1s 158us/step - loss: 20.5908 - val_loss: 20.3612\n",
      "Epoch 194/200\n",
      "3633/3633 [==============================] - 1s 185us/step - loss: 20.5922 - val_loss: 20.3612\n",
      "Epoch 195/200\n",
      "3633/3633 [==============================] - 1s 195us/step - loss: 20.5907 - val_loss: 20.3606\n",
      "Epoch 196/200\n",
      "3633/3633 [==============================] - 1s 196us/step - loss: 20.5911 - val_loss: 20.3599\n",
      "Epoch 197/200\n",
      "3633/3633 [==============================] - 1s 147us/step - loss: 20.5895 - val_loss: 20.3604\n",
      "Epoch 198/200\n",
      "3633/3633 [==============================] - 1s 138us/step - loss: 20.5912 - val_loss: 20.3609\n",
      "Epoch 199/200\n",
      "3633/3633 [==============================] - 0s 132us/step - loss: 20.5909 - val_loss: 20.3605\n",
      "Epoch 200/200\n",
      "3633/3633 [==============================] - 0s 134us/step - loss: 20.5916 - val_loss: 20.3602\n",
      "---------current test year is 2009---------\n",
      "Train on 3531 samples, validate on 1177 samples\n",
      "Epoch 1/200\n",
      "3531/3531 [==============================] - 1s 377us/step - loss: 71.1733 - val_loss: 32.6234\n",
      "Epoch 2/200\n",
      "3531/3531 [==============================] - 1s 158us/step - loss: 24.2274 - val_loss: 20.6783\n",
      "Epoch 3/200\n",
      "3531/3531 [==============================] - 1s 160us/step - loss: 23.4106 - val_loss: 20.5092\n",
      "Epoch 4/200\n",
      "3531/3531 [==============================] - 1s 172us/step - loss: 23.3584 - val_loss: 20.4683\n",
      "Epoch 5/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3405 - val_loss: 20.7409\n",
      "Epoch 6/200\n",
      "3531/3531 [==============================] - 0s 125us/step - loss: 23.3775 - val_loss: 20.4382\n",
      "Epoch 7/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3712 - val_loss: 20.5579\n",
      "Epoch 8/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3846 - val_loss: 20.4347\n",
      "Epoch 9/200\n",
      "3531/3531 [==============================] - 1s 184us/step - loss: 23.3371 - val_loss: 20.4401\n",
      "Epoch 10/200\n",
      "3531/3531 [==============================] - 1s 189us/step - loss: 23.3383 - val_loss: 20.4250\n",
      "Epoch 11/200\n",
      "3531/3531 [==============================] - 0s 129us/step - loss: 23.3477 - val_loss: 20.4198\n",
      "Epoch 12/200\n",
      "3531/3531 [==============================] - 0s 132us/step - loss: 23.3126 - val_loss: 20.5335\n",
      "Epoch 13/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3934 - val_loss: 20.4760\n",
      "Epoch 14/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3609 - val_loss: 20.6743\n",
      "Epoch 15/200\n",
      "3531/3531 [==============================] - 1s 158us/step - loss: 23.3334 - val_loss: 20.4377\n",
      "Epoch 16/200\n",
      "3531/3531 [==============================] - 1s 154us/step - loss: 23.3379 - val_loss: 20.4142\n",
      "Epoch 17/200\n",
      "3531/3531 [==============================] - 1s 142us/step - loss: 23.3377 - val_loss: 20.4166\n",
      "Epoch 18/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3598 - val_loss: 20.5235\n",
      "Epoch 19/200\n",
      "3531/3531 [==============================] - 1s 156us/step - loss: 23.3406 - val_loss: 20.4260\n",
      "Epoch 20/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3599 - val_loss: 20.4115\n",
      "Epoch 21/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3374 - val_loss: 20.4356\n",
      "Epoch 22/200\n",
      "3531/3531 [==============================] - 0s 139us/step - loss: 23.3252 - val_loss: 20.5472\n",
      "Epoch 23/200\n",
      "3531/3531 [==============================] - 0s 136us/step - loss: 23.3427 - val_loss: 20.4507\n",
      "Epoch 24/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3442 - val_loss: 20.4376\n",
      "Epoch 25/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3363 - val_loss: 20.5632\n",
      "Epoch 26/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3488 - val_loss: 20.4311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3406 - val_loss: 20.5028\n",
      "Epoch 28/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3577 - val_loss: 20.4200\n",
      "Epoch 29/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3238 - val_loss: 20.5318\n",
      "Epoch 30/200\n",
      "3531/3531 [==============================] - 1s 149us/step - loss: 23.3276 - val_loss: 20.4197\n",
      "Epoch 31/200\n",
      "3531/3531 [==============================] - 1s 148us/step - loss: 23.3436 - val_loss: 20.4313\n",
      "Epoch 32/200\n",
      "3531/3531 [==============================] - 1s 155us/step - loss: 23.3381 - val_loss: 20.4599\n",
      "Epoch 33/200\n",
      "3531/3531 [==============================] - 1s 148us/step - loss: 23.3253 - val_loss: 20.4602\n",
      "Epoch 34/200\n",
      "3531/3531 [==============================] - 0s 140us/step - loss: 23.3680 - val_loss: 20.4177\n",
      "Epoch 35/200\n",
      "3531/3531 [==============================] - 1s 145us/step - loss: 23.3447 - val_loss: 20.4274\n",
      "Epoch 36/200\n",
      "3531/3531 [==============================] - 1s 152us/step - loss: 23.3450 - val_loss: 20.4398\n",
      "Epoch 37/200\n",
      "3531/3531 [==============================] - 1s 155us/step - loss: 23.3562 - val_loss: 20.4312\n",
      "Epoch 38/200\n",
      "3531/3531 [==============================] - 1s 158us/step - loss: 23.3255 - val_loss: 20.4377\n",
      "Epoch 39/200\n",
      "3531/3531 [==============================] - 1s 148us/step - loss: 23.3600 - val_loss: 20.4286\n",
      "Epoch 40/200\n",
      "3531/3531 [==============================] - 0s 128us/step - loss: 23.3553 - val_loss: 20.4143\n",
      "Epoch 41/200\n",
      "3531/3531 [==============================] - 1s 154us/step - loss: 23.3349 - val_loss: 20.4427\n",
      "Epoch 42/200\n",
      "3531/3531 [==============================] - 1s 158us/step - loss: 23.3396 - val_loss: 20.4372\n",
      "Epoch 43/200\n",
      "3531/3531 [==============================] - 1s 159us/step - loss: 23.3382 - val_loss: 20.4161\n",
      "Epoch 44/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3380 - val_loss: 20.4373\n",
      "Epoch 45/200\n",
      "3531/3531 [==============================] - 0s 135us/step - loss: 23.3467 - val_loss: 20.4417\n",
      "Epoch 46/200\n",
      "3531/3531 [==============================] - 1s 142us/step - loss: 23.3500 - val_loss: 20.4728\n",
      "Epoch 47/200\n",
      "3531/3531 [==============================] - 1s 162us/step - loss: 23.3368 - val_loss: 20.4201\n",
      "Epoch 48/200\n",
      "3531/3531 [==============================] - 1s 179us/step - loss: 23.3368 - val_loss: 20.4212\n",
      "Epoch 49/200\n",
      "3531/3531 [==============================] - 1s 180us/step - loss: 23.3554 - val_loss: 20.4682\n",
      "Epoch 50/200\n",
      "3531/3531 [==============================] - 1s 155us/step - loss: 23.3660 - val_loss: 20.4628\n",
      "Epoch 51/200\n",
      "3531/3531 [==============================] - 0s 125us/step - loss: 23.3519 - val_loss: 20.4745\n",
      "Epoch 52/200\n",
      "3531/3531 [==============================] - 0s 129us/step - loss: 23.3244 - val_loss: 20.4095\n",
      "Epoch 53/200\n",
      "3531/3531 [==============================] - 1s 145us/step - loss: 23.3321 - val_loss: 20.7565\n",
      "Epoch 54/200\n",
      "3531/3531 [==============================] - 1s 173us/step - loss: 23.3496 - val_loss: 20.4374\n",
      "Epoch 55/200\n",
      "3531/3531 [==============================] - 1s 178us/step - loss: 23.3576 - val_loss: 20.4174\n",
      "Epoch 56/200\n",
      "3531/3531 [==============================] - 1s 182us/step - loss: 23.3427 - val_loss: 20.4167\n",
      "Epoch 57/200\n",
      "3531/3531 [==============================] - 1s 179us/step - loss: 23.3289 - val_loss: 20.4129\n",
      "Epoch 58/200\n",
      "3531/3531 [==============================] - 1s 174us/step - loss: 23.3145 - val_loss: 20.4438\n",
      "Epoch 59/200\n",
      "3531/3531 [==============================] - 1s 143us/step - loss: 23.3274 - val_loss: 20.4169\n",
      "Epoch 60/200\n",
      "3531/3531 [==============================] - 0s 132us/step - loss: 23.3592 - val_loss: 20.4178\n",
      "Epoch 61/200\n",
      "3531/3531 [==============================] - 0s 130us/step - loss: 23.3724 - val_loss: 20.4597\n",
      "Epoch 62/200\n",
      "3531/3531 [==============================] - 0s 129us/step - loss: 23.3487 - val_loss: 20.4990\n",
      "Epoch 63/200\n",
      "3531/3531 [==============================] - 0s 125us/step - loss: 23.3441 - val_loss: 20.4199\n",
      "Epoch 64/200\n",
      "3531/3531 [==============================] - 1s 145us/step - loss: 23.3504 - val_loss: 20.4141\n",
      "Epoch 65/200\n",
      "3531/3531 [==============================] - 1s 178us/step - loss: 23.3417 - val_loss: 20.6505\n",
      "Epoch 66/200\n",
      "3531/3531 [==============================] - 1s 174us/step - loss: 23.3547 - val_loss: 20.5739\n",
      "Epoch 67/200\n",
      "3531/3531 [==============================] - 0s 134us/step - loss: 23.3296 - val_loss: 20.4339\n",
      "Epoch 68/200\n",
      "3531/3531 [==============================] - 0s 133us/step - loss: 23.3453 - val_loss: 20.4420\n",
      "Epoch 69/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3374 - val_loss: 20.4599\n",
      "Epoch 70/200\n",
      "3531/3531 [==============================] - 1s 178us/step - loss: 23.3364 - val_loss: 20.4451\n",
      "Epoch 71/200\n",
      "3531/3531 [==============================] - 1s 180us/step - loss: 23.3542 - val_loss: 20.5159\n",
      "Epoch 72/200\n",
      "3531/3531 [==============================] - 1s 168us/step - loss: 23.3281 - val_loss: 20.4561\n",
      "Epoch 73/200\n",
      "3531/3531 [==============================] - 0s 125us/step - loss: 23.3125 - val_loss: 20.4313\n",
      "Epoch 74/200\n",
      "3531/3531 [==============================] - 0s 127us/step - loss: 23.3345 - val_loss: 20.4608\n",
      "Epoch 75/200\n",
      "3531/3531 [==============================] - 0s 139us/step - loss: 23.3319 - val_loss: 20.4745\n",
      "Epoch 76/200\n",
      "3531/3531 [==============================] - 1s 148us/step - loss: 23.3468 - val_loss: 20.4268\n",
      "Epoch 77/200\n",
      "3531/3531 [==============================] - 1s 151us/step - loss: 23.3395 - val_loss: 20.4571\n",
      "Epoch 78/200\n",
      "3531/3531 [==============================] - 1s 156us/step - loss: 23.3346 - val_loss: 20.4221\n",
      "Epoch 79/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3254 - val_loss: 20.4683\n",
      "Epoch 80/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3429 - val_loss: 20.4295\n",
      "Epoch 81/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3354 - val_loss: 20.4245\n",
      "Epoch 82/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3288 - val_loss: 20.6113\n",
      "Epoch 83/200\n",
      "3531/3531 [==============================] - 0s 137us/step - loss: 23.3535 - val_loss: 20.4160\n",
      "Epoch 84/200\n",
      "3531/3531 [==============================] - 1s 146us/step - loss: 23.3541 - val_loss: 20.4187\n",
      "Epoch 85/200\n",
      "3531/3531 [==============================] - 1s 173us/step - loss: 23.3436 - val_loss: 20.4241\n",
      "Epoch 86/200\n",
      "3531/3531 [==============================] - 1s 178us/step - loss: 23.3385 - val_loss: 20.4846\n",
      "Epoch 87/200\n",
      "3531/3531 [==============================] - 1s 178us/step - loss: 23.3289 - val_loss: 20.4108\n",
      "Epoch 88/200\n",
      "3531/3531 [==============================] - 1s 180us/step - loss: 23.3310 - val_loss: 20.4574\n",
      "Epoch 89/200\n",
      "3531/3531 [==============================] - 1s 145us/step - loss: 23.3177 - val_loss: 20.4436\n",
      "Epoch 90/200\n",
      "3531/3531 [==============================] - 0s 124us/step - loss: 23.3294 - val_loss: 20.4267\n",
      "Epoch 91/200\n",
      "3531/3531 [==============================] - 0s 128us/step - loss: 23.3594 - val_loss: 20.4221\n",
      "Epoch 92/200\n",
      "3531/3531 [==============================] - 1s 145us/step - loss: 23.2907 - val_loss: 20.7152\n",
      "Epoch 93/200\n",
      "3531/3531 [==============================] - 1s 160us/step - loss: 23.3568 - val_loss: 20.5532\n",
      "Epoch 94/200\n",
      "3531/3531 [==============================] - 1s 176us/step - loss: 23.3374 - val_loss: 20.5189\n",
      "Epoch 95/200\n",
      "3531/3531 [==============================] - 1s 155us/step - loss: 23.3382 - val_loss: 20.4131\n",
      "Epoch 96/200\n",
      "3531/3531 [==============================] - 0s 137us/step - loss: 23.3645 - val_loss: 20.4385\n",
      "Epoch 97/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3260 - val_loss: 20.4158\n",
      "Epoch 98/200\n",
      "3531/3531 [==============================] - 1s 167us/step - loss: 23.3304 - val_loss: 20.4313\n",
      "Epoch 99/200\n",
      "3531/3531 [==============================] - 1s 165us/step - loss: 23.3388 - val_loss: 20.4368\n",
      "Epoch 100/200\n",
      "3531/3531 [==============================] - 1s 157us/step - loss: 23.3379 - val_loss: 20.4174\n",
      "Epoch 101/200\n",
      "3531/3531 [==============================] - 0s 131us/step - loss: 23.3096 - val_loss: 20.4528\n",
      "Epoch 102/200\n",
      "3531/3531 [==============================] - 0s 138us/step - loss: 23.3293 - val_loss: 20.4189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3422 - val_loss: 20.4166\n",
      "Epoch 104/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3358 - val_loss: 20.4123\n",
      "Epoch 105/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3417 - val_loss: 20.4397\n",
      "Epoch 106/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3254 - val_loss: 20.4262\n",
      "Epoch 107/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3420 - val_loss: 20.4385\n",
      "Epoch 108/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3320 - val_loss: 20.5484\n",
      "Epoch 109/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3489 - val_loss: 20.4631\n",
      "Epoch 110/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3305 - val_loss: 20.4415\n",
      "Epoch 111/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3498 - val_loss: 20.4194\n",
      "Epoch 112/200\n",
      "3531/3531 [==============================] - 1s 149us/step - loss: 23.3320 - val_loss: 20.4421\n",
      "Epoch 113/200\n",
      "3531/3531 [==============================] - 0s 140us/step - loss: 23.3552 - val_loss: 20.4333\n",
      "Epoch 114/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3293 - val_loss: 20.4219\n",
      "Epoch 115/200\n",
      "3531/3531 [==============================] - 1s 165us/step - loss: 23.3442 - val_loss: 20.4173\n",
      "Epoch 116/200\n",
      "3531/3531 [==============================] - 1s 163us/step - loss: 23.3471 - val_loss: 20.4163\n",
      "Epoch 117/200\n",
      "3531/3531 [==============================] - 1s 167us/step - loss: 23.3257 - val_loss: 20.4218\n",
      "Epoch 118/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3437 - val_loss: 20.4107\n",
      "Epoch 119/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3396 - val_loss: 20.4139\n",
      "Epoch 120/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3271 - val_loss: 20.4216\n",
      "Epoch 121/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3400 - val_loss: 20.4460\n",
      "Epoch 122/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3273 - val_loss: 20.4557\n",
      "Epoch 123/200\n",
      "3531/3531 [==============================] - 0s 131us/step - loss: 23.3689 - val_loss: 20.4585\n",
      "Epoch 124/200\n",
      "3531/3531 [==============================] - 0s 121us/step - loss: 23.3309 - val_loss: 20.4564\n",
      "Epoch 125/200\n",
      "3531/3531 [==============================] - 0s 124us/step - loss: 23.3464 - val_loss: 20.4300\n",
      "Epoch 126/200\n",
      "3531/3531 [==============================] - 0s 133us/step - loss: 23.3382 - val_loss: 20.4217\n",
      "Epoch 127/200\n",
      "3531/3531 [==============================] - 1s 155us/step - loss: 23.3275 - val_loss: 20.4581\n",
      "Epoch 128/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3067 - val_loss: 20.5183\n",
      "Epoch 129/200\n",
      "3531/3531 [==============================] - 1s 146us/step - loss: 23.3223 - val_loss: 20.5695\n",
      "Epoch 130/200\n",
      "3531/3531 [==============================] - 1s 151us/step - loss: 23.3350 - val_loss: 20.6081\n",
      "Epoch 131/200\n",
      "3531/3531 [==============================] - 1s 170us/step - loss: 23.3236 - val_loss: 20.4280\n",
      "Epoch 132/200\n",
      "3531/3531 [==============================] - 1s 167us/step - loss: 23.3270 - val_loss: 20.4782\n",
      "Epoch 133/200\n",
      "3531/3531 [==============================] - 0s 134us/step - loss: 23.3258 - val_loss: 20.4204\n",
      "Epoch 134/200\n",
      "3531/3531 [==============================] - 1s 143us/step - loss: 23.3294 - val_loss: 20.4345\n",
      "Epoch 135/200\n",
      "3531/3531 [==============================] - 0s 138us/step - loss: 23.3391 - val_loss: 20.5021\n",
      "Epoch 136/200\n",
      "3531/3531 [==============================] - 1s 142us/step - loss: 23.3498 - val_loss: 20.5113\n",
      "Epoch 137/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3199 - val_loss: 20.4148\n",
      "Epoch 138/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3279 - val_loss: 20.4536\n",
      "Epoch 139/200\n",
      "3531/3531 [==============================] - 1s 180us/step - loss: 23.3407 - val_loss: 20.4203\n",
      "Epoch 140/200\n",
      "3531/3531 [==============================] - 1s 155us/step - loss: 23.3606 - val_loss: 20.4278\n",
      "Epoch 141/200\n",
      "3531/3531 [==============================] - 0s 126us/step - loss: 23.3363 - val_loss: 20.4262\n",
      "Epoch 142/200\n",
      "3531/3531 [==============================] - 0s 140us/step - loss: 23.3402 - val_loss: 20.4244\n",
      "Epoch 143/200\n",
      "3531/3531 [==============================] - 1s 160us/step - loss: 23.3237 - val_loss: 20.4704\n",
      "Epoch 144/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3401 - val_loss: 20.4143\n",
      "Epoch 145/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3420 - val_loss: 20.4669\n",
      "Epoch 146/200\n",
      "3531/3531 [==============================] - 0s 141us/step - loss: 23.2946 - val_loss: 20.7505\n",
      "Epoch 147/200\n",
      "3531/3531 [==============================] - 1s 142us/step - loss: 23.3565 - val_loss: 20.4556\n",
      "Epoch 148/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3219 - val_loss: 20.4354\n",
      "Epoch 149/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3321 - val_loss: 20.4184\n",
      "Epoch 150/200\n",
      "3531/3531 [==============================] - 1s 152us/step - loss: 23.3304 - val_loss: 20.5232\n",
      "Epoch 151/200\n",
      "3531/3531 [==============================] - 1s 148us/step - loss: 23.3317 - val_loss: 20.5231\n",
      "Epoch 152/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3188 - val_loss: 20.4858\n",
      "Epoch 153/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3560 - val_loss: 20.4171\n",
      "Epoch 154/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3398 - val_loss: 20.4258\n",
      "Epoch 155/200\n",
      "3531/3531 [==============================] - 1s 145us/step - loss: 23.3393 - val_loss: 20.4380\n",
      "Epoch 156/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.2914 - val_loss: 20.4896\n",
      "Epoch 157/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3487 - val_loss: 20.5684\n",
      "Epoch 158/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3258 - val_loss: 20.4155\n",
      "Epoch 159/200\n",
      "3531/3531 [==============================] - 1s 159us/step - loss: 23.3148 - val_loss: 20.4323\n",
      "Epoch 160/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3384 - val_loss: 20.4721\n",
      "Epoch 161/200\n",
      "3531/3531 [==============================] - 1s 151us/step - loss: 23.3387 - val_loss: 20.4367\n",
      "Epoch 162/200\n",
      "3531/3531 [==============================] - 0s 125us/step - loss: 23.3264 - val_loss: 20.4588\n",
      "Epoch 163/200\n",
      "3531/3531 [==============================] - 0s 141us/step - loss: 23.3148 - val_loss: 20.4451\n",
      "Epoch 164/200\n",
      "3531/3531 [==============================] - 1s 151us/step - loss: 23.3349 - val_loss: 20.4256\n",
      "Epoch 165/200\n",
      "3531/3531 [==============================] - 1s 168us/step - loss: 23.3440 - val_loss: 20.4136\n",
      "Epoch 166/200\n",
      "3531/3531 [==============================] - 0s 127us/step - loss: 23.3542 - val_loss: 20.4367\n",
      "Epoch 167/200\n",
      "3531/3531 [==============================] - 0s 136us/step - loss: 23.3257 - val_loss: 20.4275\n",
      "Epoch 168/200\n",
      "3531/3531 [==============================] - 1s 150us/step - loss: 23.3390 - val_loss: 20.5626\n",
      "Epoch 169/200\n",
      "3531/3531 [==============================] - 1s 154us/step - loss: 23.3318 - val_loss: 20.4249\n",
      "Epoch 170/200\n",
      "3531/3531 [==============================] - 1s 152us/step - loss: 23.3382 - val_loss: 20.4799\n",
      "Epoch 171/200\n",
      "3531/3531 [==============================] - 1s 157us/step - loss: 23.3184 - val_loss: 20.4379\n",
      "Epoch 172/200\n",
      "3531/3531 [==============================] - 1s 154us/step - loss: 23.3283 - val_loss: 20.4356\n",
      "Epoch 173/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3462 - val_loss: 20.4181\n",
      "Epoch 174/200\n",
      "3531/3531 [==============================] - 1s 159us/step - loss: 23.3432 - val_loss: 20.5165\n",
      "Epoch 175/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3093 - val_loss: 20.4584\n",
      "Epoch 176/200\n",
      "3531/3531 [==============================] - 1s 148us/step - loss: 23.3396 - val_loss: 20.5118\n",
      "Epoch 177/200\n",
      "3531/3531 [==============================] - 1s 146us/step - loss: 23.3404 - val_loss: 20.4208\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3531/3531 [==============================] - 1s 167us/step - loss: 23.3278 - val_loss: 20.4364\n",
      "Epoch 179/200\n",
      "3531/3531 [==============================] - 1s 156us/step - loss: 23.3315 - val_loss: 20.4406\n",
      "Epoch 180/200\n",
      "3531/3531 [==============================] - 0s 141us/step - loss: 23.3280 - val_loss: 20.4307\n",
      "Epoch 181/200\n",
      "3531/3531 [==============================] - 1s 144us/step - loss: 23.3126 - val_loss: 20.5824\n",
      "Epoch 182/200\n",
      "3531/3531 [==============================] - 1s 159us/step - loss: 23.3293 - val_loss: 20.4234\n",
      "Epoch 183/200\n",
      "3531/3531 [==============================] - 1s 170us/step - loss: 23.3520 - val_loss: 20.4156\n",
      "Epoch 184/200\n",
      "3531/3531 [==============================] - 1s 153us/step - loss: 23.3180 - val_loss: 20.5218\n",
      "Epoch 185/200\n",
      "3531/3531 [==============================] - 0s 136us/step - loss: 23.3210 - val_loss: 20.4221\n",
      "Epoch 186/200\n",
      "3531/3531 [==============================] - 1s 156us/step - loss: 23.2978 - val_loss: 20.5009\n",
      "Epoch 187/200\n",
      "3531/3531 [==============================] - 1s 167us/step - loss: 23.3338 - val_loss: 20.5789\n",
      "Epoch 188/200\n",
      "3531/3531 [==============================] - 1s 148us/step - loss: 23.3321 - val_loss: 20.4201\n",
      "Epoch 189/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3239 - val_loss: 20.4373\n",
      "Epoch 190/200\n",
      "3531/3531 [==============================] - 1s 169us/step - loss: 23.3254 - val_loss: 20.4560\n",
      "Epoch 191/200\n",
      "3531/3531 [==============================] - 1s 171us/step - loss: 23.3187 - val_loss: 20.4144\n",
      "Epoch 192/200\n",
      "3531/3531 [==============================] - 1s 169us/step - loss: 23.3424 - val_loss: 20.4243\n",
      "Epoch 193/200\n",
      "3531/3531 [==============================] - 1s 164us/step - loss: 23.3497 - val_loss: 20.5144\n",
      "Epoch 194/200\n",
      "3531/3531 [==============================] - 1s 169us/step - loss: 23.3320 - val_loss: 20.4233\n",
      "Epoch 195/200\n",
      "3531/3531 [==============================] - 1s 167us/step - loss: 23.3405 - val_loss: 20.4429\n",
      "Epoch 196/200\n",
      "3531/3531 [==============================] - 1s 157us/step - loss: 23.3164 - val_loss: 20.4541\n",
      "Epoch 197/200\n",
      "3531/3531 [==============================] - 1s 174us/step - loss: 23.3473 - val_loss: 20.4383\n",
      "Epoch 198/200\n",
      "3531/3531 [==============================] - 1s 176us/step - loss: 23.3275 - val_loss: 20.4215\n",
      "Epoch 199/200\n",
      "3531/3531 [==============================] - 1s 147us/step - loss: 23.3204 - val_loss: 20.5650\n",
      "Epoch 200/200\n",
      "3531/3531 [==============================] - 1s 160us/step - loss: 23.3375 - val_loss: 20.4433\n",
      "---------current test year is 2010---------\n",
      "Train on 3408 samples, validate on 1136 samples\n",
      "Epoch 1/200\n",
      "3408/3408 [==============================] - 1s 405us/step - loss: 65.9209 - val_loss: 25.8765\n",
      "Epoch 2/200\n",
      "3408/3408 [==============================] - 1s 179us/step - loss: 21.3644 - val_loss: 22.3934\n",
      "Epoch 3/200\n",
      "3408/3408 [==============================] - 1s 184us/step - loss: 21.2002 - val_loss: 22.2911\n",
      "Epoch 4/200\n",
      "3408/3408 [==============================] - 1s 148us/step - loss: 21.1667 - val_loss: 22.6246\n",
      "Epoch 5/200\n",
      "3408/3408 [==============================] - 0s 140us/step - loss: 21.2172 - val_loss: 22.2919\n",
      "Epoch 6/200\n",
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.1646 - val_loss: 22.2585\n",
      "Epoch 7/200\n",
      "3408/3408 [==============================] - 1s 161us/step - loss: 21.1174 - val_loss: 22.2422\n",
      "Epoch 8/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.1412 - val_loss: 22.2743\n",
      "Epoch 9/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0794 - val_loss: 22.3518\n",
      "Epoch 10/200\n",
      "3408/3408 [==============================] - 0s 146us/step - loss: 21.1251 - val_loss: 22.3643\n",
      "Epoch 11/200\n",
      "3408/3408 [==============================] - 1s 148us/step - loss: 21.0968 - val_loss: 22.2304\n",
      "Epoch 12/200\n",
      "3408/3408 [==============================] - 1s 154us/step - loss: 21.1145 - val_loss: 22.2280\n",
      "Epoch 13/200\n",
      "3408/3408 [==============================] - 1s 183us/step - loss: 21.0930 - val_loss: 22.3138\n",
      "Epoch 14/200\n",
      "3408/3408 [==============================] - 1s 183us/step - loss: 21.0771 - val_loss: 22.4642\n",
      "Epoch 15/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0807 - val_loss: 22.2311\n",
      "Epoch 16/200\n",
      "3408/3408 [==============================] - 0s 141us/step - loss: 21.0790 - val_loss: 22.2788\n",
      "Epoch 17/200\n",
      "3408/3408 [==============================] - 1s 171us/step - loss: 21.0673 - val_loss: 22.2343\n",
      "Epoch 18/200\n",
      "3408/3408 [==============================] - 1s 169us/step - loss: 21.0622 - val_loss: 22.4155\n",
      "Epoch 19/200\n",
      "3408/3408 [==============================] - 1s 170us/step - loss: 21.0391 - val_loss: 22.2423\n",
      "Epoch 20/200\n",
      "3408/3408 [==============================] - 1s 170us/step - loss: 21.0046 - val_loss: 22.4247\n",
      "Epoch 21/200\n",
      "3408/3408 [==============================] - 0s 126us/step - loss: 21.0761 - val_loss: 22.2597\n",
      "Epoch 22/200\n",
      "3408/3408 [==============================] - 0s 132us/step - loss: 21.0519 - val_loss: 22.2515\n",
      "Epoch 23/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0220 - val_loss: 22.4669\n",
      "Epoch 24/200\n",
      "3408/3408 [==============================] - 1s 179us/step - loss: 21.0865 - val_loss: 22.2732\n",
      "Epoch 25/200\n",
      "3408/3408 [==============================] - 1s 164us/step - loss: 21.0381 - val_loss: 22.2682\n",
      "Epoch 26/200\n",
      "3408/3408 [==============================] - 1s 154us/step - loss: 21.0603 - val_loss: 22.2793\n",
      "Epoch 27/200\n",
      "3408/3408 [==============================] - 1s 150us/step - loss: 21.0294 - val_loss: 22.3501\n",
      "Epoch 28/200\n",
      "3408/3408 [==============================] - 1s 151us/step - loss: 21.0133 - val_loss: 22.2495\n",
      "Epoch 29/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0367 - val_loss: 22.2665\n",
      "Epoch 30/200\n",
      "3408/3408 [==============================] - 1s 161us/step - loss: 21.0436 - val_loss: 22.2625\n",
      "Epoch 31/200\n",
      "3408/3408 [==============================] - 1s 162us/step - loss: 21.0418 - val_loss: 22.2805\n",
      "Epoch 32/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0483 - val_loss: 22.2413\n",
      "Epoch 33/200\n",
      "3408/3408 [==============================] - 1s 150us/step - loss: 21.0417 - val_loss: 22.2745\n",
      "Epoch 34/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0281 - val_loss: 22.2576\n",
      "Epoch 35/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0261 - val_loss: 22.2777\n",
      "Epoch 36/200\n",
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.0435 - val_loss: 22.3109\n",
      "Epoch 37/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0308 - val_loss: 22.3128\n",
      "Epoch 38/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0266 - val_loss: 22.3414\n",
      "Epoch 39/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0419 - val_loss: 22.2620\n",
      "Epoch 40/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0255 - val_loss: 22.3431\n",
      "Epoch 41/200\n",
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.0402 - val_loss: 22.3482\n",
      "Epoch 42/200\n",
      "3408/3408 [==============================] - 0s 141us/step - loss: 21.0463 - val_loss: 22.2754\n",
      "Epoch 43/200\n",
      "3408/3408 [==============================] - 0s 144us/step - loss: 21.0473 - val_loss: 22.3085\n",
      "Epoch 44/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0505 - val_loss: 22.2971\n",
      "Epoch 45/200\n",
      "3408/3408 [==============================] - 1s 179us/step - loss: 21.0419 - val_loss: 22.2588\n",
      "Epoch 46/200\n",
      "3408/3408 [==============================] - 1s 194us/step - loss: 21.0314 - val_loss: 22.2915\n",
      "Epoch 47/200\n",
      "3408/3408 [==============================] - 1s 193us/step - loss: 21.0227 - val_loss: 22.2613\n",
      "Epoch 48/200\n",
      "3408/3408 [==============================] - 0s 137us/step - loss: 21.0229 - val_loss: 22.3776\n",
      "Epoch 49/200\n",
      "3408/3408 [==============================] - 0s 136us/step - loss: 21.0319 - val_loss: 22.2649\n",
      "Epoch 50/200\n",
      "3408/3408 [==============================] - 0s 141us/step - loss: 21.0318 - val_loss: 22.2626\n",
      "Epoch 51/200\n",
      "3408/3408 [==============================] - 1s 152us/step - loss: 21.0404 - val_loss: 22.2601\n",
      "Epoch 52/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0218 - val_loss: 22.2767\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.0324 - val_loss: 22.2613\n",
      "Epoch 54/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 21.0041 - val_loss: 22.2894\n",
      "Epoch 55/200\n",
      "3408/3408 [==============================] - 1s 150us/step - loss: 21.0232 - val_loss: 22.3279\n",
      "Epoch 56/200\n",
      "3408/3408 [==============================] - 1s 152us/step - loss: 21.0295 - val_loss: 22.3018\n",
      "Epoch 57/200\n",
      "3408/3408 [==============================] - 1s 154us/step - loss: 21.0318 - val_loss: 22.2614\n",
      "Epoch 58/200\n",
      "3408/3408 [==============================] - 1s 161us/step - loss: 21.0261 - val_loss: 22.2740\n",
      "Epoch 59/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0209 - val_loss: 22.2591\n",
      "Epoch 60/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0317 - val_loss: 22.2622\n",
      "Epoch 61/200\n",
      "3408/3408 [==============================] - 1s 148us/step - loss: 21.0217 - val_loss: 22.2608\n",
      "Epoch 62/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0070 - val_loss: 22.2645\n",
      "Epoch 63/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 21.0406 - val_loss: 22.2618\n",
      "Epoch 64/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 21.0175 - val_loss: 22.3087\n",
      "Epoch 65/200\n",
      "3408/3408 [==============================] - 1s 152us/step - loss: 21.0240 - val_loss: 22.3242\n",
      "Epoch 66/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0327 - val_loss: 22.3023\n",
      "Epoch 67/200\n",
      "3408/3408 [==============================] - 1s 161us/step - loss: 21.0280 - val_loss: 22.2855\n",
      "Epoch 68/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0397 - val_loss: 22.2693\n",
      "Epoch 69/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0054 - val_loss: 22.4152\n",
      "Epoch 70/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0585 - val_loss: 22.3355\n",
      "Epoch 71/200\n",
      "3408/3408 [==============================] - 1s 150us/step - loss: 21.0115 - val_loss: 22.4123\n",
      "Epoch 72/200\n",
      "3408/3408 [==============================] - 0s 139us/step - loss: 21.0652 - val_loss: 22.2765\n",
      "Epoch 73/200\n",
      "3408/3408 [==============================] - 1s 152us/step - loss: 21.0606 - val_loss: 22.2765\n",
      "Epoch 74/200\n",
      "3408/3408 [==============================] - 1s 151us/step - loss: 21.0161 - val_loss: 22.2697\n",
      "Epoch 75/200\n",
      "3408/3408 [==============================] - 1s 163us/step - loss: 21.0111 - val_loss: 22.2675\n",
      "Epoch 76/200\n",
      "3408/3408 [==============================] - 1s 181us/step - loss: 20.9989 - val_loss: 22.3245\n",
      "Epoch 77/200\n",
      "3408/3408 [==============================] - 1s 188us/step - loss: 21.0291 - val_loss: 22.3047\n",
      "Epoch 78/200\n",
      "3408/3408 [==============================] - 1s 185us/step - loss: 21.0178 - val_loss: 22.2627\n",
      "Epoch 79/200\n",
      "3408/3408 [==============================] - 1s 184us/step - loss: 21.0245 - val_loss: 22.2651\n",
      "Epoch 80/200\n",
      "3408/3408 [==============================] - 1s 187us/step - loss: 21.0383 - val_loss: 22.2706\n",
      "Epoch 81/200\n",
      "3408/3408 [==============================] - 1s 185us/step - loss: 21.0086 - val_loss: 22.2740\n",
      "Epoch 82/200\n",
      "3408/3408 [==============================] - 0s 144us/step - loss: 21.0432 - val_loss: 22.2970\n",
      "Epoch 83/200\n",
      "3408/3408 [==============================] - 0s 126us/step - loss: 21.0290 - val_loss: 22.2625\n",
      "Epoch 84/200\n",
      "3408/3408 [==============================] - 0s 138us/step - loss: 21.0352 - val_loss: 22.2634\n",
      "Epoch 85/200\n",
      "3408/3408 [==============================] - 0s 138us/step - loss: 21.0413 - val_loss: 22.2718\n",
      "Epoch 86/200\n",
      "3408/3408 [==============================] - 0s 138us/step - loss: 21.0309 - val_loss: 22.3176\n",
      "Epoch 87/200\n",
      "3408/3408 [==============================] - 0s 132us/step - loss: 21.0115 - val_loss: 22.3406\n",
      "Epoch 88/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0325 - val_loss: 22.2663\n",
      "Epoch 89/200\n",
      "3408/3408 [==============================] - 1s 179us/step - loss: 20.9890 - val_loss: 22.3284\n",
      "Epoch 90/200\n",
      "3408/3408 [==============================] - 1s 185us/step - loss: 21.0170 - val_loss: 22.2824\n",
      "Epoch 91/200\n",
      "3408/3408 [==============================] - 1s 187us/step - loss: 21.0220 - val_loss: 22.3042\n",
      "Epoch 92/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0505 - val_loss: 22.3132\n",
      "Epoch 93/200\n",
      "3408/3408 [==============================] - 0s 138us/step - loss: 21.0166 - val_loss: 22.2983\n",
      "Epoch 94/200\n",
      "3408/3408 [==============================] - 0s 134us/step - loss: 21.0474 - val_loss: 22.3211\n",
      "Epoch 95/200\n",
      "3408/3408 [==============================] - 0s 142us/step - loss: 21.0263 - val_loss: 22.3167\n",
      "Epoch 96/200\n",
      "3408/3408 [==============================] - 1s 162us/step - loss: 20.9857 - val_loss: 22.2879\n",
      "Epoch 97/200\n",
      "3408/3408 [==============================] - 1s 163us/step - loss: 20.9705 - val_loss: 22.3133\n",
      "Epoch 98/200\n",
      "3408/3408 [==============================] - 1s 161us/step - loss: 21.0589 - val_loss: 22.3093\n",
      "Epoch 99/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0142 - val_loss: 22.3344\n",
      "Epoch 100/200\n",
      "3408/3408 [==============================] - 0s 135us/step - loss: 21.0431 - val_loss: 22.2667\n",
      "Epoch 101/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0155 - val_loss: 22.2778\n",
      "Epoch 102/200\n",
      "3408/3408 [==============================] - 1s 150us/step - loss: 21.0264 - val_loss: 22.2713\n",
      "Epoch 103/200\n",
      "3408/3408 [==============================] - 1s 180us/step - loss: 21.0402 - val_loss: 22.2656\n",
      "Epoch 104/200\n",
      "3408/3408 [==============================] - 1s 192us/step - loss: 21.0349 - val_loss: 22.2961\n",
      "Epoch 105/200\n",
      "3408/3408 [==============================] - 0s 143us/step - loss: 21.0084 - val_loss: 22.3573\n",
      "Epoch 106/200\n",
      "3408/3408 [==============================] - 0s 140us/step - loss: 21.0362 - val_loss: 22.3024\n",
      "Epoch 107/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0070 - val_loss: 22.2851\n",
      "Epoch 108/200\n",
      "3408/3408 [==============================] - 1s 164us/step - loss: 21.0228 - val_loss: 22.2735\n",
      "Epoch 109/200\n",
      "3408/3408 [==============================] - 1s 164us/step - loss: 21.0293 - val_loss: 22.2690\n",
      "Epoch 110/200\n",
      "3408/3408 [==============================] - 1s 161us/step - loss: 21.0311 - val_loss: 22.2852\n",
      "Epoch 111/200\n",
      "3408/3408 [==============================] - 0s 138us/step - loss: 21.0232 - val_loss: 22.2608\n",
      "Epoch 112/200\n",
      "3408/3408 [==============================] - 1s 150us/step - loss: 21.0073 - val_loss: 22.2961\n",
      "Epoch 113/200\n",
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.0352 - val_loss: 22.2783\n",
      "Epoch 114/200\n",
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.0290 - val_loss: 22.2646\n",
      "Epoch 115/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 21.0182 - val_loss: 22.2660\n",
      "Epoch 116/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0049 - val_loss: 22.3151\n",
      "Epoch 117/200\n",
      "3408/3408 [==============================] - 0s 141us/step - loss: 20.9984 - val_loss: 22.3875\n",
      "Epoch 118/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0289 - val_loss: 22.2618\n",
      "Epoch 119/200\n",
      "3408/3408 [==============================] - 1s 171us/step - loss: 21.0138 - val_loss: 22.2880\n",
      "Epoch 120/200\n",
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.0401 - val_loss: 22.2981\n",
      "Epoch 121/200\n",
      "3408/3408 [==============================] - 1s 167us/step - loss: 21.0336 - val_loss: 22.2623\n",
      "Epoch 122/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0161 - val_loss: 22.2953\n",
      "Epoch 123/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0319 - val_loss: 22.2764\n",
      "Epoch 124/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0356 - val_loss: 22.2664\n",
      "Epoch 125/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0169 - val_loss: 22.2675\n",
      "Epoch 126/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0136 - val_loss: 22.3167\n",
      "Epoch 127/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0284 - val_loss: 22.2651\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408/3408 [==============================] - 0s 147us/step - loss: 21.0445 - val_loss: 22.2686\n",
      "Epoch 129/200\n",
      "3408/3408 [==============================] - 1s 150us/step - loss: 21.0263 - val_loss: 22.2891\n",
      "Epoch 130/200\n",
      "3408/3408 [==============================] - 1s 153us/step - loss: 21.0100 - val_loss: 22.2950\n",
      "Epoch 131/200\n",
      "3408/3408 [==============================] - 0s 141us/step - loss: 21.0161 - val_loss: 22.2657\n",
      "Epoch 132/200\n",
      "3408/3408 [==============================] - 1s 173us/step - loss: 21.0340 - val_loss: 22.2819\n",
      "Epoch 133/200\n",
      "3408/3408 [==============================] - 1s 171us/step - loss: 21.0399 - val_loss: 22.2619\n",
      "Epoch 134/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0003 - val_loss: 22.3350\n",
      "Epoch 135/200\n",
      "3408/3408 [==============================] - 1s 151us/step - loss: 21.0186 - val_loss: 22.2622\n",
      "Epoch 136/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 20.9949 - val_loss: 22.2874\n",
      "Epoch 137/200\n",
      "3408/3408 [==============================] - 1s 154us/step - loss: 21.0067 - val_loss: 22.3336\n",
      "Epoch 138/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0324 - val_loss: 22.3027\n",
      "Epoch 139/200\n",
      "3408/3408 [==============================] - 0s 147us/step - loss: 21.0376 - val_loss: 22.2641\n",
      "Epoch 140/200\n",
      "3408/3408 [==============================] - 0s 144us/step - loss: 21.0224 - val_loss: 22.2807\n",
      "Epoch 141/200\n",
      "3408/3408 [==============================] - 1s 155us/step - loss: 21.0113 - val_loss: 22.2914\n",
      "Epoch 142/200\n",
      "3408/3408 [==============================] - 1s 176us/step - loss: 20.9896 - val_loss: 22.3054\n",
      "Epoch 143/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0092 - val_loss: 22.3083\n",
      "Epoch 144/200\n",
      "3408/3408 [==============================] - 0s 144us/step - loss: 21.0247 - val_loss: 22.2881\n",
      "Epoch 145/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 21.0251 - val_loss: 22.2694\n",
      "Epoch 146/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0000 - val_loss: 22.2702\n",
      "Epoch 147/200\n",
      "3408/3408 [==============================] - 1s 154us/step - loss: 21.0297 - val_loss: 22.2703\n",
      "Epoch 148/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 21.0075 - val_loss: 22.3595\n",
      "Epoch 149/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0243 - val_loss: 22.2595\n",
      "Epoch 150/200\n",
      "3408/3408 [==============================] - 1s 154us/step - loss: 21.0273 - val_loss: 22.2873\n",
      "Epoch 151/200\n",
      "3408/3408 [==============================] - 0s 144us/step - loss: 21.0342 - val_loss: 22.2716\n",
      "Epoch 152/200\n",
      "3408/3408 [==============================] - 0s 141us/step - loss: 21.0208 - val_loss: 22.2807\n",
      "Epoch 153/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0208 - val_loss: 22.3716\n",
      "Epoch 154/200\n",
      "3408/3408 [==============================] - 1s 177us/step - loss: 21.0144 - val_loss: 22.3046\n",
      "Epoch 155/200\n",
      "3408/3408 [==============================] - 0s 142us/step - loss: 20.9949 - val_loss: 22.2700\n",
      "Epoch 156/200\n",
      "3408/3408 [==============================] - 0s 140us/step - loss: 21.0025 - val_loss: 22.3122\n",
      "Epoch 157/200\n",
      "3408/3408 [==============================] - 1s 165us/step - loss: 21.0405 - val_loss: 22.2751\n",
      "Epoch 158/200\n",
      "3408/3408 [==============================] - 1s 168us/step - loss: 21.0287 - val_loss: 22.2804\n",
      "Epoch 159/200\n",
      "3408/3408 [==============================] - 1s 177us/step - loss: 21.0161 - val_loss: 22.2657\n",
      "Epoch 160/200\n",
      "3408/3408 [==============================] - 1s 175us/step - loss: 21.0139 - val_loss: 22.2758\n",
      "Epoch 161/200\n",
      "3408/3408 [==============================] - 1s 177us/step - loss: 20.9826 - val_loss: 22.4606\n",
      "Epoch 162/200\n",
      "3408/3408 [==============================] - 1s 179us/step - loss: 21.0326 - val_loss: 22.2730\n",
      "Epoch 163/200\n",
      "3408/3408 [==============================] - 1s 176us/step - loss: 21.0131 - val_loss: 22.2648\n",
      "Epoch 164/200\n",
      "3408/3408 [==============================] - 1s 179us/step - loss: 21.0011 - val_loss: 22.3056\n",
      "Epoch 165/200\n",
      "3408/3408 [==============================] - 1s 176us/step - loss: 21.0106 - val_loss: 22.2640\n",
      "Epoch 166/200\n",
      "3408/3408 [==============================] - 1s 164us/step - loss: 21.0153 - val_loss: 22.2767\n",
      "Epoch 167/200\n",
      "3408/3408 [==============================] - 0s 135us/step - loss: 21.0199 - val_loss: 22.2873\n",
      "Epoch 168/200\n",
      "3408/3408 [==============================] - 0s 139us/step - loss: 21.0002 - val_loss: 22.2904\n",
      "Epoch 169/200\n",
      "3408/3408 [==============================] - 0s 137us/step - loss: 20.9743 - val_loss: 22.2797\n",
      "Epoch 170/200\n",
      "3408/3408 [==============================] - 0s 137us/step - loss: 21.0155 - val_loss: 22.3562\n",
      "Epoch 171/200\n",
      "3408/3408 [==============================] - 0s 138us/step - loss: 20.9757 - val_loss: 22.3097\n",
      "Epoch 172/200\n",
      "3408/3408 [==============================] - 0s 139us/step - loss: 21.0205 - val_loss: 22.2839\n",
      "Epoch 173/200\n",
      "3408/3408 [==============================] - 0s 136us/step - loss: 21.0107 - val_loss: 22.3192\n",
      "Epoch 174/200\n",
      "3408/3408 [==============================] - 1s 159us/step - loss: 21.0006 - val_loss: 22.2844\n",
      "Epoch 175/200\n",
      "3408/3408 [==============================] - 1s 191us/step - loss: 21.0017 - val_loss: 22.3476\n",
      "Epoch 176/200\n",
      "3408/3408 [==============================] - 1s 167us/step - loss: 21.0376 - val_loss: 22.2758\n",
      "Epoch 177/200\n",
      "3408/3408 [==============================] - 0s 141us/step - loss: 20.9976 - val_loss: 22.2786\n",
      "Epoch 178/200\n",
      "3408/3408 [==============================] - 0s 144us/step - loss: 21.0089 - val_loss: 22.3011\n",
      "Epoch 179/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0241 - val_loss: 22.2838\n",
      "Epoch 180/200\n",
      "3408/3408 [==============================] - 1s 183us/step - loss: 21.0206 - val_loss: 22.3463\n",
      "Epoch 181/200\n",
      "3408/3408 [==============================] - 1s 187us/step - loss: 21.0224 - val_loss: 22.2807\n",
      "Epoch 182/200\n",
      "3408/3408 [==============================] - 1s 184us/step - loss: 21.0171 - val_loss: 22.2896\n",
      "Epoch 183/200\n",
      "3408/3408 [==============================] - 1s 168us/step - loss: 21.0273 - val_loss: 22.2790\n",
      "Epoch 184/200\n",
      "3408/3408 [==============================] - 0s 132us/step - loss: 21.0052 - val_loss: 22.2650\n",
      "Epoch 185/200\n",
      "3408/3408 [==============================] - 0s 138us/step - loss: 21.0222 - val_loss: 22.2817\n",
      "Epoch 186/200\n",
      "3408/3408 [==============================] - 0s 137us/step - loss: 21.0030 - val_loss: 22.2835\n",
      "Epoch 187/200\n",
      "3408/3408 [==============================] - 0s 144us/step - loss: 21.0072 - val_loss: 22.3295\n",
      "Epoch 188/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 20.9908 - val_loss: 22.3111\n",
      "Epoch 189/200\n",
      "3408/3408 [==============================] - 1s 162us/step - loss: 21.0282 - val_loss: 22.2656\n",
      "Epoch 190/200\n",
      "3408/3408 [==============================] - 1s 151us/step - loss: 21.0257 - val_loss: 22.2626\n",
      "Epoch 191/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0127 - val_loss: 22.2777\n",
      "Epoch 192/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 20.9765 - val_loss: 22.3732\n",
      "Epoch 193/200\n",
      "3408/3408 [==============================] - 1s 156us/step - loss: 21.0175 - val_loss: 22.3083\n",
      "Epoch 194/200\n",
      "3408/3408 [==============================] - 1s 158us/step - loss: 21.0148 - val_loss: 22.2854\n",
      "Epoch 195/200\n",
      "3408/3408 [==============================] - 1s 149us/step - loss: 21.0057 - val_loss: 22.3050\n",
      "Epoch 196/200\n",
      "3408/3408 [==============================] - 1s 151us/step - loss: 21.0222 - val_loss: 22.2634\n",
      "Epoch 197/200\n",
      "3408/3408 [==============================] - 1s 157us/step - loss: 21.0213 - val_loss: 22.2614\n",
      "Epoch 198/200\n",
      "3408/3408 [==============================] - 1s 162us/step - loss: 21.0117 - val_loss: 22.2729\n",
      "Epoch 199/200\n",
      "3408/3408 [==============================] - 1s 160us/step - loss: 21.0149 - val_loss: 22.2721\n",
      "Epoch 200/200\n",
      "3408/3408 [==============================] - 1s 164us/step - loss: 20.9982 - val_loss: 22.3343\n",
      "---------current test year is 2011---------\n",
      "Train on 3371 samples, validate on 1124 samples\n",
      "Epoch 1/200\n",
      "3371/3371 [==============================] - 1s 406us/step - loss: 94.0707 - val_loss: 88.1933\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 1s 151us/step - loss: 82.8651 - val_loss: 77.0124\n",
      "Epoch 3/200\n",
      "3371/3371 [==============================] - 1s 166us/step - loss: 72.3074 - val_loss: 66.4330\n",
      "Epoch 4/200\n",
      "3371/3371 [==============================] - 0s 145us/step - loss: 62.6740 - val_loss: 56.5886\n",
      "Epoch 5/200\n",
      "3371/3371 [==============================] - 1s 167us/step - loss: 53.7277 - val_loss: 47.2119\n",
      "Epoch 6/200\n",
      "3371/3371 [==============================] - 1s 166us/step - loss: 45.1369 - val_loss: 38.2505\n",
      "Epoch 7/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 37.0508 - val_loss: 29.9345\n",
      "Epoch 8/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 30.2332 - val_loss: 24.3278\n",
      "Epoch 9/200\n",
      "3371/3371 [==============================] - 0s 147us/step - loss: 26.8291 - val_loss: 21.9807\n",
      "Epoch 10/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 25.6149 - val_loss: 21.2187\n",
      "Epoch 11/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 25.2280 - val_loss: 20.8315\n",
      "Epoch 12/200\n",
      "3371/3371 [==============================] - 1s 163us/step - loss: 25.0682 - val_loss: 20.6503\n",
      "Epoch 13/200\n",
      "3371/3371 [==============================] - 1s 163us/step - loss: 25.0009 - val_loss: 20.5391\n",
      "Epoch 14/200\n",
      "3371/3371 [==============================] - 1s 177us/step - loss: 24.9807 - val_loss: 20.5350\n",
      "Epoch 15/200\n",
      "3371/3371 [==============================] - 1s 214us/step - loss: 24.9796 - val_loss: 20.5221\n",
      "Epoch 16/200\n",
      "3371/3371 [==============================] - 1s 219us/step - loss: 24.9811 - val_loss: 20.5148\n",
      "Epoch 17/200\n",
      "3371/3371 [==============================] - 1s 205us/step - loss: 24.9789 - val_loss: 20.5193\n",
      "Epoch 18/200\n",
      "3371/3371 [==============================] - 1s 189us/step - loss: 24.9822 - val_loss: 20.5070\n",
      "Epoch 19/200\n",
      "3371/3371 [==============================] - 1s 167us/step - loss: 24.9779 - val_loss: 20.5031\n",
      "Epoch 20/200\n",
      "3371/3371 [==============================] - 1s 158us/step - loss: 24.9811 - val_loss: 20.5288\n",
      "Epoch 21/200\n",
      "3371/3371 [==============================] - 1s 162us/step - loss: 24.9808 - val_loss: 20.5270\n",
      "Epoch 22/200\n",
      "3371/3371 [==============================] - 1s 161us/step - loss: 24.9769 - val_loss: 20.4971\n",
      "Epoch 23/200\n",
      "3371/3371 [==============================] - 1s 166us/step - loss: 24.9819 - val_loss: 20.5061\n",
      "Epoch 24/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9833 - val_loss: 20.5269\n",
      "Epoch 25/200\n",
      "3371/3371 [==============================] - 0s 145us/step - loss: 24.9777 - val_loss: 20.5063\n",
      "Epoch 26/200\n",
      "3371/3371 [==============================] - 1s 169us/step - loss: 24.9802 - val_loss: 20.5173\n",
      "Epoch 27/200\n",
      "3371/3371 [==============================] - 1s 163us/step - loss: 24.9784 - val_loss: 20.5240\n",
      "Epoch 28/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9869 - val_loss: 20.5296\n",
      "Epoch 29/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9798 - val_loss: 20.5302\n",
      "Epoch 30/200\n",
      "3371/3371 [==============================] - 1s 161us/step - loss: 24.9797 - val_loss: 20.5077\n",
      "Epoch 31/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9819 - val_loss: 20.5168\n",
      "Epoch 32/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9819 - val_loss: 20.5251\n",
      "Epoch 33/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9821 - val_loss: 20.5200\n",
      "Epoch 34/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9769 - val_loss: 20.5085\n",
      "Epoch 35/200\n",
      "3371/3371 [==============================] - 1s 158us/step - loss: 24.9803 - val_loss: 20.4968\n",
      "Epoch 36/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9815 - val_loss: 20.5047\n",
      "Epoch 37/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9806 - val_loss: 20.5042\n",
      "Epoch 38/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9790 - val_loss: 20.5173\n",
      "Epoch 39/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9783 - val_loss: 20.5162\n",
      "Epoch 40/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9840 - val_loss: 20.5123\n",
      "Epoch 41/200\n",
      "3371/3371 [==============================] - 1s 163us/step - loss: 24.9803 - val_loss: 20.5321\n",
      "Epoch 42/200\n",
      "3371/3371 [==============================] - 1s 160us/step - loss: 24.9782 - val_loss: 20.5145\n",
      "Epoch 43/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9799 - val_loss: 20.5063\n",
      "Epoch 44/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9796 - val_loss: 20.5121\n",
      "Epoch 45/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9795 - val_loss: 20.5171\n",
      "Epoch 46/200\n",
      "3371/3371 [==============================] - 1s 159us/step - loss: 24.9790 - val_loss: 20.5304\n",
      "Epoch 47/200\n",
      "3371/3371 [==============================] - 1s 161us/step - loss: 24.9784 - val_loss: 20.4970\n",
      "Epoch 48/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9841 - val_loss: 20.5097\n",
      "Epoch 49/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9815 - val_loss: 20.5113\n",
      "Epoch 50/200\n",
      "3371/3371 [==============================] - 1s 158us/step - loss: 24.9787 - val_loss: 20.5096\n",
      "Epoch 51/200\n",
      "3371/3371 [==============================] - 0s 148us/step - loss: 24.9776 - val_loss: 20.5062\n",
      "Epoch 52/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9817 - val_loss: 20.5171\n",
      "Epoch 53/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9784 - val_loss: 20.5182\n",
      "Epoch 54/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9791 - val_loss: 20.5063\n",
      "Epoch 55/200\n",
      "3371/3371 [==============================] - 1s 181us/step - loss: 24.9786 - val_loss: 20.5141\n",
      "Epoch 56/200\n",
      "3371/3371 [==============================] - 1s 187us/step - loss: 24.9808 - val_loss: 20.5366\n",
      "Epoch 57/200\n",
      "3371/3371 [==============================] - 1s 184us/step - loss: 24.9810 - val_loss: 20.5411\n",
      "Epoch 58/200\n",
      "3371/3371 [==============================] - 1s 167us/step - loss: 24.9798 - val_loss: 20.5124\n",
      "Epoch 59/200\n",
      "3371/3371 [==============================] - 0s 128us/step - loss: 24.9793 - val_loss: 20.5195\n",
      "Epoch 60/200\n",
      "3371/3371 [==============================] - 0s 128us/step - loss: 24.9793 - val_loss: 20.5061\n",
      "Epoch 61/200\n",
      "3371/3371 [==============================] - 0s 142us/step - loss: 24.9787 - val_loss: 20.5080\n",
      "Epoch 62/200\n",
      "3371/3371 [==============================] - 1s 152us/step - loss: 24.9822 - val_loss: 20.5249\n",
      "Epoch 63/200\n",
      "3371/3371 [==============================] - 1s 164us/step - loss: 24.9823 - val_loss: 20.5088\n",
      "Epoch 64/200\n",
      "3371/3371 [==============================] - 1s 166us/step - loss: 24.9814 - val_loss: 20.5132\n",
      "Epoch 65/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9806 - val_loss: 20.5076\n",
      "Epoch 66/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9765 - val_loss: 20.5005\n",
      "Epoch 67/200\n",
      "3371/3371 [==============================] - 1s 159us/step - loss: 24.9833 - val_loss: 20.5147\n",
      "Epoch 68/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9772 - val_loss: 20.5087\n",
      "Epoch 69/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9782 - val_loss: 20.5093\n",
      "Epoch 70/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9776 - val_loss: 20.5336\n",
      "Epoch 71/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9828 - val_loss: 20.5215\n",
      "Epoch 72/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9815 - val_loss: 20.5089\n",
      "Epoch 73/200\n",
      "3371/3371 [==============================] - 1s 159us/step - loss: 24.9795 - val_loss: 20.5109\n",
      "Epoch 74/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9814 - val_loss: 20.5157\n",
      "Epoch 75/200\n",
      "3371/3371 [==============================] - 1s 159us/step - loss: 24.9837 - val_loss: 20.5180\n",
      "Epoch 76/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9778 - val_loss: 20.5162\n",
      "Epoch 77/200\n",
      "3371/3371 [==============================] - 0s 147us/step - loss: 24.9774 - val_loss: 20.5152\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 1s 152us/step - loss: 24.9777 - val_loss: 20.5098\n",
      "Epoch 79/200\n",
      "3371/3371 [==============================] - 1s 158us/step - loss: 24.9776 - val_loss: 20.5151\n",
      "Epoch 80/200\n",
      "3371/3371 [==============================] - 0s 148us/step - loss: 24.9818 - val_loss: 20.5301\n",
      "Epoch 81/200\n",
      "3371/3371 [==============================] - 1s 169us/step - loss: 24.9817 - val_loss: 20.5484\n",
      "Epoch 82/200\n",
      "3371/3371 [==============================] - 1s 167us/step - loss: 24.9826 - val_loss: 20.5117\n",
      "Epoch 83/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9788 - val_loss: 20.5132\n",
      "Epoch 84/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9806 - val_loss: 20.5150\n",
      "Epoch 85/200\n",
      "3371/3371 [==============================] - 1s 150us/step - loss: 24.9816 - val_loss: 20.5073\n",
      "Epoch 86/200\n",
      "3371/3371 [==============================] - 1s 163us/step - loss: 24.9838 - val_loss: 20.5172\n",
      "Epoch 87/200\n",
      "3371/3371 [==============================] - 1s 160us/step - loss: 24.9776 - val_loss: 20.5082\n",
      "Epoch 88/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9763 - val_loss: 20.5046\n",
      "Epoch 89/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9823 - val_loss: 20.5142\n",
      "Epoch 90/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9798 - val_loss: 20.5078\n",
      "Epoch 91/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9750 - val_loss: 20.5068\n",
      "Epoch 92/200\n",
      "3371/3371 [==============================] - 0s 146us/step - loss: 24.9766 - val_loss: 20.5020\n",
      "Epoch 93/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9837 - val_loss: 20.5146\n",
      "Epoch 94/200\n",
      "3371/3371 [==============================] - 1s 168us/step - loss: 24.9789 - val_loss: 20.5187\n",
      "Epoch 95/200\n",
      "3371/3371 [==============================] - 1s 166us/step - loss: 24.9817 - val_loss: 20.5235\n",
      "Epoch 96/200\n",
      "3371/3371 [==============================] - 1s 172us/step - loss: 24.9756 - val_loss: 20.5026\n",
      "Epoch 97/200\n",
      "3371/3371 [==============================] - 1s 171us/step - loss: 24.9792 - val_loss: 20.5214\n",
      "Epoch 98/200\n",
      "3371/3371 [==============================] - 1s 170us/step - loss: 24.9778 - val_loss: 20.5174\n",
      "Epoch 99/200\n",
      "3371/3371 [==============================] - 0s 136us/step - loss: 24.9772 - val_loss: 20.5020\n",
      "Epoch 100/200\n",
      "3371/3371 [==============================] - 0s 127us/step - loss: 24.9905 - val_loss: 20.5149\n",
      "Epoch 101/200\n",
      "3371/3371 [==============================] - 0s 147us/step - loss: 24.9793 - val_loss: 20.5129\n",
      "Epoch 102/200\n",
      "3371/3371 [==============================] - 1s 150us/step - loss: 24.9740 - val_loss: 20.4939\n",
      "Epoch 103/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9823 - val_loss: 20.5029\n",
      "Epoch 104/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9818 - val_loss: 20.5059\n",
      "Epoch 105/200\n",
      "3371/3371 [==============================] - 0s 146us/step - loss: 24.9788 - val_loss: 20.5078\n",
      "Epoch 106/200\n",
      "3371/3371 [==============================] - 1s 150us/step - loss: 24.9769 - val_loss: 20.5165\n",
      "Epoch 107/200\n",
      "3371/3371 [==============================] - 1s 162us/step - loss: 24.9780 - val_loss: 20.5121\n",
      "Epoch 108/200\n",
      "3371/3371 [==============================] - 1s 172us/step - loss: 24.9774 - val_loss: 20.5341\n",
      "Epoch 109/200\n",
      "3371/3371 [==============================] - 1s 178us/step - loss: 24.9803 - val_loss: 20.5406\n",
      "Epoch 110/200\n",
      "3371/3371 [==============================] - 1s 160us/step - loss: 24.9794 - val_loss: 20.5109\n",
      "Epoch 111/200\n",
      "3371/3371 [==============================] - 0s 142us/step - loss: 24.9765 - val_loss: 20.5079\n",
      "Epoch 112/200\n",
      "3371/3371 [==============================] - 0s 145us/step - loss: 24.9828 - val_loss: 20.5072\n",
      "Epoch 113/200\n",
      "3371/3371 [==============================] - 1s 152us/step - loss: 24.9794 - val_loss: 20.5245\n",
      "Epoch 114/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9789 - val_loss: 20.5191\n",
      "Epoch 115/200\n",
      "3371/3371 [==============================] - 1s 165us/step - loss: 24.9776 - val_loss: 20.5047\n",
      "Epoch 116/200\n",
      "3371/3371 [==============================] - 1s 152us/step - loss: 24.9852 - val_loss: 20.5073\n",
      "Epoch 117/200\n",
      "3371/3371 [==============================] - 0s 148us/step - loss: 24.9761 - val_loss: 20.5171\n",
      "Epoch 118/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9857 - val_loss: 20.5147\n",
      "Epoch 119/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9789 - val_loss: 20.5114\n",
      "Epoch 120/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9777 - val_loss: 20.5148\n",
      "Epoch 121/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9806 - val_loss: 20.5117\n",
      "Epoch 122/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9798 - val_loss: 20.5112\n",
      "Epoch 123/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9837 - val_loss: 20.5220\n",
      "Epoch 124/200\n",
      "3371/3371 [==============================] - 1s 152us/step - loss: 24.9767 - val_loss: 20.5273\n",
      "Epoch 125/200\n",
      "3371/3371 [==============================] - 1s 169us/step - loss: 24.9800 - val_loss: 20.5270\n",
      "Epoch 126/200\n",
      "3371/3371 [==============================] - 1s 178us/step - loss: 24.9792 - val_loss: 20.5140\n",
      "Epoch 127/200\n",
      "3371/3371 [==============================] - 1s 173us/step - loss: 24.9790 - val_loss: 20.5265\n",
      "Epoch 128/200\n",
      "3371/3371 [==============================] - 0s 131us/step - loss: 24.9777 - val_loss: 20.5181\n",
      "Epoch 129/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9747 - val_loss: 20.5018\n",
      "Epoch 130/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9830 - val_loss: 20.5129\n",
      "Epoch 131/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9795 - val_loss: 20.5048\n",
      "Epoch 132/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9798 - val_loss: 20.5243\n",
      "Epoch 133/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9778 - val_loss: 20.5123\n",
      "Epoch 134/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9756 - val_loss: 20.5008\n",
      "Epoch 135/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9801 - val_loss: 20.5080\n",
      "Epoch 136/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9744 - val_loss: 20.4993\n",
      "Epoch 137/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9802 - val_loss: 20.5127\n",
      "Epoch 138/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9767 - val_loss: 20.5103\n",
      "Epoch 139/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9760 - val_loss: 20.5184\n",
      "Epoch 140/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9792 - val_loss: 20.5303\n",
      "Epoch 141/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9790 - val_loss: 20.5247\n",
      "Epoch 142/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9792 - val_loss: 20.5188\n",
      "Epoch 143/200\n",
      "3371/3371 [==============================] - 1s 158us/step - loss: 24.9799 - val_loss: 20.5404\n",
      "Epoch 144/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9813 - val_loss: 20.5265\n",
      "Epoch 145/200\n",
      "3371/3371 [==============================] - 1s 159us/step - loss: 24.9799 - val_loss: 20.5075\n",
      "Epoch 146/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9800 - val_loss: 20.5161\n",
      "Epoch 147/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9764 - val_loss: 20.5078\n",
      "Epoch 148/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9802 - val_loss: 20.5184\n",
      "Epoch 149/200\n",
      "3371/3371 [==============================] - 0s 144us/step - loss: 24.9762 - val_loss: 20.5191\n",
      "Epoch 150/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9747 - val_loss: 20.5088\n",
      "Epoch 151/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9816 - val_loss: 20.5306\n",
      "Epoch 152/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9794 - val_loss: 20.5193\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9807 - val_loss: 20.5156\n",
      "Epoch 154/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9783 - val_loss: 20.5160\n",
      "Epoch 155/200\n",
      "3371/3371 [==============================] - 1s 150us/step - loss: 24.9786 - val_loss: 20.5231\n",
      "Epoch 156/200\n",
      "3371/3371 [==============================] - 1s 167us/step - loss: 24.9766 - val_loss: 20.5062\n",
      "Epoch 157/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9792 - val_loss: 20.5083\n",
      "Epoch 158/200\n",
      "3371/3371 [==============================] - 1s 164us/step - loss: 24.9785 - val_loss: 20.5141\n",
      "Epoch 159/200\n",
      "3371/3371 [==============================] - 1s 172us/step - loss: 24.9747 - val_loss: 20.5015\n",
      "Epoch 160/200\n",
      "3371/3371 [==============================] - 0s 139us/step - loss: 24.9819 - val_loss: 20.5044\n",
      "Epoch 161/200\n",
      "3371/3371 [==============================] - 0s 137us/step - loss: 24.9777 - val_loss: 20.5164\n",
      "Epoch 162/200\n",
      "3371/3371 [==============================] - 1s 160us/step - loss: 24.9798 - val_loss: 20.5232\n",
      "Epoch 163/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9804 - val_loss: 20.5159\n",
      "Epoch 164/200\n",
      "3371/3371 [==============================] - 1s 185us/step - loss: 24.9789 - val_loss: 20.5347\n",
      "Epoch 165/200\n",
      "3371/3371 [==============================] - 1s 183us/step - loss: 24.9784 - val_loss: 20.5147\n",
      "Epoch 166/200\n",
      "3371/3371 [==============================] - 0s 144us/step - loss: 24.9787 - val_loss: 20.5141\n",
      "Epoch 167/200\n",
      "3371/3371 [==============================] - 0s 137us/step - loss: 24.9782 - val_loss: 20.5188\n",
      "Epoch 168/200\n",
      "3371/3371 [==============================] - 0s 138us/step - loss: 24.9766 - val_loss: 20.5109\n",
      "Epoch 169/200\n",
      "3371/3371 [==============================] - 0s 144us/step - loss: 24.9786 - val_loss: 20.5278\n",
      "Epoch 170/200\n",
      "3371/3371 [==============================] - 1s 159us/step - loss: 24.9783 - val_loss: 20.5121\n",
      "Epoch 171/200\n",
      "3371/3371 [==============================] - 1s 171us/step - loss: 24.9763 - val_loss: 20.5105\n",
      "Epoch 172/200\n",
      "3371/3371 [==============================] - 1s 169us/step - loss: 24.9796 - val_loss: 20.5084\n",
      "Epoch 173/200\n",
      "3371/3371 [==============================] - 1s 166us/step - loss: 24.9778 - val_loss: 20.5295\n",
      "Epoch 174/200\n",
      "3371/3371 [==============================] - 0s 144us/step - loss: 24.9825 - val_loss: 20.5357\n",
      "Epoch 175/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9816 - val_loss: 20.5183\n",
      "Epoch 176/200\n",
      "3371/3371 [==============================] - 1s 155us/step - loss: 24.9790 - val_loss: 20.5172\n",
      "Epoch 177/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9778 - val_loss: 20.5115\n",
      "Epoch 178/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9784 - val_loss: 20.5210\n",
      "Epoch 179/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9764 - val_loss: 20.5093\n",
      "Epoch 180/200\n",
      "3371/3371 [==============================] - 0s 148us/step - loss: 24.9803 - val_loss: 20.5134\n",
      "Epoch 181/200\n",
      "3371/3371 [==============================] - 0s 145us/step - loss: 24.9782 - val_loss: 20.5325\n",
      "Epoch 182/200\n",
      "3371/3371 [==============================] - 1s 157us/step - loss: 24.9797 - val_loss: 20.5132\n",
      "Epoch 183/200\n",
      "3371/3371 [==============================] - 1s 164us/step - loss: 24.9776 - val_loss: 20.5129\n",
      "Epoch 184/200\n",
      "3371/3371 [==============================] - 1s 149us/step - loss: 24.9793 - val_loss: 20.5124\n",
      "Epoch 185/200\n",
      "3371/3371 [==============================] - 0s 148us/step - loss: 24.9824 - val_loss: 20.5198\n",
      "Epoch 186/200\n",
      "3371/3371 [==============================] - 1s 168us/step - loss: 24.9795 - val_loss: 20.5326\n",
      "Epoch 187/200\n",
      "3371/3371 [==============================] - 1s 174us/step - loss: 24.9747 - val_loss: 20.5018\n",
      "Epoch 188/200\n",
      "3371/3371 [==============================] - 1s 171us/step - loss: 24.9815 - val_loss: 20.5131\n",
      "Epoch 189/200\n",
      "3371/3371 [==============================] - 1s 170us/step - loss: 24.9790 - val_loss: 20.5158\n",
      "Epoch 190/200\n",
      "3371/3371 [==============================] - 1s 166us/step - loss: 24.9776 - val_loss: 20.5189\n",
      "Epoch 191/200\n",
      "3371/3371 [==============================] - 0s 133us/step - loss: 24.9782 - val_loss: 20.5152\n",
      "Epoch 192/200\n",
      "3371/3371 [==============================] - 0s 131us/step - loss: 24.9749 - val_loss: 20.5293\n",
      "Epoch 193/200\n",
      "3371/3371 [==============================] - 0s 147us/step - loss: 24.9803 - val_loss: 20.5193\n",
      "Epoch 194/200\n",
      "3371/3371 [==============================] - 1s 151us/step - loss: 24.9770 - val_loss: 20.5101\n",
      "Epoch 195/200\n",
      "3371/3371 [==============================] - 1s 158us/step - loss: 24.9857 - val_loss: 20.5117\n",
      "Epoch 196/200\n",
      "3371/3371 [==============================] - 1s 158us/step - loss: 24.9775 - val_loss: 20.5114\n",
      "Epoch 197/200\n",
      "3371/3371 [==============================] - 1s 153us/step - loss: 24.9822 - val_loss: 20.5137\n",
      "Epoch 198/200\n",
      "3371/3371 [==============================] - 1s 154us/step - loss: 24.9778 - val_loss: 20.5058\n",
      "Epoch 199/200\n",
      "3371/3371 [==============================] - 1s 160us/step - loss: 24.9758 - val_loss: 20.5039\n",
      "Epoch 200/200\n",
      "3371/3371 [==============================] - 1s 156us/step - loss: 24.9794 - val_loss: 20.5030\n",
      "---------current test year is 2013---------\n",
      "Train on 3363 samples, validate on 1122 samples\n",
      "Epoch 1/200\n",
      "3363/3363 [==============================] - 2s 448us/step - loss: 100.4298 - val_loss: 87.5162\n",
      "Epoch 2/200\n",
      "3363/3363 [==============================] - 0s 148us/step - loss: 81.9931 - val_loss: 75.8522\n",
      "Epoch 3/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 70.9270 - val_loss: 64.9015\n",
      "Epoch 4/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 60.7267 - val_loss: 54.8190\n",
      "Epoch 5/200\n",
      "3363/3363 [==============================] - 1s 164us/step - loss: 51.0906 - val_loss: 45.0290\n",
      "Epoch 6/200\n",
      "3363/3363 [==============================] - 1s 172us/step - loss: 41.8170 - val_loss: 36.0364\n",
      "Epoch 7/200\n",
      "3363/3363 [==============================] - 1s 194us/step - loss: 33.1910 - val_loss: 27.8282\n",
      "Epoch 8/200\n",
      "3363/3363 [==============================] - 1s 190us/step - loss: 26.9063 - val_loss: 23.4077\n",
      "Epoch 9/200\n",
      "3363/3363 [==============================] - 1s 189us/step - loss: 24.1400 - val_loss: 21.6607\n",
      "Epoch 10/200\n",
      "3363/3363 [==============================] - 0s 149us/step - loss: 23.2224 - val_loss: 20.9714\n",
      "Epoch 11/200\n",
      "3363/3363 [==============================] - 0s 143us/step - loss: 22.8746 - val_loss: 20.6740\n",
      "Epoch 12/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7751 - val_loss: 20.6213\n",
      "Epoch 13/200\n",
      "3363/3363 [==============================] - 1s 151us/step - loss: 22.7456 - val_loss: 20.5794\n",
      "Epoch 14/200\n",
      "3363/3363 [==============================] - 1s 153us/step - loss: 22.7369 - val_loss: 20.5728\n",
      "Epoch 15/200\n",
      "3363/3363 [==============================] - 1s 150us/step - loss: 22.7362 - val_loss: 20.5720\n",
      "Epoch 16/200\n",
      "3363/3363 [==============================] - 1s 150us/step - loss: 22.7361 - val_loss: 20.5681\n",
      "Epoch 17/200\n",
      "3363/3363 [==============================] - 0s 145us/step - loss: 22.7366 - val_loss: 20.5675\n",
      "Epoch 18/200\n",
      "3363/3363 [==============================] - 1s 159us/step - loss: 22.7348 - val_loss: 20.5655\n",
      "Epoch 19/200\n",
      "3363/3363 [==============================] - 1s 174us/step - loss: 22.7360 - val_loss: 20.5624\n",
      "Epoch 20/200\n",
      "3363/3363 [==============================] - 1s 200us/step - loss: 22.7360 - val_loss: 20.5638\n",
      "Epoch 21/200\n",
      "3363/3363 [==============================] - 1s 171us/step - loss: 22.7339 - val_loss: 20.5575\n",
      "Epoch 22/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7429 - val_loss: 20.5586\n",
      "Epoch 23/200\n",
      "3363/3363 [==============================] - 1s 153us/step - loss: 22.7357 - val_loss: 20.5566\n",
      "Epoch 24/200\n",
      "3363/3363 [==============================] - 1s 151us/step - loss: 22.7382 - val_loss: 20.5581\n",
      "Epoch 25/200\n",
      "3363/3363 [==============================] - 0s 148us/step - loss: 22.7363 - val_loss: 20.5613\n",
      "Epoch 26/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7362 - val_loss: 20.5583\n",
      "Epoch 27/200\n",
      "3363/3363 [==============================] - 1s 153us/step - loss: 22.7367 - val_loss: 20.5584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "3363/3363 [==============================] - 1s 165us/step - loss: 22.7388 - val_loss: 20.5596\n",
      "Epoch 29/200\n",
      "3363/3363 [==============================] - 1s 176us/step - loss: 22.7350 - val_loss: 20.5576\n",
      "Epoch 30/200\n",
      "3363/3363 [==============================] - 1s 156us/step - loss: 22.7346 - val_loss: 20.5553\n",
      "Epoch 31/200\n",
      "3363/3363 [==============================] - 1s 154us/step - loss: 22.7359 - val_loss: 20.5594\n",
      "Epoch 32/200\n",
      "3363/3363 [==============================] - 1s 150us/step - loss: 22.7366 - val_loss: 20.5618\n",
      "Epoch 33/200\n",
      "3363/3363 [==============================] - 1s 151us/step - loss: 22.7353 - val_loss: 20.5585\n",
      "Epoch 34/200\n",
      "3363/3363 [==============================] - 1s 156us/step - loss: 22.7362 - val_loss: 20.5599\n",
      "Epoch 35/200\n",
      "3363/3363 [==============================] - 1s 198us/step - loss: 22.7370 - val_loss: 20.5587\n",
      "Epoch 36/200\n",
      "3363/3363 [==============================] - 1s 248us/step - loss: 22.7382 - val_loss: 20.5516\n",
      "Epoch 37/200\n",
      "3363/3363 [==============================] - 1s 200us/step - loss: 22.7400 - val_loss: 20.5587\n",
      "Epoch 38/200\n",
      "3363/3363 [==============================] - 1s 169us/step - loss: 22.7367 - val_loss: 20.5552\n",
      "Epoch 39/200\n",
      "3363/3363 [==============================] - 1s 196us/step - loss: 22.7371 - val_loss: 20.5513\n",
      "Epoch 40/200\n",
      "3363/3363 [==============================] - 1s 188us/step - loss: 22.7370 - val_loss: 20.5522\n",
      "Epoch 41/200\n",
      "3363/3363 [==============================] - 1s 191us/step - loss: 22.7361 - val_loss: 20.5547\n",
      "Epoch 42/200\n",
      "3363/3363 [==============================] - 1s 164us/step - loss: 22.7365 - val_loss: 20.5572\n",
      "Epoch 43/200\n",
      "3363/3363 [==============================] - 0s 140us/step - loss: 22.7356 - val_loss: 20.5516\n",
      "Epoch 44/200\n",
      "3363/3363 [==============================] - 0s 144us/step - loss: 22.7363 - val_loss: 20.5587\n",
      "Epoch 45/200\n",
      "3363/3363 [==============================] - 1s 163us/step - loss: 22.7375 - val_loss: 20.5592\n",
      "Epoch 46/200\n",
      "3363/3363 [==============================] - 1s 159us/step - loss: 22.7372 - val_loss: 20.5501\n",
      "Epoch 47/200\n",
      "3363/3363 [==============================] - 0s 140us/step - loss: 22.7335 - val_loss: 20.5652\n",
      "Epoch 48/200\n",
      "3363/3363 [==============================] - 0s 149us/step - loss: 22.7384 - val_loss: 20.5664\n",
      "Epoch 49/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7370 - val_loss: 20.5608\n",
      "Epoch 50/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7363 - val_loss: 20.5621\n",
      "Epoch 51/200\n",
      "3363/3363 [==============================] - 1s 154us/step - loss: 22.7404 - val_loss: 20.5576\n",
      "Epoch 52/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7378 - val_loss: 20.5546\n",
      "Epoch 53/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7357 - val_loss: 20.5551\n",
      "Epoch 54/200\n",
      "3363/3363 [==============================] - 1s 159us/step - loss: 22.7359 - val_loss: 20.5525\n",
      "Epoch 55/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7364 - val_loss: 20.5547\n",
      "Epoch 56/200\n",
      "3363/3363 [==============================] - 0s 145us/step - loss: 22.7359 - val_loss: 20.5539\n",
      "Epoch 57/200\n",
      "3363/3363 [==============================] - 1s 172us/step - loss: 22.7408 - val_loss: 20.5534\n",
      "Epoch 58/200\n",
      "3363/3363 [==============================] - 1s 181us/step - loss: 22.7344 - val_loss: 20.5550\n",
      "Epoch 59/200\n",
      "3363/3363 [==============================] - 1s 178us/step - loss: 22.7395 - val_loss: 20.5573\n",
      "Epoch 60/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7356 - val_loss: 20.5527\n",
      "Epoch 61/200\n",
      "3363/3363 [==============================] - 0s 146us/step - loss: 22.7344 - val_loss: 20.5559\n",
      "Epoch 62/200\n",
      "3363/3363 [==============================] - 0s 140us/step - loss: 22.7368 - val_loss: 20.5588\n",
      "Epoch 63/200\n",
      "3363/3363 [==============================] - 0s 144us/step - loss: 22.7366 - val_loss: 20.5614\n",
      "Epoch 64/200\n",
      "3363/3363 [==============================] - 1s 154us/step - loss: 22.7360 - val_loss: 20.5582\n",
      "Epoch 65/200\n",
      "3363/3363 [==============================] - 1s 168us/step - loss: 22.7406 - val_loss: 20.5596\n",
      "Epoch 66/200\n",
      "3363/3363 [==============================] - 1s 151us/step - loss: 22.7370 - val_loss: 20.5584\n",
      "Epoch 67/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7353 - val_loss: 20.5669\n",
      "Epoch 68/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7357 - val_loss: 20.5649\n",
      "Epoch 69/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7362 - val_loss: 20.5586\n",
      "Epoch 70/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7362 - val_loss: 20.5637\n",
      "Epoch 71/200\n",
      "3363/3363 [==============================] - 0s 142us/step - loss: 22.7353 - val_loss: 20.5564\n",
      "Epoch 72/200\n",
      "3363/3363 [==============================] - 0s 144us/step - loss: 22.7362 - val_loss: 20.5652\n",
      "Epoch 73/200\n",
      "3363/3363 [==============================] - 1s 176us/step - loss: 22.7360 - val_loss: 20.5625\n",
      "Epoch 74/200\n",
      "3363/3363 [==============================] - 1s 184us/step - loss: 22.7369 - val_loss: 20.5643\n",
      "Epoch 75/200\n",
      "3363/3363 [==============================] - 1s 183us/step - loss: 22.7341 - val_loss: 20.5620\n",
      "Epoch 76/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7366 - val_loss: 20.5648\n",
      "Epoch 77/200\n",
      "3363/3363 [==============================] - 0s 129us/step - loss: 22.7365 - val_loss: 20.5574\n",
      "Epoch 78/200\n",
      "3363/3363 [==============================] - 0s 143us/step - loss: 22.7365 - val_loss: 20.5568\n",
      "Epoch 79/200\n",
      "3363/3363 [==============================] - 1s 167us/step - loss: 22.7360 - val_loss: 20.5604\n",
      "Epoch 80/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7364 - val_loss: 20.5649\n",
      "Epoch 81/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7348 - val_loss: 20.5653\n",
      "Epoch 82/200\n",
      "3363/3363 [==============================] - 0s 145us/step - loss: 22.7357 - val_loss: 20.5647\n",
      "Epoch 83/200\n",
      "3363/3363 [==============================] - 0s 146us/step - loss: 22.7363 - val_loss: 20.5603\n",
      "Epoch 84/200\n",
      "3363/3363 [==============================] - 1s 157us/step - loss: 22.7345 - val_loss: 20.5620\n",
      "Epoch 85/200\n",
      "3363/3363 [==============================] - 1s 184us/step - loss: 22.7344 - val_loss: 20.5587\n",
      "Epoch 86/200\n",
      "3363/3363 [==============================] - 1s 179us/step - loss: 22.7361 - val_loss: 20.5579\n",
      "Epoch 87/200\n",
      "3363/3363 [==============================] - 0s 137us/step - loss: 22.7356 - val_loss: 20.5516\n",
      "Epoch 88/200\n",
      "3363/3363 [==============================] - 0s 140us/step - loss: 22.7353 - val_loss: 20.5595\n",
      "Epoch 89/200\n",
      "3363/3363 [==============================] - 1s 176us/step - loss: 22.7362 - val_loss: 20.5613\n",
      "Epoch 90/200\n",
      "3363/3363 [==============================] - 1s 190us/step - loss: 22.7347 - val_loss: 20.5597\n",
      "Epoch 91/200\n",
      "3363/3363 [==============================] - 1s 193us/step - loss: 22.7364 - val_loss: 20.5589\n",
      "Epoch 92/200\n",
      "3363/3363 [==============================] - 1s 193us/step - loss: 22.7350 - val_loss: 20.5593\n",
      "Epoch 93/200\n",
      "3363/3363 [==============================] - 1s 194us/step - loss: 22.7362 - val_loss: 20.5614\n",
      "Epoch 94/200\n",
      "3363/3363 [==============================] - 1s 164us/step - loss: 22.7366 - val_loss: 20.5586\n",
      "Epoch 95/200\n",
      "3363/3363 [==============================] - 1s 163us/step - loss: 22.7358 - val_loss: 20.5597\n",
      "Epoch 96/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7380 - val_loss: 20.5564\n",
      "Epoch 97/200\n",
      "3363/3363 [==============================] - 0s 139us/step - loss: 22.7368 - val_loss: 20.5614\n",
      "Epoch 98/200\n",
      "3363/3363 [==============================] - 0s 138us/step - loss: 22.7389 - val_loss: 20.5509\n",
      "Epoch 99/200\n",
      "3363/3363 [==============================] - 0s 126us/step - loss: 22.7353 - val_loss: 20.5645\n",
      "Epoch 100/200\n",
      "3363/3363 [==============================] - 0s 133us/step - loss: 22.7369 - val_loss: 20.5613\n",
      "Epoch 101/200\n",
      "3363/3363 [==============================] - 0s 133us/step - loss: 22.7343 - val_loss: 20.5591\n",
      "Epoch 102/200\n",
      "3363/3363 [==============================] - 1s 151us/step - loss: 22.7356 - val_loss: 20.5630\n",
      "Epoch 103/200\n",
      "3363/3363 [==============================] - 1s 168us/step - loss: 22.7352 - val_loss: 20.5602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200\n",
      "3363/3363 [==============================] - 1s 178us/step - loss: 22.7353 - val_loss: 20.5584\n",
      "Epoch 105/200\n",
      "3363/3363 [==============================] - 0s 146us/step - loss: 22.7361 - val_loss: 20.5609\n",
      "Epoch 106/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7347 - val_loss: 20.5576\n",
      "Epoch 107/200\n",
      "3363/3363 [==============================] - 1s 154us/step - loss: 22.7355 - val_loss: 20.5539\n",
      "Epoch 108/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7369 - val_loss: 20.5585\n",
      "Epoch 109/200\n",
      "3363/3363 [==============================] - 1s 151us/step - loss: 22.7369 - val_loss: 20.5557\n",
      "Epoch 110/200\n",
      "3363/3363 [==============================] - 1s 166us/step - loss: 22.7365 - val_loss: 20.5593\n",
      "Epoch 111/200\n",
      "3363/3363 [==============================] - 1s 178us/step - loss: 22.7382 - val_loss: 20.5633\n",
      "Epoch 112/200\n",
      "3363/3363 [==============================] - 1s 179us/step - loss: 22.7335 - val_loss: 20.5542\n",
      "Epoch 113/200\n",
      "3363/3363 [==============================] - 0s 146us/step - loss: 22.7378 - val_loss: 20.5581\n",
      "Epoch 114/200\n",
      "3363/3363 [==============================] - 0s 134us/step - loss: 22.7335 - val_loss: 20.5630\n",
      "Epoch 115/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7382 - val_loss: 20.5613\n",
      "Epoch 116/200\n",
      "3363/3363 [==============================] - 1s 167us/step - loss: 22.7366 - val_loss: 20.5611\n",
      "Epoch 117/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7348 - val_loss: 20.5554\n",
      "Epoch 118/200\n",
      "3363/3363 [==============================] - 1s 149us/step - loss: 22.7353 - val_loss: 20.5556\n",
      "Epoch 119/200\n",
      "3363/3363 [==============================] - 1s 159us/step - loss: 22.7345 - val_loss: 20.5593\n",
      "Epoch 120/200\n",
      "3363/3363 [==============================] - 1s 149us/step - loss: 22.7344 - val_loss: 20.5569\n",
      "Epoch 121/200\n",
      "3363/3363 [==============================] - 1s 180us/step - loss: 22.7373 - val_loss: 20.5563\n",
      "Epoch 122/200\n",
      "3363/3363 [==============================] - 1s 163us/step - loss: 22.7372 - val_loss: 20.5558\n",
      "Epoch 123/200\n",
      "3363/3363 [==============================] - 0s 149us/step - loss: 22.7347 - val_loss: 20.5531\n",
      "Epoch 124/200\n",
      "3363/3363 [==============================] - 0s 138us/step - loss: 22.7407 - val_loss: 20.5522\n",
      "Epoch 125/200\n",
      "3363/3363 [==============================] - 1s 161us/step - loss: 22.7349 - val_loss: 20.5540\n",
      "Epoch 126/200\n",
      "3363/3363 [==============================] - 1s 181us/step - loss: 22.7372 - val_loss: 20.5545\n",
      "Epoch 127/200\n",
      "3363/3363 [==============================] - 1s 154us/step - loss: 22.7375 - val_loss: 20.5550\n",
      "Epoch 128/200\n",
      "3363/3363 [==============================] - 0s 140us/step - loss: 22.7374 - val_loss: 20.5573\n",
      "Epoch 129/200\n",
      "3363/3363 [==============================] - 1s 167us/step - loss: 22.7350 - val_loss: 20.5629\n",
      "Epoch 130/200\n",
      "3363/3363 [==============================] - 1s 161us/step - loss: 22.7347 - val_loss: 20.5590\n",
      "Epoch 131/200\n",
      "3363/3363 [==============================] - 1s 167us/step - loss: 22.7346 - val_loss: 20.5592\n",
      "Epoch 132/200\n",
      "3363/3363 [==============================] - 1s 167us/step - loss: 22.7369 - val_loss: 20.5552\n",
      "Epoch 133/200\n",
      "3363/3363 [==============================] - 0s 148us/step - loss: 22.7338 - val_loss: 20.5607\n",
      "Epoch 134/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7383 - val_loss: 20.5563\n",
      "Epoch 135/200\n",
      "3363/3363 [==============================] - 1s 157us/step - loss: 22.7360 - val_loss: 20.5594\n",
      "Epoch 136/200\n",
      "3363/3363 [==============================] - 1s 160us/step - loss: 22.7351 - val_loss: 20.5581\n",
      "Epoch 137/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7344 - val_loss: 20.5584\n",
      "Epoch 138/200\n",
      "3363/3363 [==============================] - 1s 149us/step - loss: 22.7361 - val_loss: 20.5590\n",
      "Epoch 139/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7341 - val_loss: 20.5571\n",
      "Epoch 140/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7361 - val_loss: 20.5564\n",
      "Epoch 141/200\n",
      "3363/3363 [==============================] - 1s 157us/step - loss: 22.7345 - val_loss: 20.5582\n",
      "Epoch 142/200\n",
      "3363/3363 [==============================] - 1s 160us/step - loss: 22.7351 - val_loss: 20.5589\n",
      "Epoch 143/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7362 - val_loss: 20.5556\n",
      "Epoch 144/200\n",
      "3363/3363 [==============================] - 1s 157us/step - loss: 22.7354 - val_loss: 20.5514\n",
      "Epoch 145/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7368 - val_loss: 20.5616\n",
      "Epoch 146/200\n",
      "3363/3363 [==============================] - 1s 149us/step - loss: 22.7344 - val_loss: 20.5622\n",
      "Epoch 147/200\n",
      "3363/3363 [==============================] - 0s 139us/step - loss: 22.7341 - val_loss: 20.5558\n",
      "Epoch 148/200\n",
      "3363/3363 [==============================] - 1s 153us/step - loss: 22.7358 - val_loss: 20.5557\n",
      "Epoch 149/200\n",
      "3363/3363 [==============================] - 1s 153us/step - loss: 22.7358 - val_loss: 20.5596\n",
      "Epoch 150/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7353 - val_loss: 20.5599\n",
      "Epoch 151/200\n",
      "3363/3363 [==============================] - 1s 152us/step - loss: 22.7357 - val_loss: 20.5564\n",
      "Epoch 152/200\n",
      "3363/3363 [==============================] - 1s 176us/step - loss: 22.7345 - val_loss: 20.5567\n",
      "Epoch 153/200\n",
      "3363/3363 [==============================] - 1s 178us/step - loss: 22.7334 - val_loss: 20.5569\n",
      "Epoch 154/200\n",
      "3363/3363 [==============================] - 1s 174us/step - loss: 22.7349 - val_loss: 20.5559\n",
      "Epoch 155/200\n",
      "3363/3363 [==============================] - 1s 176us/step - loss: 22.7352 - val_loss: 20.5566\n",
      "Epoch 156/200\n",
      "3363/3363 [==============================] - 0s 129us/step - loss: 22.7352 - val_loss: 20.5567\n",
      "Epoch 157/200\n",
      "3363/3363 [==============================] - 0s 139us/step - loss: 22.7354 - val_loss: 20.5532\n",
      "Epoch 158/200\n",
      "3363/3363 [==============================] - 0s 140us/step - loss: 22.7347 - val_loss: 20.5548\n",
      "Epoch 159/200\n",
      "3363/3363 [==============================] - 0s 145us/step - loss: 22.7373 - val_loss: 20.5566\n",
      "Epoch 160/200\n",
      "3363/3363 [==============================] - 1s 162us/step - loss: 22.7368 - val_loss: 20.5550\n",
      "Epoch 161/200\n",
      "3363/3363 [==============================] - 1s 177us/step - loss: 22.7360 - val_loss: 20.5598\n",
      "Epoch 162/200\n",
      "3363/3363 [==============================] - 0s 135us/step - loss: 22.7351 - val_loss: 20.5578\n",
      "Epoch 163/200\n",
      "3363/3363 [==============================] - 1s 151us/step - loss: 22.7348 - val_loss: 20.5557\n",
      "Epoch 164/200\n",
      "3363/3363 [==============================] - 1s 170us/step - loss: 22.7336 - val_loss: 20.5595\n",
      "Epoch 165/200\n",
      "3363/3363 [==============================] - 1s 184us/step - loss: 22.7357 - val_loss: 20.5588\n",
      "Epoch 166/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7344 - val_loss: 20.5598\n",
      "Epoch 167/200\n",
      "3363/3363 [==============================] - 0s 146us/step - loss: 22.7364 - val_loss: 20.5578\n",
      "Epoch 168/200\n",
      "3363/3363 [==============================] - 0s 149us/step - loss: 22.7349 - val_loss: 20.5566\n",
      "Epoch 169/200\n",
      "3363/3363 [==============================] - 1s 158us/step - loss: 22.7347 - val_loss: 20.5558\n",
      "Epoch 170/200\n",
      "3363/3363 [==============================] - 1s 178us/step - loss: 22.7346 - val_loss: 20.5552\n",
      "Epoch 171/200\n",
      "3363/3363 [==============================] - 1s 177us/step - loss: 22.7368 - val_loss: 20.5542\n",
      "Epoch 172/200\n",
      "3363/3363 [==============================] - 1s 159us/step - loss: 22.7349 - val_loss: 20.5628\n",
      "Epoch 173/200\n",
      "3363/3363 [==============================] - 0s 134us/step - loss: 22.7370 - val_loss: 20.5599\n",
      "Epoch 174/200\n",
      "3363/3363 [==============================] - 1s 149us/step - loss: 22.7351 - val_loss: 20.5581\n",
      "Epoch 175/200\n",
      "3363/3363 [==============================] - 1s 163us/step - loss: 22.7346 - val_loss: 20.5556\n",
      "Epoch 176/200\n",
      "3363/3363 [==============================] - 1s 166us/step - loss: 22.7373 - val_loss: 20.5567\n",
      "Epoch 177/200\n",
      "3363/3363 [==============================] - 1s 164us/step - loss: 22.7345 - val_loss: 20.5580\n",
      "Epoch 178/200\n",
      "3363/3363 [==============================] - 1s 163us/step - loss: 22.7361 - val_loss: 20.5516\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3363/3363 [==============================] - 0s 137us/step - loss: 22.7383 - val_loss: 20.5604\n",
      "Epoch 180/200\n",
      "3363/3363 [==============================] - ETA: 0s - loss: 22.79 - 0s 142us/step - loss: 22.7344 - val_loss: 20.5607\n",
      "Epoch 181/200\n",
      "3363/3363 [==============================] - 1s 167us/step - loss: 22.7345 - val_loss: 20.5580\n",
      "Epoch 182/200\n",
      "3363/3363 [==============================] - 1s 169us/step - loss: 22.7332 - val_loss: 20.5642\n",
      "Epoch 183/200\n",
      "3363/3363 [==============================] - 1s 174us/step - loss: 22.7354 - val_loss: 20.5620\n",
      "Epoch 184/200\n",
      "3363/3363 [==============================] - 1s 150us/step - loss: 22.7346 - val_loss: 20.5615\n",
      "Epoch 185/200\n",
      "3363/3363 [==============================] - 0s 128us/step - loss: 22.7343 - val_loss: 20.5566\n",
      "Epoch 186/200\n",
      "3363/3363 [==============================] - 0s 140us/step - loss: 22.7348 - val_loss: 20.5561\n",
      "Epoch 187/200\n",
      "3363/3363 [==============================] - 1s 178us/step - loss: 22.7353 - val_loss: 20.5525\n",
      "Epoch 188/200\n",
      "3363/3363 [==============================] - 1s 168us/step - loss: 22.7351 - val_loss: 20.5576\n",
      "Epoch 189/200\n",
      "3363/3363 [==============================] - 1s 163us/step - loss: 22.7340 - val_loss: 20.5596\n",
      "Epoch 190/200\n",
      "3363/3363 [==============================] - 0s 137us/step - loss: 22.7341 - val_loss: 20.5568\n",
      "Epoch 191/200\n",
      "3363/3363 [==============================] - 0s 143us/step - loss: 22.7352 - val_loss: 20.5580\n",
      "Epoch 192/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7367 - val_loss: 20.5593\n",
      "Epoch 193/200\n",
      "3363/3363 [==============================] - 1s 169us/step - loss: 22.7326 - val_loss: 20.5564\n",
      "Epoch 194/200\n",
      "3363/3363 [==============================] - 1s 170us/step - loss: 22.7348 - val_loss: 20.5604\n",
      "Epoch 195/200\n",
      "3363/3363 [==============================] - 1s 162us/step - loss: 22.7338 - val_loss: 20.5594\n",
      "Epoch 196/200\n",
      "3363/3363 [==============================] - 1s 149us/step - loss: 22.7345 - val_loss: 20.5624\n",
      "Epoch 197/200\n",
      "3363/3363 [==============================] - 0s 138us/step - loss: 22.7344 - val_loss: 20.5563\n",
      "Epoch 198/200\n",
      "3363/3363 [==============================] - 1s 155us/step - loss: 22.7360 - val_loss: 20.5605\n",
      "Epoch 199/200\n",
      "3363/3363 [==============================] - 1s 160us/step - loss: 22.7358 - val_loss: 20.5557\n",
      "Epoch 200/200\n",
      "3363/3363 [==============================] - 1s 159us/step - loss: 22.7354 - val_loss: 20.5533\n",
      "---------current test year is 2015---------\n",
      "Train on 3350 samples, validate on 1117 samples\n",
      "Epoch 1/200\n",
      "3350/3350 [==============================] - 1s 433us/step - loss: 93.8977 - val_loss: 87.8285\n",
      "Epoch 2/200\n",
      "3350/3350 [==============================] - 1s 164us/step - loss: 81.6357 - val_loss: 76.0138\n",
      "Epoch 3/200\n",
      "3350/3350 [==============================] - 1s 190us/step - loss: 69.6113 - val_loss: 65.2601\n",
      "Epoch 4/200\n",
      "3350/3350 [==============================] - 1s 160us/step - loss: 58.3899 - val_loss: 55.2014\n",
      "Epoch 5/200\n",
      "3350/3350 [==============================] - 1s 169us/step - loss: 47.6457 - val_loss: 45.3856\n",
      "Epoch 6/200\n",
      "3350/3350 [==============================] - 1s 158us/step - loss: 37.3685 - val_loss: 36.1032\n",
      "Epoch 7/200\n",
      "3350/3350 [==============================] - 0s 147us/step - loss: 28.1290 - val_loss: 28.8408\n",
      "Epoch 8/200\n",
      "3350/3350 [==============================] - 1s 173us/step - loss: 22.3272 - val_loss: 25.6821\n",
      "Epoch 9/200\n",
      "3350/3350 [==============================] - 1s 164us/step - loss: 19.9842 - val_loss: 24.5747\n",
      "Epoch 10/200\n",
      "3350/3350 [==============================] - 1s 157us/step - loss: 19.1386 - val_loss: 24.2503\n",
      "Epoch 11/200\n",
      "3350/3350 [==============================] - 1s 150us/step - loss: 18.8523 - val_loss: 24.1813\n",
      "Epoch 12/200\n",
      "3350/3350 [==============================] - 1s 170us/step - loss: 18.7807 - val_loss: 24.1941\n",
      "Epoch 13/200\n",
      "3350/3350 [==============================] - 1s 155us/step - loss: 18.7598 - val_loss: 24.2013\n",
      "Epoch 14/200\n",
      "3350/3350 [==============================] - 0s 149us/step - loss: 18.7568 - val_loss: 24.2129\n",
      "Epoch 15/200\n",
      "3350/3350 [==============================] - 1s 169us/step - loss: 18.7548 - val_loss: 24.2155\n",
      "Epoch 16/200\n",
      "3350/3350 [==============================] - 1s 155us/step - loss: 18.7537 - val_loss: 24.2288\n",
      "Epoch 17/200\n",
      "3350/3350 [==============================] - 1s 185us/step - loss: 18.7549 - val_loss: 24.2265\n",
      "Epoch 18/200\n",
      "3350/3350 [==============================] - 1s 199us/step - loss: 18.7539 - val_loss: 24.2342\n",
      "Epoch 19/200\n",
      "3350/3350 [==============================] - 1s 198us/step - loss: 18.7519 - val_loss: 24.2323\n",
      "Epoch 20/200\n",
      "3350/3350 [==============================] - 0s 145us/step - loss: 18.7515 - val_loss: 24.2254\n",
      "Epoch 21/200\n",
      "3350/3350 [==============================] - 0s 135us/step - loss: 18.7532 - val_loss: 24.2312\n",
      "Epoch 22/200\n",
      "3350/3350 [==============================] - 1s 154us/step - loss: 18.7536 - val_loss: 24.2271\n",
      "Epoch 23/200\n",
      "3350/3350 [==============================] - 0s 146us/step - loss: 18.7537 - val_loss: 24.2265\n",
      "Epoch 24/200\n",
      "3350/3350 [==============================] - 1s 174us/step - loss: 18.7541 - val_loss: 24.2329\n",
      "Epoch 25/200\n",
      "3350/3350 [==============================] - 1s 170us/step - loss: 18.7547 - val_loss: 24.2319\n",
      "Epoch 26/200\n",
      "3350/3350 [==============================] - 1s 152us/step - loss: 18.7545 - val_loss: 24.2356\n",
      "Epoch 27/200\n",
      "3350/3350 [==============================] - 1s 157us/step - loss: 18.7513 - val_loss: 24.2299\n",
      "Epoch 28/200\n",
      "3350/3350 [==============================] - 1s 155us/step - loss: 18.7550 - val_loss: 24.2307\n",
      "Epoch 29/200\n",
      "3350/3350 [==============================] - 1s 166us/step - loss: 18.7522 - val_loss: 24.2226\n",
      "Epoch 30/200\n",
      "3350/3350 [==============================] - 1s 180us/step - loss: 18.7554 - val_loss: 24.2252\n",
      "Epoch 31/200\n",
      "3350/3350 [==============================] - 1s 168us/step - loss: 18.7541 - val_loss: 24.2319\n",
      "Epoch 32/200\n",
      "3350/3350 [==============================] - 1s 162us/step - loss: 18.7547 - val_loss: 24.2298\n",
      "Epoch 33/200\n",
      "3350/3350 [==============================] - 1s 163us/step - loss: 18.7548 - val_loss: 24.2306\n",
      "Epoch 34/200\n",
      "3350/3350 [==============================] - 1s 164us/step - loss: 18.7546 - val_loss: 24.2355\n",
      "Epoch 35/200\n",
      "3350/3350 [==============================] - 1s 163us/step - loss: 18.7442 - val_loss: 24.2138\n",
      "Epoch 36/200\n",
      "3350/3350 [==============================] - 1s 164us/step - loss: 18.7544 - val_loss: 24.2229\n",
      "Epoch 37/200\n",
      "3350/3350 [==============================] - 0s 145us/step - loss: 18.7555 - val_loss: 24.2265\n",
      "Epoch 38/200\n",
      "3350/3350 [==============================] - 1s 152us/step - loss: 18.7554 - val_loss: 24.2317\n",
      "Epoch 39/200\n",
      "3350/3350 [==============================] - 1s 156us/step - loss: 18.7549 - val_loss: 24.2323\n",
      "Epoch 40/200\n",
      "3350/3350 [==============================] - 1s 177us/step - loss: 18.7544 - val_loss: 24.2302\n",
      "Epoch 41/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7544 - val_loss: 24.2367\n",
      "Epoch 42/200\n",
      "3350/3350 [==============================] - 1s 192us/step - loss: 18.7553 - val_loss: 24.2247\n",
      "Epoch 43/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7529 - val_loss: 24.2317\n",
      "Epoch 44/200\n",
      "3350/3350 [==============================] - 1s 192us/step - loss: 18.7533 - val_loss: 24.2288\n",
      "Epoch 45/200\n",
      "3350/3350 [==============================] - 1s 190us/step - loss: 18.7555 - val_loss: 24.2307\n",
      "Epoch 46/200\n",
      "3350/3350 [==============================] - 1s 170us/step - loss: 18.7536 - val_loss: 24.2397\n",
      "Epoch 47/200\n",
      "3350/3350 [==============================] - 0s 136us/step - loss: 18.7548 - val_loss: 24.2406\n",
      "Epoch 48/200\n",
      "3350/3350 [==============================] - 0s 138us/step - loss: 18.7549 - val_loss: 24.2343\n",
      "Epoch 49/200\n",
      "3350/3350 [==============================] - 0s 139us/step - loss: 18.7532 - val_loss: 24.2319\n",
      "Epoch 50/200\n",
      "3350/3350 [==============================] - 0s 134us/step - loss: 18.7533 - val_loss: 24.2311\n",
      "Epoch 51/200\n",
      "3350/3350 [==============================] - 0s 141us/step - loss: 18.7561 - val_loss: 24.2281\n",
      "Epoch 52/200\n",
      "3350/3350 [==============================] - 1s 161us/step - loss: 18.7473 - val_loss: 24.2184\n",
      "Epoch 53/200\n",
      "3350/3350 [==============================] - 1s 196us/step - loss: 18.7530 - val_loss: 24.2347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200\n",
      "3350/3350 [==============================] - 1s 177us/step - loss: 18.7532 - val_loss: 24.2307\n",
      "Epoch 55/200\n",
      "3350/3350 [==============================] - 1s 180us/step - loss: 18.7531 - val_loss: 24.2328\n",
      "Epoch 56/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7533 - val_loss: 24.2317\n",
      "Epoch 57/200\n",
      "3350/3350 [==============================] - 1s 180us/step - loss: 18.7539 - val_loss: 24.2281\n",
      "Epoch 58/200\n",
      "3350/3350 [==============================] - 1s 170us/step - loss: 18.7535 - val_loss: 24.2307\n",
      "Epoch 59/200\n",
      "3350/3350 [==============================] - 1s 178us/step - loss: 18.7539 - val_loss: 24.2304\n",
      "Epoch 60/200\n",
      "3350/3350 [==============================] - 1s 167us/step - loss: 18.7536 - val_loss: 24.2338\n",
      "Epoch 61/200\n",
      "3350/3350 [==============================] - 1s 182us/step - loss: 18.7533 - val_loss: 24.2266\n",
      "Epoch 62/200\n",
      "3350/3350 [==============================] - 1s 188us/step - loss: 18.7550 - val_loss: 24.2251\n",
      "Epoch 63/200\n",
      "3350/3350 [==============================] - 0s 139us/step - loss: 18.7568 - val_loss: 24.2287\n",
      "Epoch 64/200\n",
      "3350/3350 [==============================] - 1s 159us/step - loss: 18.7537 - val_loss: 24.2247\n",
      "Epoch 65/200\n",
      "3350/3350 [==============================] - 1s 174us/step - loss: 18.7540 - val_loss: 24.2297\n",
      "Epoch 66/200\n",
      "3350/3350 [==============================] - 1s 164us/step - loss: 18.7533 - val_loss: 24.2301\n",
      "Epoch 67/200\n",
      "3350/3350 [==============================] - 1s 164us/step - loss: 18.7533 - val_loss: 24.2300\n",
      "Epoch 68/200\n",
      "3350/3350 [==============================] - 1s 155us/step - loss: 18.7535 - val_loss: 24.2308\n",
      "Epoch 69/200\n",
      "3350/3350 [==============================] - 1s 162us/step - loss: 18.7541 - val_loss: 24.2347\n",
      "Epoch 70/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7556 - val_loss: 24.2359\n",
      "Epoch 71/200\n",
      "3350/3350 [==============================] - 1s 199us/step - loss: 18.7548 - val_loss: 24.2354\n",
      "Epoch 72/200\n",
      "3350/3350 [==============================] - 1s 197us/step - loss: 18.7550 - val_loss: 24.2351\n",
      "Epoch 73/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7532 - val_loss: 24.2275\n",
      "Epoch 74/200\n",
      "3350/3350 [==============================] - 0s 135us/step - loss: 18.7547 - val_loss: 24.2305\n",
      "Epoch 75/200\n",
      "3350/3350 [==============================] - 0s 137us/step - loss: 18.7538 - val_loss: 24.2306\n",
      "Epoch 76/200\n",
      "3350/3350 [==============================] - 0s 137us/step - loss: 18.7540 - val_loss: 24.2271\n",
      "Epoch 77/200\n",
      "3350/3350 [==============================] - 1s 150us/step - loss: 18.7532 - val_loss: 24.2221\n",
      "Epoch 78/200\n",
      "3350/3350 [==============================] - 1s 186us/step - loss: 18.7541 - val_loss: 24.2234\n",
      "Epoch 79/200\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.7553 - val_loss: 24.2278\n",
      "Epoch 80/200\n",
      "3350/3350 [==============================] - 0s 147us/step - loss: 18.7567 - val_loss: 24.2285\n",
      "Epoch 81/200\n",
      "3350/3350 [==============================] - 1s 160us/step - loss: 18.7539 - val_loss: 24.2331\n",
      "Epoch 82/200\n",
      "3350/3350 [==============================] - 1s 175us/step - loss: 18.7533 - val_loss: 24.2344\n",
      "Epoch 83/200\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.7534 - val_loss: 24.2357\n",
      "Epoch 84/200\n",
      "3350/3350 [==============================] - 0s 146us/step - loss: 18.7547 - val_loss: 24.2345\n",
      "Epoch 85/200\n",
      "3350/3350 [==============================] - 0s 139us/step - loss: 18.7548 - val_loss: 24.2297\n",
      "Epoch 86/200\n",
      "3350/3350 [==============================] - 1s 152us/step - loss: 18.7529 - val_loss: 24.2336\n",
      "Epoch 87/200\n",
      "3350/3350 [==============================] - 1s 166us/step - loss: 18.7556 - val_loss: 24.2300\n",
      "Epoch 88/200\n",
      "3350/3350 [==============================] - 1s 177us/step - loss: 18.7523 - val_loss: 24.2355\n",
      "Epoch 89/200\n",
      "3350/3350 [==============================] - 1s 190us/step - loss: 18.7566 - val_loss: 24.2359\n",
      "Epoch 90/200\n",
      "3350/3350 [==============================] - 1s 168us/step - loss: 18.7482 - val_loss: 24.2241\n",
      "Epoch 91/200\n",
      "3350/3350 [==============================] - 1s 175us/step - loss: 18.7539 - val_loss: 24.2339\n",
      "Epoch 92/200\n",
      "3350/3350 [==============================] - 1s 181us/step - loss: 18.7534 - val_loss: 24.2303\n",
      "Epoch 93/200\n",
      "3350/3350 [==============================] - 1s 180us/step - loss: 18.7532 - val_loss: 24.2372\n",
      "Epoch 94/200\n",
      "3350/3350 [==============================] - 1s 171us/step - loss: 18.7551 - val_loss: 24.2399\n",
      "Epoch 95/200\n",
      "3350/3350 [==============================] - 1s 163us/step - loss: 18.7533 - val_loss: 24.2352\n",
      "Epoch 96/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7540 - val_loss: 24.2374\n",
      "Epoch 97/200\n",
      "3350/3350 [==============================] - 1s 174us/step - loss: 18.7513 - val_loss: 24.2197\n",
      "Epoch 98/200\n",
      "3350/3350 [==============================] - 1s 225us/step - loss: 18.7546 - val_loss: 24.2288\n",
      "Epoch 99/200\n",
      "3350/3350 [==============================] - 1s 207us/step - loss: 18.7544 - val_loss: 24.2302\n",
      "Epoch 100/200\n",
      "3350/3350 [==============================] - 1s 188us/step - loss: 18.7551 - val_loss: 24.2288\n",
      "Epoch 101/200\n",
      "3350/3350 [==============================] - 1s 171us/step - loss: 18.7525 - val_loss: 24.2344\n",
      "Epoch 102/200\n",
      "3350/3350 [==============================] - 1s 196us/step - loss: 18.7539 - val_loss: 24.2339\n",
      "Epoch 103/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7546 - val_loss: 24.2299\n",
      "Epoch 104/200\n",
      "3350/3350 [==============================] - 1s 255us/step - loss: 18.7540 - val_loss: 24.2359\n",
      "Epoch 105/200\n",
      "3350/3350 [==============================] - 1s 254us/step - loss: 18.7481 - val_loss: 24.2149\n",
      "Epoch 106/200\n",
      "3350/3350 [==============================] - 1s 187us/step - loss: 18.7557 - val_loss: 24.2273\n",
      "Epoch 107/200\n",
      "3350/3350 [==============================] - 1s 163us/step - loss: 18.7539 - val_loss: 24.2323\n",
      "Epoch 108/200\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.7444 - val_loss: 24.2164\n",
      "Epoch 109/200\n",
      "3350/3350 [==============================] - 1s 167us/step - loss: 18.7556 - val_loss: 24.2204\n",
      "Epoch 110/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7546 - val_loss: 24.2269\n",
      "Epoch 111/200\n",
      "3350/3350 [==============================] - 1s 172us/step - loss: 18.7532 - val_loss: 24.2330\n",
      "Epoch 112/200\n",
      "3350/3350 [==============================] - 1s 170us/step - loss: 18.7525 - val_loss: 24.2395\n",
      "Epoch 113/200\n",
      "3350/3350 [==============================] - 1s 152us/step - loss: 18.7557 - val_loss: 24.2440\n",
      "Epoch 114/200\n",
      "3350/3350 [==============================] - 1s 159us/step - loss: 18.7535 - val_loss: 24.2395\n",
      "Epoch 115/200\n",
      "3350/3350 [==============================] - 1s 175us/step - loss: 18.7540 - val_loss: 24.2322\n",
      "Epoch 116/200\n",
      "3350/3350 [==============================] - 1s 167us/step - loss: 18.7540 - val_loss: 24.2381\n",
      "Epoch 117/200\n",
      "3350/3350 [==============================] - 1s 181us/step - loss: 18.7521 - val_loss: 24.2311\n",
      "Epoch 118/200\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.7527 - val_loss: 24.2294\n",
      "Epoch 119/200\n",
      "3350/3350 [==============================] - 1s 209us/step - loss: 18.7564 - val_loss: 24.2233\n",
      "Epoch 120/200\n",
      "3350/3350 [==============================] - 1s 200us/step - loss: 18.7530 - val_loss: 24.2257\n",
      "Epoch 121/200\n",
      "3350/3350 [==============================] - 1s 193us/step - loss: 18.7529 - val_loss: 24.2282\n",
      "Epoch 122/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7529 - val_loss: 24.2313\n",
      "Epoch 123/200\n",
      "3350/3350 [==============================] - 1s 190us/step - loss: 18.7536 - val_loss: 24.2335\n",
      "Epoch 124/200\n",
      "3350/3350 [==============================] - 1s 193us/step - loss: 18.7539 - val_loss: 24.2349\n",
      "Epoch 125/200\n",
      "3350/3350 [==============================] - 1s 199us/step - loss: 18.7544 - val_loss: 24.2340\n",
      "Epoch 126/200\n",
      "3350/3350 [==============================] - 1s 202us/step - loss: 18.7496 - val_loss: 24.2199\n",
      "Epoch 127/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7537 - val_loss: 24.2241\n",
      "Epoch 128/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7530 - val_loss: 24.2389\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7549 - val_loss: 24.2315\n",
      "Epoch 130/200\n",
      "3350/3350 [==============================] - 1s 192us/step - loss: 18.7527 - val_loss: 24.2362\n",
      "Epoch 131/200\n",
      "3350/3350 [==============================] - 1s 187us/step - loss: 18.7539 - val_loss: 24.2400\n",
      "Epoch 132/200\n",
      "3350/3350 [==============================] - 1s 189us/step - loss: 18.7538 - val_loss: 24.2386\n",
      "Epoch 133/200\n",
      "3350/3350 [==============================] - 1s 186us/step - loss: 18.7531 - val_loss: 24.2405\n",
      "Epoch 134/200\n",
      "3350/3350 [==============================] - 1s 183us/step - loss: 18.7546 - val_loss: 24.2353\n",
      "Epoch 135/200\n",
      "3350/3350 [==============================] - 1s 183us/step - loss: 18.7545 - val_loss: 24.2320\n",
      "Epoch 136/200\n",
      "3350/3350 [==============================] - 1s 185us/step - loss: 18.7538 - val_loss: 24.2346\n",
      "Epoch 137/200\n",
      "3350/3350 [==============================] - 1s 192us/step - loss: 18.7521 - val_loss: 24.2262\n",
      "Epoch 138/200\n",
      "3350/3350 [==============================] - 1s 198us/step - loss: 18.7532 - val_loss: 24.2279\n",
      "Epoch 139/200\n",
      "3350/3350 [==============================] - 1s 197us/step - loss: 18.7527 - val_loss: 24.2333\n",
      "Epoch 140/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7535 - val_loss: 24.2365\n",
      "Epoch 141/200\n",
      "3350/3350 [==============================] - 1s 193us/step - loss: 18.7536 - val_loss: 24.2351\n",
      "Epoch 142/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7522 - val_loss: 24.2247\n",
      "Epoch 143/200\n",
      "3350/3350 [==============================] - 1s 192us/step - loss: 18.7531 - val_loss: 24.2306\n",
      "Epoch 144/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7544 - val_loss: 24.2326\n",
      "Epoch 145/200\n",
      "3350/3350 [==============================] - 1s 190us/step - loss: 18.7531 - val_loss: 24.2381\n",
      "Epoch 146/200\n",
      "3350/3350 [==============================] - 1s 183us/step - loss: 18.7540 - val_loss: 24.2365\n",
      "Epoch 147/200\n",
      "3350/3350 [==============================] - 1s 192us/step - loss: 18.7560 - val_loss: 24.2363\n",
      "Epoch 148/200\n",
      "3350/3350 [==============================] - 1s 192us/step - loss: 18.7539 - val_loss: 24.2363\n",
      "Epoch 149/200\n",
      "3350/3350 [==============================] - 1s 188us/step - loss: 18.7544 - val_loss: 24.2358\n",
      "Epoch 150/200\n",
      "3350/3350 [==============================] - 1s 193us/step - loss: 18.7534 - val_loss: 24.2323\n",
      "Epoch 151/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7529 - val_loss: 24.2384\n",
      "Epoch 152/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7527 - val_loss: 24.2306\n",
      "Epoch 153/200\n",
      "3350/3350 [==============================] - 1s 195us/step - loss: 18.7539 - val_loss: 24.2372\n",
      "Epoch 154/200\n",
      "3350/3350 [==============================] - 1s 205us/step - loss: 18.7523 - val_loss: 24.2338\n",
      "Epoch 155/200\n",
      "3350/3350 [==============================] - 1s 183us/step - loss: 18.7533 - val_loss: 24.2294\n",
      "Epoch 156/200\n",
      "3350/3350 [==============================] - 1s 180us/step - loss: 18.7539 - val_loss: 24.2339\n",
      "Epoch 157/200\n",
      "3350/3350 [==============================] - 1s 195us/step - loss: 18.7550 - val_loss: 24.2322\n",
      "Epoch 158/200\n",
      "3350/3350 [==============================] - 1s 225us/step - loss: 18.7519 - val_loss: 24.2380\n",
      "Epoch 159/200\n",
      "3350/3350 [==============================] - 1s 213us/step - loss: 18.7541 - val_loss: 24.2424\n",
      "Epoch 160/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7546 - val_loss: 24.2424\n",
      "Epoch 161/200\n",
      "3350/3350 [==============================] - 1s 217us/step - loss: 18.7546 - val_loss: 24.2358\n",
      "Epoch 162/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7523 - val_loss: 24.2350\n",
      "Epoch 163/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7541 - val_loss: 24.2367\n",
      "Epoch 164/200\n",
      "3350/3350 [==============================] - 1s 194us/step - loss: 18.7465 - val_loss: 24.2199\n",
      "Epoch 165/200\n",
      "3350/3350 [==============================] - 1s 173us/step - loss: 18.7564 - val_loss: 24.2224\n",
      "Epoch 166/200\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.7529 - val_loss: 24.2253\n",
      "Epoch 167/200\n",
      "3350/3350 [==============================] - 1s 164us/step - loss: 18.7519 - val_loss: 24.2264\n",
      "Epoch 168/200\n",
      "3350/3350 [==============================] - 1s 159us/step - loss: 18.7534 - val_loss: 24.2339\n",
      "Epoch 169/200\n",
      "3350/3350 [==============================] - 1s 158us/step - loss: 18.7532 - val_loss: 24.2296\n",
      "Epoch 170/200\n",
      "3350/3350 [==============================] - 1s 158us/step - loss: 18.7541 - val_loss: 24.2344\n",
      "Epoch 171/200\n",
      "3350/3350 [==============================] - 1s 155us/step - loss: 18.7531 - val_loss: 24.2347\n",
      "Epoch 172/200\n",
      "3350/3350 [==============================] - 1s 167us/step - loss: 18.7524 - val_loss: 24.2380\n",
      "Epoch 173/200\n",
      "3350/3350 [==============================] - 1s 167us/step - loss: 18.7533 - val_loss: 24.2316\n",
      "Epoch 174/200\n",
      "3350/3350 [==============================] - 1s 167us/step - loss: 18.7529 - val_loss: 24.2263\n",
      "Epoch 175/200\n",
      "3350/3350 [==============================] - 1s 155us/step - loss: 18.7521 - val_loss: 24.2335\n",
      "Epoch 176/200\n",
      "3350/3350 [==============================] - 0s 140us/step - loss: 18.7524 - val_loss: 24.2319\n",
      "Epoch 177/200\n",
      "3350/3350 [==============================] - 1s 149us/step - loss: 18.7540 - val_loss: 24.2327\n",
      "Epoch 178/200\n",
      "3350/3350 [==============================] - 1s 152us/step - loss: 18.7520 - val_loss: 24.2341\n",
      "Epoch 179/200\n",
      "3350/3350 [==============================] - 1s 170us/step - loss: 18.7530 - val_loss: 24.2316\n",
      "Epoch 180/200\n",
      "3350/3350 [==============================] - 1s 200us/step - loss: 18.7526 - val_loss: 24.2302\n",
      "Epoch 181/200\n",
      "3350/3350 [==============================] - 1s 184us/step - loss: 18.7533 - val_loss: 24.2296\n",
      "Epoch 182/200\n",
      "3350/3350 [==============================] - 1s 184us/step - loss: 18.7537 - val_loss: 24.2299\n",
      "Epoch 183/200\n",
      "3350/3350 [==============================] - 1s 182us/step - loss: 18.7540 - val_loss: 24.2302\n",
      "Epoch 184/200\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.7532 - val_loss: 24.2330\n",
      "Epoch 185/200\n",
      "3350/3350 [==============================] - 1s 162us/step - loss: 18.7530 - val_loss: 24.2318\n",
      "Epoch 186/200\n",
      "3350/3350 [==============================] - 1s 191us/step - loss: 18.7530 - val_loss: 24.2340\n",
      "Epoch 187/200\n",
      "3350/3350 [==============================] - 1s 159us/step - loss: 18.7546 - val_loss: 24.2410\n",
      "Epoch 188/200\n",
      "3350/3350 [==============================] - 1s 153us/step - loss: 18.7532 - val_loss: 24.2308\n",
      "Epoch 189/200\n",
      "3350/3350 [==============================] - 1s 150us/step - loss: 18.7520 - val_loss: 24.2311\n",
      "Epoch 190/200\n",
      "3350/3350 [==============================] - 1s 166us/step - loss: 18.7530 - val_loss: 24.2324\n",
      "Epoch 191/200\n",
      "3350/3350 [==============================] - 1s 163us/step - loss: 18.7526 - val_loss: 24.2301\n",
      "Epoch 192/200\n",
      "3350/3350 [==============================] - 1s 172us/step - loss: 18.7526 - val_loss: 24.2285\n",
      "Epoch 193/200\n",
      "3350/3350 [==============================] - 1s 176us/step - loss: 18.7550 - val_loss: 24.2319\n",
      "Epoch 194/200\n",
      "3350/3350 [==============================] - 1s 216us/step - loss: 18.7551 - val_loss: 24.2344\n",
      "Epoch 195/200\n",
      "3350/3350 [==============================] - 1s 207us/step - loss: 18.7527 - val_loss: 24.2299\n",
      "Epoch 196/200\n",
      "3350/3350 [==============================] - 1s 196us/step - loss: 18.7542 - val_loss: 24.2277\n",
      "Epoch 197/200\n",
      "3350/3350 [==============================] - 1s 170us/step - loss: 18.7525 - val_loss: 24.2315\n",
      "Epoch 198/200\n",
      "3350/3350 [==============================] - 1s 163us/step - loss: 18.7548 - val_loss: 24.2376\n",
      "Epoch 199/200\n",
      "3350/3350 [==============================] - 1s 155us/step - loss: 18.7521 - val_loss: 24.2346\n",
      "Epoch 200/200\n",
      "3350/3350 [==============================] - 1s 152us/step - loss: 18.7526 - val_loss: 24.2342\n"
     ]
    }
   ],
   "source": [
    "years = [i for i in range(2005,2012)]+[2013]+[2015]\n",
    "res={}\n",
    "res['ols'],res['trees'], res['nets'] = [],[],[]\n",
    "for y in years:\n",
    "    print(\"---------current test year is {}---------\".format(y) )\n",
    "    data = dat[dat['year']==y]\n",
    "    train, test =  train_test_split(data, test_size = 0.25, random_state = 12)\n",
    "    y_train, X_train, y_test, X_test = train['lnwage'], train.iloc[:,4:], test['lnwage'], test.iloc[:,4:]\n",
    "    res['ols'].append( mincer(X_train, y_train,  X_test, y_test) )\n",
    "    res['trees'].append( trees(X_train, y_train, X_test, y_test, n_estimators=50, max_depth=5) )\n",
    "    res['nets'].append(nn(X_train, y_train, X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(years, res['ols'], label = \"Mincer\")\n",
    "plt.plot(years, res['trees'], label = \"Random Forest\")\n",
    "plt.plot(years, res['nets'], label = \"Neural Nets\")\n",
    "plt.ylabel('r-square')\n",
    "plt.xlabel('year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam\n",
    "\n",
    "\n",
    "def mincer(X_train, y_train,  X_test, y_test):\n",
    "    regressor = LinearRegression()\n",
    "    reg = regressor.fit(X_train,y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    # return reg.score(X_test, y_test)\n",
    "#     return np.mean(np.square(y_pred-y_test))\n",
    "    return metrics.r2_score(y_test,y_pred)\n",
    "\n",
    "def trees(X_train, y_train, X_test, y_test, model=\"rf\", n_estimators=5, max_depth=5):\n",
    "    if  model == \"gb\":\n",
    "        regressor = GradientBoostingRegressor(max_depth=15)\n",
    "    else:\n",
    "        regressor = RandomForestRegressor(n_estimators=5, max_depth=1, random_state=1)\n",
    "    reg = regressor.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    r2 = reg.score(X_test, y_test)\n",
    "#     return np.mean(np.square(y_pred-y_test))\n",
    "    return metrics.r2_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "def nn(X_train, y_train, X_test, y_test):\n",
    "    dim =  X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation='sigmoid'))\n",
    "    model.add(Dense(4, activation='sigmoid'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    opt = adam(lr=1e-3, decay=1e-3 / 200)\n",
    "    model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "    model.fit(X_train, y_train, validation_data=(  X_test, y_test), epochs=200, batch_size=10)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "#     print(\"The squrare error of this MLP is: {:.2f}\".format(np.mean(np.square(y_pred.flatten() - y_test))))\n",
    "    \n",
    "    return metrics.r2_score(y_test,y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
