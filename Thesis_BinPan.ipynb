{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Reseach Topic: Predicting Wage with Machline Methods\n","## What am I doing ?\n","For now, we predict salary with several models.\n","\n","## Data\n","The source of data is NLSY97. The portal can be accessed from [here](https://www.nlsinfo.org/investigator/pages/search.jsp?s=NLSY97). The data set is pre-cleaned by STATA with around 4500 obervations each year from 2. Features include age, year of experience, gender, schooling, race, marital status, industry, region(not yet!!). \n","\n","## Run Some Models\n","Bofore running anything, we split the data into training set, validation set, test set. <br/> I am preparing to do the following models:\n","- Linear model (Panel Data Regression)\n","- Trees and Forests (Regression Trees, Random Forests and XGBoost)\n","- Neural Nets (Multilayer Perceptrons)\n","\n","The performences of models will be compared in accuracy and R-square\n","\n","## Further Thoughts\n","(Haven't done yet) We resample the data into previous years and the following years. Then we train the model on previous years data and test it on the data from the following years."]},{"cell_type":"markdown","metadata":{},"source":["# Let's get started :D"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","\n","'''\n","Read in Data\n","'''\n","data_path = './data/mincer.xlsx'\n","dat = pd.read_excel(data_path)\n","dat = dat.fillna(0)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["years = [i for i in range(2005,2011)]+[2013]+[2015]\n","dat_tmp = dat \n","train, test =  train_test_split(dat_tmp, test_size = 0.2, random_state = 12)\n","y_train, X_train, y_test, X_test = train['lnwage'], train.iloc[:,4:], test['lnwage'], test.iloc[:,4:]"]},{"cell_type":"markdown","execution_count":2,"metadata":{},"outputs":[],"source":["##  Explore the dataset"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","sns.set(style = \"whitegrid\")\n","\n","sns.distplot(dat[dat['year']==2005]['age'],kde = False)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.set(style=\"whitegrid\")\n","f, axes = plt.subplots(2, 4, figsize=(8, 4), sharex=True)\n","\n","i,j = 0,0\n","for y in years:\n","    sns.distplot(dat[dat['year']==y]['lnwage'], ax = axes[j,i])\n","    if i < 3:\n","        i += 1\n","    else:\n","        i,j = 0,1 \n","plt.setp(axes, yticks=[])\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["## Define Models here"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","from sklearn.linear_model import LinearRegression\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","\n","from keras.models import Sequential\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Dropout\n","from keras.layers.core import Dense\n","from keras.layers import Flatten\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.optimizers import adam\n","\n","def mincer(X_train, y_train,  X_test, y_test):\n","    regressor = LinearRegression()\n","    reg = regressor.fit(X_train,y_train)\n","    return evaluate(reg, X_train, X_test, y_train, y_test)\n","\n","def trees(X_train, y_train, X_test, y_test,model=\"rf\", n_estimators = 100,max_depth=15):\n","    if  model == \"gb\":\n","        regressor = GradientBoostingRegressor(max_depth=max_depth, n_estimators = n_estimators)\n","    else:\n","        regressor = RandomForestRegressor(max_depth, random_state=1)\n","    reg = regressor.fit(X_train, y_train)\n","    return evaluate(reg, X_train, X_test, y_train, y_test)\n","\n","\n","def tree(X_train, y_train, X_test, y_test, max_depth = 3):\n","    regressor = DecisionTreeRegressor(max_depth=max_depth)\n","    reg = regressor.fit(X_train, y_train)\n","    return evaluate(reg, X_train, X_test, y_train, y_test)\n","\n","def nn(X_train, y_train, X_test, y_test, verbose = 1):\n","    model = Sequential()\n","    model.add(Dense(8, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n","    model.add(Dense(4, kernel_initializer='normal',activation='relu'))\n","    # model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n","    # model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n","    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n","    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n","    history = model.fit(X_train, y_train, epochs=10, batch_size = 50,  verbose = verbose, validation_split=0.10)\n","    \n","    return evaluate(model, X_train, X_test, y_train, y_test)\n","\n","def evaluate(model, X_train, X_test, y_train, y_test):\n","    y_pred_in = np.array( model.predict(X_train) )\n","    y_pred_out = np.array( model.predict(X_test) )\n","    if y_pred_in.ndim >1:\n","        y_pred_in = y_pred_in.flatten()\n","        y_pred_out =  y_pred_out.flatten()\n","\n","    r2_in = metrics.r2_score(y_train,y_pred_in)\n","    r2_out = metrics.r2_score(y_test,y_pred_out)\n","    mse_in = np.mean((y_pred_in - np.array(y_train))**2)\n","    mse_out = np.mean((y_pred_out - np.array( y_test) )**2 )\n","\n","    return mse_in, mse_out, r2_in, r2_out"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# Test 1: Comparasion Between the Models"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## Classification And Regression Trees (CART)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["from sklearn import tree\n","plt.figure()\n","regressor = DecisionTreeRegressor(max_depth=3)\n","reg = regressor.fit(X_train, y_train)\n","tree.plot_tree(reg, filled= True)\n","plt.show()\n","# plt.savefig('./image_output/tree.png')"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## Random Forests"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["regressor = RandomForestRegressor(max_depth = 10, random_state=1)\n","forest = regressor.fit(X_train, y_train)\n","importances = forest.feature_importances_\n","std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n","             axis=0)\n","indices = np.argsort(importances)[::-1]\n","n_features = X_train.shape[1]\n","# n_features = 10\n","# Print the feature ranking\n","print(\"Feature ranking:\")\n","\n","for f in range(n_features ):\n","    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n","\n","# Plot the feature importances of the forest\n","plt.figure()\n","plt.title(\"Feature importances\")\n","plt.bar(range(n_features ), importances[indices],\n","       color=\"r\", yerr=std[indices], align=\"center\")\n","plt.xticks(range(n_features), indices)\n","plt.xlim([-1, n_features])\n","plt.show()"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["trees(X_train, y_train, X_test, y_test, model=\"rf\",max_depth=40)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'module' object is not callable","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-11-de2c91e2e380>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rf\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrf_mse_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"]}],"source":["# Brute force the optimal depth for CART and Random Forest\n","rf, tr =[],[] \n","idx = [i for i in range(1,40)]\n","for dp in idx:\n","    rf.append(trees(X_train, y_train, X_test, y_test, model=\"rf\",max_depth=dp))\n","    tr.append(tree(X_train, y_train, X_test, y_test, max_depth=dp))\n","\n","rf_mse_out = np.array(rf)[:,1]\n","rf_r2_out = np.array(rf)[:,3]\n","tree_mse_out = np.array(tr)[:,1]\n","tree_r2_out = np.array(tr)[:,3]\n","\n","plt.plot(idx, rf_mse_out, label = \"Random Forest\")\n","plt.plot(idx, tree_mse_out, label = 'Regression Tree')\n","plt.xlabel(\"max_depth\")\n","plt.ylabel(\"Mean Squared Error\")\n","plt.legend()\n","plt.show()\n","\n","plt.plot(idx, rf_r2_out, label = \"Random Forest\")\n","plt.plot(idx, tree_r2_out, label = 'Regression Tree')\n","plt.xlabel(\"max_depth\")\n","plt.ylabel(r'$R^2$')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","execution_count":7,"metadata":{},"outputs":[],"source":["## Gradient Boosting Machine"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["regressor = GradientBoostingRegressor(max_depth=5)\n","boost= regressor.fit(X_train, y_train)\n","importances = boost.feature_importances_\n","plt.bar(range(len(boost.feature_importances_)), boost.feature_importances_)\n","plt.show()"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Brute force the optimal depth for CART and Random Forest\n","a, b, c =[],[],[] \n","idx = [i for i in range(1,20)]\n","for dp in idx:\n","    a.append(trees(X_train, y_train, X_test, y_test, model=\"gb\",n_estimators = 50,max_depth=dp))\n","    b.append(trees(X_train, y_train, X_test, y_test, model=\"gb\",n_estimators = 100,max_depth=dp))\n","    c.append(trees(X_train, y_train, X_test, y_test, model=\"gb\",n_estimators = 200,max_depth=dp))\n","a = np.array(a)[:,1]\n","b = np.array(b)[:,1]\n","c = np.array(c)[:,1]\n","\n","plt.plot(idx, a, label = \"n_estimators = 50\")\n","plt.plot(idx, b, label = 'n_estimators = 100')\n","plt.plot(idx, c, label = 'n_estimators = 200')\n","\n","plt.xlabel(\"max_depth\")\n","plt.ylabel(\"Mean Squared Error\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["plt.plot(idx, a, label = \"n_estimators = 50\")\n","plt.plot(idx, b, label = 'n_estimators = 100')\n","plt.plot(idx, c, label = 'n_estimators = 200')\n","\n","plt.xlabel(\"max_depth\")\n","plt.ylabel(r'$R^2$')\n","plt.legend()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## Neural Net"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["model = Sequential()\n","model.add(Dense(128, input_dim=29, kernel_initializer='normal', activation='relu'))\n","model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n","# model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n","# model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n","model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n","model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n","history = model.fit(X_train, y_train, epochs=50, batch_size = 32,  verbose=1, validation_split=0.2)\n","\n"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Plot training & validation accuracy values\n","plt.plot(history.history['mae'])\n","plt.plot(history.history['val_mae'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy (MAE)')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","execution_count":6,"metadata":{},"outputs":[],"source":["## Overall comparation Run 4 models and take the averges"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["iters = 5\n","res_linear, res_tree, res_rf, res_xgb, res_nn = np.array([0.0]*4),np.array([0.0]*4),np.array([0.0]*4),np.array([0.0]*4),np.array([0.0]*4)\n","for i in range(iters):\n","    print(\"foler:{}\".format(i+1))\n","    res_linear += np.array(mincer(X_train, y_train, X_test, y_test) )\n","\n","    res_tree += np.array( tree(X_train, y_train, X_test, y_test) )\n","\n","    res_rf += np.array( trees(X_train, y_train, X_test, y_test ) )\n","\n","    res_xgb += np.array( trees(X_train, y_train, X_test, y_test,model = \"gb\",max_depth=12) )\n","\n","    res_nn += np.array( nn(X_train, y_train, X_test, y_test) )\n","\n","tab1 = \"Regression & \"+\" & \".join([str(round(num,5)) for num in res_linear/iters ]) + r\" \\\\\" + \"\\n\" \n","tab2 = \"Regression Tree & \"+\" & \".join([str(round(num,5)) for num in res_tree/iters ]) + r\" \\\\\" + \"\\n\"\n","tab3 = \"Random Forest & \"+\" & \".join([str(round(num,5)) for num in res_rf/iters]) +r\" \\\\\" +  \"\\n\"\n","tab4 = \"XGBoost & \"+\" & \".join([str(round(num,5)) for num in res_xgb/iters]) + r\" \\\\\" + \"\\n\"\n","tab5 = \"Neural Networks & \"+\" & \".join([str(round(num,5)) for num in res_nn/iters]) + r\" \\\\\" + \"\\n\"\n","print( tab1+ tab2+ tab3+ tab4+ tab5)"]},{"cell_type":"markdown","metadata":{},"source":["# Test 2: Performance Over the Years\n","Now I am running the models over the years (2005 to 2011, 2013, 2015). "]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["years = [i for i in range(2005,2011)]+[2013]+[2015]\n","res={}\n","res['ols'],res['rf'],res['gbm'],res['trees'], res['nets'] = [],[],[],[],[]\n","for y in years:\n","    print(\"---------current test year is {}---------\".format(y) )\n","    data = dat[dat['year']==y]\n","    train, test =  train_test_split(data, test_size = 0.25, random_state = 12)\n","    y_train, X_train, y_test, X_test = train['lnwage'], train.iloc[:,4:], test['lnwage'], test.iloc[:,4:]\n","    res['ols'].append( mincer(X_train, y_train,  X_test, y_test) )\n","    res['rf'].append( trees(X_train, y_train, X_test, y_test, model= \"rf\",max_depth=5) )\n","    res['gbm'].append( trees(X_train, y_train, X_test, y_test, model= \"gb\",max_depth=5) )\n","    nn_res = list(nn(X_train, y_train, X_test, y_test, verbose = 0))\n","    if abs(nn_res[3])>1: \n","        nn[3] = 0\n","    res['nets'].append(nn_res)\n","    "]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","ylabels = ['MSE','MSE',r'$R^2$',r'$R^2$']\n","titles= ['MSE, training set',\n","         'MSE, testing set',\n","        r'$R^2$, training set',\n","        r'$R^2$, testing set'\n","        ]\n","for i in range(4):\n","    print(titles[i])\n","    plt.plot(years, np.array(res['ols'])[:,i], label = \"Mincer\")\n","    plt.plot(years, np.array(res['rf'])[:,i], label = \"Random Forests\")\n","    plt.plot(years, np.array(res['gbm'])[:,i], label = \"Gradient Boosting Machine\")\n","    plt.plot(years, np.array(res['nets'])[:,i], label = \"Neural Nets\")\n","    plt.ylabel(ylabels[i])\n","    plt.xlabel('year')\n","    plt.title(titles[i])\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### Plot activation functions"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","\n","x = np.arange(-2, 2, .1)\n","zero = np.zeros(len(z))\n","a = np.max([zero, z], axis=0)\n","b = 1/(1+np.exp(-x))\n","c = np.tanh(x)\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","ax.plot(x, a, label = \"Relu\")\n","ax.plot(x, b, label = \"Sigmoid\")\n","ax.plot(x,c,label = \"tanh\")\n","ax.set_ylim([-1.0, 1.0])\n","ax.set_xlim([-2.0, 2.0])\n","# ax.grid(True)\n","ax.set_xlabel('z')\n","ax.legend()\n","ax.set_title('Popular Activation Functions')\n","\n","plt.show()"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.9"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}